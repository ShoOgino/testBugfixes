{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","commits":[{"id":"47777586dd4c026834be0b2cc454d527cf8884b3","date":1330348390,"type":0,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0984ad47974c2d5d354519ddb2aa8358973a6271","date":1330868053,"type":3,"author":"Christian Moen","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":0,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c46b5eed1428b2cecc6851b67142702486279f89","date":1332284557,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"26bd77fc30a420a3e33c85e6fa6b0887eac4b029","date":1332293004,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c983d9f75169f8df08cc7d8006298cddc144075","date":1332524353,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n\n      if (random.nextInt(10) == 7) {\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          text = text.substring(0, maxWordLength);\n        }\n      } else {\n        if (simple) { \n          text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n        } else {\n          switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      if (simple) { \n        text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n      } else {\n        switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694","date":1332597963,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n\n      if (random.nextInt(10) == 7) {\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          // Take care not to split up a surrogate pair:\n          if (Character.isHighSurrogate(text.charAt(maxWordLength-1))) {\n            text = text.substring(0, maxWordLength-1);\n          } else {\n            text = text.substring(0, maxWordLength);\n          }\n        }\n      } else {\n        if (simple) { \n          text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n        } else {\n          switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n\n      if (random.nextInt(10) == 7) {\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          text = text.substring(0, maxWordLength);\n        }\n      } else {\n        if (simple) { \n          text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n        } else {\n          switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98b3157475550a08a1536940d79af1c32dbd6663","date":1332600844,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          // Take care not to split up a surrogate pair:\n          if (Character.isHighSurrogate(text.charAt(maxWordLength-1))) {\n            text = text.substring(0, maxWordLength-1);\n          } else {\n            text = text.substring(0, maxWordLength);\n          }\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n\n      if (random.nextInt(10) == 7) {\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          // Take care not to split up a surrogate pair:\n          if (Character.isHighSurrogate(text.charAt(maxWordLength-1))) {\n            text = text.substring(0, maxWordLength-1);\n          } else {\n            text = text.substring(0, maxWordLength);\n          }\n        }\n      } else {\n        if (simple) { \n          text = random.nextBoolean() ? _TestUtil.randomSimpleString(random, maxWordLength) : _TestUtil.randomHtmlishString(random, maxWordLength);\n        } else {\n          switch(_TestUtil.nextInt(random, 0, 4)) {\n          case 0: \n            text = _TestUtil.randomSimpleString(random, maxWordLength);\n            break;\n          case 1:\n            text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n            break;\n          case 2:\n            text = _TestUtil.randomHtmlishString(random, maxWordLength);\n            break;\n          default:\n            text = _TestUtil.randomUnicodeString(random, maxWordLength);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9e2e45dd476dec6eef3e891b76aa1424d9f8aff0","date":1332605336,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n          // Take care not to split up a surrogate pair:\n          if (Character.isHighSurrogate(text.charAt(maxWordLength-1))) {\n            text = text.substring(0, maxWordLength-1);\n          } else {\n            text = text.substring(0, maxWordLength);\n          }\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9bc1b742d25bc388ff8c6ebb9a948233878c07c","date":1332606347,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7d6adf8ea59977891966389011f3905e09932183","date":1332622471,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      checkAnalysisConsistency(random, a, useCharFilter, text);\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> positionLengths = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n\n      // First pass: save away \"correct\" tokens\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n\n        // KWTokenizer (for example) can produce a token\n        // even when input is length 0:\n        if (text.length() != 0) {\n\n          // (Optional) second pass: do something evil:\n          final int evilness = random.nextInt(50);\n          if (evilness == 17) {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n            }\n            // Throw an errant exception from the Reader:\n\n            MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n            evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n            reader = evilReader;\n\n            try {\n              // NOTE: some Tokenizers go and read characters\n              // when you call .setReader(Reader), eg\n              // PatternTokenizer.  This is a bit\n              // iffy... (really, they should only\n              // pull from the Reader when you call\n              // .incremenToken(), I think?), but we\n              // currently allow it, so, we must call\n              // a.tokenStream inside the try since we may\n              // hit the exc on init:\n              ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n              ts.reset();\n              while (ts.incrementToken());\n              fail(\"did not hit exception\");\n            } catch (RuntimeException re) {\n              assertTrue(MockReaderWrapper.isMyEvilException(re));\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          } else if (evilness == 7) {\n            // Only consume a subset of the tokens:\n            final int numTokensToRead = random.nextInt(tokens.size());\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n            }\n\n            reader = new StringReader(text);\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n              assertTrue(ts.incrementToken());\n            }\n            try {\n              ts.end();\n            } catch (AssertionError ae) {\n              // Catch & ignore MockTokenizer's\n              // anger...\n              if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n                // OK\n              } else {\n                throw ae;\n              }\n            }\n            ts.close();\n          }\n        }\n\n        // Final pass: verify clean tokenization matches\n        // results from first pass:\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n        }\n        reader = new StringReader(text);\n\n        if (random.nextInt(30) == 7) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n          }\n\n          reader = new MockReaderWrapper(random, reader);\n        }\n        \n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            toIntArray(positionLengths),\n            text.length());\n        } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            null,\n            text.length());\n        } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n          // offset + pos + posLength\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              toIntArray(positionLengths),\n              text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              null,\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1","date":1332710629,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text=\" + text);\n        throw new RuntimeException(t);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n\n      checkAnalysisConsistency(random, a, useCharFilter, text);\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f106f36fed7af3f5d4b31c051a3dd1b157913054","date":1332724926,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + text + \"'\");\n        throw new RuntimeException(t);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text=\" + text);\n        throw new RuntimeException(t);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["1fe9452de26a70442324c5bdc5a5a333e55f07db"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f76434de2201457cd82e97824eeaf8c8172fb6c7","date":1333047537,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + text + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + text + \"'\");\n        throw new RuntimeException(t);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1fe9452de26a70442324c5bdc5a5a333e55f07db","date":1333912637,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + escape(text) + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + text + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","bugFix":["f106f36fed7af3f5d4b31c051a3dd1b157913054"],"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d87cda278a2b66b08f00f816c73adc6f799851a","date":1333917898,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        // TODO: really we should pass a random seed to\n        // checkAnalysisConsistency then print it here too:\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + escape(text) + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + escape(text) + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple, offsetsAreCorrect);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        // TODO: really we should pass a random seed to\n        // checkAnalysisConsistency then print it here too:\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + escape(text) + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86aff04d7e822b5c613e383a069876a32efbfb61","date":1334085604,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple, offsetsAreCorrect);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple, offsetsAreCorrect);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6ec240b76d7ff137d58bc77b869525ebba68917f","date":1334086261,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n    // so this should only really fail from another thread if its an actual thread problem\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple, offsetsAreCorrect);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n    // so this should only really fail from another thread if its an actual thread problem\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","sourceOld":"  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple) throws IOException {\n\n    final LineFileDocs docs = new LineFileDocs(random);\n\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      \n      if (random.nextInt(10) == 7) {\n        // real data from linedocs\n        text = docs.nextDoc().get(\"body\");\n        if (text.length() > maxWordLength) {\n\n          // Take a random slice from the text...:\n          int startPos = random.nextInt(text.length() - maxWordLength);\n          if (startPos > 0 && Character.isLowSurrogate(text.charAt(startPos))) {\n            // Take care not to split up a surrogate pair:\n            startPos--;\n            assert Character.isHighSurrogate(text.charAt(startPos));\n          }\n          int endPos = startPos + maxWordLength - 1;\n          if (Character.isHighSurrogate(text.charAt(endPos))) {\n            // Take care not to split up a surrogate pair:\n            endPos--;\n          }\n          text = text.substring(startPos, 1+endPos);\n        }\n      } else {\n        // synthetic\n        text = randomAnalysisString(random, maxWordLength, simple);\n      }\n\n      try {\n        checkAnalysisConsistency(random, a, useCharFilter, text);\n      } catch (Throwable t) {\n        // TODO: really we should pass a random seed to\n        // checkAnalysisConsistency then print it here too:\n        System.err.println(\"TEST FAIL: useCharFilter=\" + useCharFilter + \" text='\" + escape(text) + \"'\");\n        Rethrow.rethrow(t);\n      }\n    }\n  }\n\n","bugFix":["b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694","58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1","9e2e45dd476dec6eef3e891b76aa1424d9f8aff0","47777586dd4c026834be0b2cc454d527cf8884b3","26bd77fc30a420a3e33c85e6fa6b0887eac4b029","2d87cda278a2b66b08f00f816c73adc6f799851a","8c983d9f75169f8df08cc7d8006298cddc144075","98b3157475550a08a1536940d79af1c32dbd6663","a9bc1b742d25bc388ff8c6ebb9a948233878c07c","f76434de2201457cd82e97824eeaf8c8172fb6c7","1fe9452de26a70442324c5bdc5a5a333e55f07db"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9","date":1334679794,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n    // so this should only really fail from another thread if its an actual thread problem\n    int numThreads = _TestUtil.nextInt(random, 2, 4);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n    // so this should only really fail from another thread if its an actual thread problem\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","date":1339522233,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    if (rarely(random)) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n    // so this should only really fail from another thread if its an actual thread problem\n    int numThreads = _TestUtil.nextInt(random, 2, 4);\n    AnalysisThread threads[] = new AnalysisThread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    for (int i = 0; i < threads.length; i++) {\n      if (threads[i].failed) {\n        throw new RuntimeException(\"some thread(s) failed\");\n      }\n    }\n  }\n\n","bugFix":["58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1","888c2d6bca1edd8d9293631d6e1d188b036e0f05","47777586dd4c026834be0b2cc454d527cf8884b3","6ec240b76d7ff137d58bc77b869525ebba68917f","d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9","86aff04d7e822b5c613e383a069876a32efbfb61"],"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3f4b164af1339f77b3d0bd5fdd344c01388ba90b","date":1340136530,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    if (rarely(random)) {\n      final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n      Assume.assumeTrue(iterations * maxWordLength < 100000 ||\n                        !(postingsFormat.equals(\"Memory\") ||\n                          postingsFormat.equals(\"SimpleText\")));\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    if (rarely(random)) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5da98d65f0e03ae614d89be16b7ab273854b2b6c","date":1340143418,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    if (rarely(random)) {\n      final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n      Assume.assumeTrue(iterations * maxWordLength < 100000 ||\n                        !(postingsFormat.equals(\"Memory\") ||\n                          postingsFormat.equals(\"SimpleText\")));\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","date":1379858263,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  _TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(_TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = _TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1cce78185d8741d50890d1829d47f1a0dffb37e","date":1395427956,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c","date":1396633078,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(TestUtil.getTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.shutdown();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(iw, dir);\n      } else {\n        IOUtils.closeWhileHandlingException(iw, dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.shutdown();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if its an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"995993f24c9f6feb42b49b71e1982cda8fa0b37c","date":1522116154,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","a1cce78185d8741d50890d1829d47f1a0dffb37e","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","date":1522191940,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 || !(postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 ||\n        !(postingsFormat.equals(\"Memory\") ||\n            postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57c6c784f777a2cc8fa014507ea129526822714d","date":1579733373,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 && !(postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, \"best effort\");\n    long seed = random.nextLong();\n    boolean useCharFilter = random.nextBoolean();\n    Directory dir = null;\n    RandomIndexWriter iw = null;\n    final String postingsFormat =  TestUtil.getPostingsFormat(\"dummy\");\n    boolean codecOk = iterations * maxWordLength < 100000 || !(postingsFormat.equals(\"SimpleText\"));\n    if (rarely(random) && codecOk) {\n      dir = newFSDirectory(createTempDir(\"bttc\"));\n      iw = new RandomIndexWriter(new Random(seed), dir, a);\n    }\n    boolean success = false;\n    try {\n      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      // now test with multiple threads: note we do the EXACT same thing we did before in each thread,\n      // so this should only really fail from another thread if it's an actual thread problem\n      int numThreads = TestUtil.nextInt(random, 2, 4);\n      final CountDownLatch startingGun = new CountDownLatch(1);\n      AnalysisThread threads[] = new AnalysisThread[numThreads];\n      for (int i = 0; i < threads.length; i++) {\n        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);\n      }\n      for (int i = 0; i < threads.length; i++) {\n        threads[i].start();\n      }\n      startingGun.countDown();\n      for (int i = 0; i < threads.length; i++) {\n        try {\n          threads[i].join();\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      for (int i = 0; i < threads.length; i++) {\n        if (threads[i].failed) {\n          throw new RuntimeException(\"some thread(s) failed\");\n        }\n      }\n      if (iw != null) {\n        iw.close();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(dir);\n      } else {\n        IOUtils.closeWhileHandlingException(dir); // checkindex\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1":["7d6adf8ea59977891966389011f3905e09932183"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["a1cce78185d8741d50890d1829d47f1a0dffb37e","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0984ad47974c2d5d354519ddb2aa8358973a6271"],"2d87cda278a2b66b08f00f816c73adc6f799851a":["1fe9452de26a70442324c5bdc5a5a333e55f07db"],"f76434de2201457cd82e97824eeaf8c8172fb6c7":["f106f36fed7af3f5d4b31c051a3dd1b157913054"],"26bd77fc30a420a3e33c85e6fa6b0887eac4b029":["c46b5eed1428b2cecc6851b67142702486279f89"],"3f4b164af1339f77b3d0bd5fdd344c01388ba90b":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"8c983d9f75169f8df08cc7d8006298cddc144075":["26bd77fc30a420a3e33c85e6fa6b0887eac4b029"],"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["2d87cda278a2b66b08f00f816c73adc6f799851a"],"a1cce78185d8741d50890d1829d47f1a0dffb37e":["6613659748fe4411a7dcf85266e55db1f95f7315"],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["5da98d65f0e03ae614d89be16b7ab273854b2b6c"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"24f89e8a6aac05753cde4c83d62a74356098200d":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"a9bc1b742d25bc388ff8c6ebb9a948233878c07c":["9e2e45dd476dec6eef3e891b76aa1424d9f8aff0"],"98b3157475550a08a1536940d79af1c32dbd6663":["b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["2d87cda278a2b66b08f00f816c73adc6f799851a","6ec240b76d7ff137d58bc77b869525ebba68917f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9e2e45dd476dec6eef3e891b76aa1424d9f8aff0":["98b3157475550a08a1536940d79af1c32dbd6663"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9"],"1fe9452de26a70442324c5bdc5a5a333e55f07db":["f76434de2201457cd82e97824eeaf8c8172fb6c7"],"0984ad47974c2d5d354519ddb2aa8358973a6271":["47777586dd4c026834be0b2cc454d527cf8884b3"],"6613659748fe4411a7dcf85266e55db1f95f7315":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"7d6adf8ea59977891966389011f3905e09932183":["a9bc1b742d25bc388ff8c6ebb9a948233878c07c"],"86aff04d7e822b5c613e383a069876a32efbfb61":["888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"f106f36fed7af3f5d4b31c051a3dd1b157913054":["58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1"],"c46b5eed1428b2cecc6851b67142702486279f89":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"6ec240b76d7ff137d58bc77b869525ebba68917f":["86aff04d7e822b5c613e383a069876a32efbfb61"],"d0d579490a72f2e6297eaa648940611234c57cf1":["a1cce78185d8741d50890d1829d47f1a0dffb37e"],"57c6c784f777a2cc8fa014507ea129526822714d":["24f89e8a6aac05753cde4c83d62a74356098200d"],"5da98d65f0e03ae614d89be16b7ab273854b2b6c":["3f4b164af1339f77b3d0bd5fdd344c01388ba90b"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["d0d579490a72f2e6297eaa648940611234c57cf1"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"47777586dd4c026834be0b2cc454d527cf8884b3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694":["8c983d9f75169f8df08cc7d8006298cddc144075"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57c6c784f777a2cc8fa014507ea129526822714d"]},"commit2Childs":{"58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1":["f106f36fed7af3f5d4b31c051a3dd1b157913054"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"2d87cda278a2b66b08f00f816c73adc6f799851a":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"f76434de2201457cd82e97824eeaf8c8172fb6c7":["1fe9452de26a70442324c5bdc5a5a333e55f07db"],"26bd77fc30a420a3e33c85e6fa6b0887eac4b029":["8c983d9f75169f8df08cc7d8006298cddc144075"],"3f4b164af1339f77b3d0bd5fdd344c01388ba90b":["5da98d65f0e03ae614d89be16b7ab273854b2b6c"],"8c983d9f75169f8df08cc7d8006298cddc144075":["b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694"],"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["86aff04d7e822b5c613e383a069876a32efbfb61"],"a1cce78185d8741d50890d1829d47f1a0dffb37e":["2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["6613659748fe4411a7dcf85266e55db1f95f7315"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"24f89e8a6aac05753cde4c83d62a74356098200d":["57c6c784f777a2cc8fa014507ea129526822714d"],"a9bc1b742d25bc388ff8c6ebb9a948233878c07c":["7d6adf8ea59977891966389011f3905e09932183"],"98b3157475550a08a1536940d79af1c32dbd6663":["9e2e45dd476dec6eef3e891b76aa1424d9f8aff0"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","47777586dd4c026834be0b2cc454d527cf8884b3"],"9e2e45dd476dec6eef3e891b76aa1424d9f8aff0":["a9bc1b742d25bc388ff8c6ebb9a948233878c07c"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"1fe9452de26a70442324c5bdc5a5a333e55f07db":["2d87cda278a2b66b08f00f816c73adc6f799851a"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["3f4b164af1339f77b3d0bd5fdd344c01388ba90b"],"0984ad47974c2d5d354519ddb2aa8358973a6271":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","c46b5eed1428b2cecc6851b67142702486279f89"],"6613659748fe4411a7dcf85266e55db1f95f7315":["a1cce78185d8741d50890d1829d47f1a0dffb37e"],"d135d0e0e3d4b83c56b666e6cf8c585c17d2fff9":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["24f89e8a6aac05753cde4c83d62a74356098200d"],"7d6adf8ea59977891966389011f3905e09932183":["58f1e03e773df2bf4ec8d7d3bf25e6ae779b08c1"],"86aff04d7e822b5c613e383a069876a32efbfb61":["6ec240b76d7ff137d58bc77b869525ebba68917f"],"f106f36fed7af3f5d4b31c051a3dd1b157913054":["f76434de2201457cd82e97824eeaf8c8172fb6c7"],"c46b5eed1428b2cecc6851b67142702486279f89":["26bd77fc30a420a3e33c85e6fa6b0887eac4b029"],"6ec240b76d7ff137d58bc77b869525ebba68917f":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"d0d579490a72f2e6297eaa648940611234c57cf1":["a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"57c6c784f777a2cc8fa014507ea129526822714d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5da98d65f0e03ae614d89be16b7ab273854b2b6c":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"47777586dd4c026834be0b2cc454d527cf8884b3":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"b1ea9a4f42f6ad0457c5d5279c4e55721cdb7694":["98b3157475550a08a1536940d79af1c32dbd6663"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}