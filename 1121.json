{"path":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"8a093d23e938d132b81b5f2de3d6b168afe3608e","date":1455076308,"type":1,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec996bf2daff09d61a876319e8fae4aebbea58e3","date":1458661946,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLat(hash), mortonUnhashLon(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"065f36185d91a7d825ce7adac5b0d8df5cb4f73f","date":1461251676,"type":3,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLat(hash), mortonUnhashLon(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca","date":1461308643,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLat(hash), mortonUnhashLon(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d3eccadf4f2a01d2034c96afe1a5c617b2e85af","date":1461918927,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8e8d5955830d712186a4beb716e797d505af7981","date":1461951189,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9ee4c03e3ee986704eeeb45c571d001905a6430","date":1462194267,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30540ec27130887a9372c159e8fe971200f37727","date":1462223109,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55b50463286869f584cf849d1587a0fcd54d1dfa","date":1462378517,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca":["ec996bf2daff09d61a876319e8fae4aebbea58e3","065f36185d91a7d825ce7adac5b0d8df5cb4f73f"],"8e8d5955830d712186a4beb716e797d505af7981":["ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca","3d3eccadf4f2a01d2034c96afe1a5c617b2e85af"],"30540ec27130887a9372c159e8fe971200f37727":["8e8d5955830d712186a4beb716e797d505af7981","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"55b50463286869f584cf849d1587a0fcd54d1dfa":["ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca","30540ec27130887a9372c159e8fe971200f37727"],"065f36185d91a7d825ce7adac5b0d8df5cb4f73f":["ec996bf2daff09d61a876319e8fae4aebbea58e3"],"ec996bf2daff09d61a876319e8fae4aebbea58e3":["8a093d23e938d132b81b5f2de3d6b168afe3608e"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["8e8d5955830d712186a4beb716e797d505af7981"],"3d3eccadf4f2a01d2034c96afe1a5c617b2e85af":["ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["30540ec27130887a9372c159e8fe971200f37727"],"8a093d23e938d132b81b5f2de3d6b168afe3608e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"]},"commit2Childs":{"ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca":["8e8d5955830d712186a4beb716e797d505af7981","55b50463286869f584cf849d1587a0fcd54d1dfa","3d3eccadf4f2a01d2034c96afe1a5c617b2e85af"],"8e8d5955830d712186a4beb716e797d505af7981":["30540ec27130887a9372c159e8fe971200f37727","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"30540ec27130887a9372c159e8fe971200f37727":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"55b50463286869f584cf849d1587a0fcd54d1dfa":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"065f36185d91a7d825ce7adac5b0d8df5cb4f73f":["ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca"],"ec996bf2daff09d61a876319e8fae4aebbea58e3":["ec4dfcb8d7e7f83ca209a5bcc83e8b98658e9dca","065f36185d91a7d825ce7adac5b0d8df5cb4f73f"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["30540ec27130887a9372c159e8fe971200f37727"],"3d3eccadf4f2a01d2034c96afe1a5c617b2e85af":["8e8d5955830d712186a4beb716e797d505af7981"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8a093d23e938d132b81b5f2de3d6b168afe3608e"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8a093d23e938d132b81b5f2de3d6b168afe3608e":["ec996bf2daff09d61a876319e8fae4aebbea58e3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}