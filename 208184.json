{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,SlotAcc.CountSlotAcc).mjava","commits":[{"id":"a56a9893014b284af4d1af451e6c02e7ffdf5b6e","date":1590065972,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,SlotAcc.CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, SlotAcc.CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f9e4bd10604489b5817ee29e35ac96a3148cbec","date":1594345357,"type":5,"author":"Michael Gibney","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,SlotAcc.CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    SweepCountAccStruct baseCountAccStruct = SweepingCountSlotAcc.baseStructOf(processor);\n    final List<SweepCountAccStruct> others = SweepingCountSlotAcc.otherStructsOf(processor);\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet && baseCountAccStruct != null;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n      baseCountAccStruct = new SweepCountAccStruct(baseCountAccStruct, docs);\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      final int termOrd = tt.termNum;\n      Iterator<SweepCountAccStruct> othersIter = others.iterator();\n      SweepCountAccStruct entry = baseCountAccStruct != null ? baseCountAccStruct : othersIter.next();\n      for (;;) {\n        entry.countAcc.incrementCount(termOrd, searcher.numDocs(tt.termQuery, entry.docSet));\n        if (!othersIter.hasNext()) {\n          break;\n        }\n        entry = othersIter.next();\n      }\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      final SweepIteratorAndCounts iterAndCounts = SweepDocIterator.newInstance(baseCountAccStruct, others);\n      final SweepDocIterator iter = iterAndCounts.iter;\n      final SegCountGlobal counts = new SegCountGlobal(iterAndCounts.countAccs);\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int maxIdx = iter.registerCounts(counts);\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum, 1, maxIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum, 1, maxIdx);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      final CountSlotAcc baseCounts = processor.countAcc;\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        baseCounts.incrementCount(i, maxTermCounts[i] - (int) baseCounts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, SlotAcc.CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2f9e4bd10604489b5817ee29e35ac96a3148cbec":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2f9e4bd10604489b5817ee29e35ac96a3148cbec"]},"commit2Childs":{"2f9e4bd10604489b5817ee29e35ac96a3148cbec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["2f9e4bd10604489b5817ee29e35ac96a3148cbec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}