{"path":"lucene/src/java/org/apache/lucene/index/codecs/NormsWriter#merge(MergeState).mjava","commits":[{"id":"f6ee314d4978b896d2a804ee60ba6e830624d990","date":1323044870,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/NormsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  public int merge(MergeState mergeState) throws IOException {\n    int numMergedDocs = 0;\n    for (FieldInfo fi : mergeState.fieldInfos) {\n      if (fi.isIndexed && !fi.omitNorms) {\n        startField(fi);\n        int numMergedDocsForField = 0;\n        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n          final int maxDoc = reader.reader.maxDoc();\n          byte normBuffer[] = reader.reader.norms(fi.name);\n          if (normBuffer == null) {\n            // Can be null if this segment doesn't have\n            // any docs with this field\n            normBuffer = new byte[maxDoc];\n            Arrays.fill(normBuffer, (byte)0);\n          }\n          // this segment has deleted docs, so we have to\n          // check for every doc if it is deleted or not\n          final Bits liveDocs = reader.liveDocs;\n          for (int k = 0; k < maxDoc; k++) {\n            if (liveDocs == null || liveDocs.get(k)) {\n              writeNorm(normBuffer[k]);\n              numMergedDocsForField++;\n            }\n          }\n          mergeState.checkAbort.work(maxDoc);\n        }\n        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n        numMergedDocs = numMergedDocsForField;\n      }\n    }\n    finish(numMergedDocs);\n    return numMergedDocs;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/NormsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  public int merge(MergeState mergeState) throws IOException {\n    int numMergedDocs = 0;\n    for (FieldInfo fi : mergeState.fieldInfos) {\n      if (fi.isIndexed && !fi.omitNorms) {\n        startField(fi);\n        int numMergedDocsForField = 0;\n        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n          final int maxDoc = reader.reader.maxDoc();\n          byte normBuffer[] = reader.reader.norms(fi.name);\n          if (normBuffer == null) {\n            // Can be null if this segment doesn't have\n            // any docs with this field\n            normBuffer = new byte[maxDoc];\n            Arrays.fill(normBuffer, (byte)0);\n          }\n          // this segment has deleted docs, so we have to\n          // check for every doc if it is deleted or not\n          final Bits liveDocs = reader.liveDocs;\n          for (int k = 0; k < maxDoc; k++) {\n            if (liveDocs == null || liveDocs.get(k)) {\n              writeNorm(normBuffer[k]);\n              numMergedDocsForField++;\n            }\n          }\n          mergeState.checkAbort.work(maxDoc);\n        }\n        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n        numMergedDocs = numMergedDocsForField;\n      }\n    }\n    finish(numMergedDocs);\n    return numMergedDocs;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/NormsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  public int merge(MergeState mergeState) throws IOException {\n    int numMergedDocs = 0;\n    for (FieldInfo fi : mergeState.fieldInfos) {\n      if (fi.isIndexed && !fi.omitNorms) {\n        startField(fi);\n        int numMergedDocsForField = 0;\n        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n          final int maxDoc = reader.reader.maxDoc();\n          byte normBuffer[] = reader.reader.norms(fi.name);\n          if (normBuffer == null) {\n            // Can be null if this segment doesn't have\n            // any docs with this field\n            normBuffer = new byte[maxDoc];\n            Arrays.fill(normBuffer, (byte)0);\n          }\n          // this segment has deleted docs, so we have to\n          // check for every doc if it is deleted or not\n          final Bits liveDocs = reader.liveDocs;\n          for (int k = 0; k < maxDoc; k++) {\n            if (liveDocs == null || liveDocs.get(k)) {\n              writeNorm(normBuffer[k]);\n              numMergedDocsForField++;\n            }\n          }\n          mergeState.checkAbort.work(maxDoc);\n        }\n        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n        numMergedDocs = numMergedDocsForField;\n      }\n    }\n    finish(numMergedDocs);\n    return numMergedDocs;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/NormsWriter#merge(MergeState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/NormsWriter#merge(MergeState).mjava","sourceNew":"  public int merge(MergeState mergeState) throws IOException {\n    int numMergedDocs = 0;\n    for (FieldInfo fi : mergeState.fieldInfos) {\n      if (fi.isIndexed && !fi.omitNorms) {\n        startField(fi);\n        int numMergedDocsForField = 0;\n        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n          final int maxDoc = reader.reader.maxDoc();\n          byte normBuffer[] = reader.reader.norms(fi.name);\n          if (normBuffer == null) {\n            // Can be null if this segment doesn't have\n            // any docs with this field\n            normBuffer = new byte[maxDoc];\n            Arrays.fill(normBuffer, (byte)0);\n          }\n          // this segment has deleted docs, so we have to\n          // check for every doc if it is deleted or not\n          final Bits liveDocs = reader.liveDocs;\n          for (int k = 0; k < maxDoc; k++) {\n            if (liveDocs == null || liveDocs.get(k)) {\n              writeNorm(normBuffer[k]);\n              numMergedDocsForField++;\n            }\n          }\n          mergeState.checkAbort.work(maxDoc);\n        }\n        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n        numMergedDocs = numMergedDocsForField;\n      }\n    }\n    finish(numMergedDocs);\n    return numMergedDocs;\n  }\n\n","sourceOld":"  public int merge(MergeState mergeState) throws IOException {\n    int numMergedDocs = 0;\n    for (FieldInfo fi : mergeState.fieldInfos) {\n      if (fi.isIndexed && !fi.omitNorms) {\n        startField(fi);\n        int numMergedDocsForField = 0;\n        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n          final int maxDoc = reader.reader.maxDoc();\n          byte normBuffer[] = reader.reader.norms(fi.name);\n          if (normBuffer == null) {\n            // Can be null if this segment doesn't have\n            // any docs with this field\n            normBuffer = new byte[maxDoc];\n            Arrays.fill(normBuffer, (byte)0);\n          }\n          // this segment has deleted docs, so we have to\n          // check for every doc if it is deleted or not\n          final Bits liveDocs = reader.liveDocs;\n          for (int k = 0; k < maxDoc; k++) {\n            if (liveDocs == null || liveDocs.get(k)) {\n              writeNorm(normBuffer[k]);\n              numMergedDocsForField++;\n            }\n          }\n          mergeState.checkAbort.work(maxDoc);\n        }\n        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n        numMergedDocs = numMergedDocsForField;\n      }\n    }\n    finish(numMergedDocs);\n    return numMergedDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f6ee314d4978b896d2a804ee60ba6e830624d990"],"f6ee314d4978b896d2a804ee60ba6e830624d990":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3615ce4a1f785ae1b779244de52c6a7d99227e60","f6ee314d4978b896d2a804ee60ba6e830624d990"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"f6ee314d4978b896d2a804ee60ba6e830624d990":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}