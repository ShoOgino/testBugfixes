{"path":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3","date":1361894345,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","8429ddf2214f2bf8abcbb5484fefef6aaf5c417e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ded1061b23ea6e38a649b2342334ad8c9000a855","date":1382622760,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            if (sub instanceof SingletonSortedSetDocValues) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              final SortedDocValues values = ((SingletonSortedSetDocValues) sub).getSortedDocValues();\n              accumSingle(counts, startTermIndex, values, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":["8429ddf2214f2bf8abcbb5484fefef6aaf5c417e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            if (sub instanceof SingletonSortedSetDocValues) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              final SortedDocValues values = ((SingletonSortedSetDocValues) sub).getSortedDocValues();\n              accumSingle(counts, startTermIndex, values, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            if (sub instanceof SingletonSortedSetDocValues) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              final SortedDocValues values = ((SingletonSortedSetDocValues) sub).getSortedDocValues();\n              accumSingle(counts, startTermIndex, values, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8429ddf2214f2bf8abcbb5484fefef6aaf5c417e","date":1397206443,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : new SingletonSortedSetDocValues(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedSetDocValues.EMPTY;\n            }\n            if (sub instanceof SingletonSortedSetDocValues) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              final SortedDocValues values = ((SingletonSortedSetDocValues) sub).getSortedDocValues();\n              accumSingle(counts, startTermIndex, values, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = SortedDocValues.EMPTY;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":["a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3","ded1061b23ea6e38a649b2342334ad8c9000a855"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30b6ad849a21206db510322a3f583ca70ae20a2f","date":1399996150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (schemaField.multiValued()) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (schemaField.multiValued()) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED_SET;\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.EMPTY_SORTED;\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":["a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        LeafReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        AtomicReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c5280f6286c7546ab75b72c663f7bb1dc10e96","date":1427372570,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix, String contains, boolean ignoreCase) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n    \n    final BytesRef containsBR = contains != null ? new BytesRef(contains) : null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        LeafReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (containsBR != null) {\n            final BytesRef term = si.lookupOrd(startTermIndex+i);\n            if (!StringHelper.contains(term, containsBR, ignoreCase)) {\n              continue;\n            }\n          }\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0 && containsBR == null) {\n          // if mincount<=0 and we're not examining the values for contains, then\n          // we won't discard any terms and we know exactly where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount) continue;\n          BytesRef term = null;\n          if (containsBR != null) {\n            term = si.lookupOrd(startTermIndex+i);\n            if (!StringHelper.contains(term, containsBR, ignoreCase)) {\n              continue;\n            }\n          }\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          if (term == null) {\n            term = si.lookupOrd(startTermIndex+i);\n          }\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        LeafReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    NamedList<Integer> res = new NamedList<>();\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return finalize(res, searcher, schemaField, docs, -1, missing);\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this faceting method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    final BytesRefBuilder prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRefBuilder();\n      prefixRef.copyChars(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = (int) si.lookupTerm(prefixRef.get());\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = (int) si.lookupTerm(prefixRef.get());\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=(int) si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRefBuilder charsRef = new CharsRefBuilder();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      Filter filter = docs.getTopFilter();\n      List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();\n      for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n        LeafReaderContext leaf = leaves.get(subIndex);\n        DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n        if (disi != null) {\n          if (multiValued) {\n            SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySortedSet();\n            }\n            final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n            if (singleton != null) {\n              // some codecs may optimize SORTED_SET storage for single-valued fields\n              accumSingle(counts, startTermIndex, singleton, disi, subIndex, ordinalMap);\n            } else {\n              accumMulti(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n            }\n          } else {\n            SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n            if (sub == null) {\n              sub = DocValues.emptySorted();\n            }\n            accumSingle(counts, startTermIndex, sub, disi, subIndex, ordinalMap);\n          }\n        }\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          final BytesRef term = si.lookupOrd(startTermIndex+tnum);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          final BytesRef term = si.lookupOrd(startTermIndex+i);\n          ft.indexedToReadable(term, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n    \n    return finalize(res, searcher, schemaField, docs, missingCount, missing);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ded1061b23ea6e38a649b2342334ad8c9000a855"],"ded1061b23ea6e38a649b2342334ad8c9000a855":["a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3"],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"56572ec06f1407c066d6b7399413178b33176cd8":["8429ddf2214f2bf8abcbb5484fefef6aaf5c417e","93dd449115a9247533e44bab47e8429e5dccbc6d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"8429ddf2214f2bf8abcbb5484fefef6aaf5c417e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["c9fb5f46e264daf5ba3860defe623a89d202dd87","52c5280f6286c7546ab75b72c663f7bb1dc10e96"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["8429ddf2214f2bf8abcbb5484fefef6aaf5c417e","30b6ad849a21206db510322a3f583ca70ae20a2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30b6ad849a21206db510322a3f583ca70ae20a2f":["8429ddf2214f2bf8abcbb5484fefef6aaf5c417e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["52c5280f6286c7546ab75b72c663f7bb1dc10e96"]},"commit2Childs":{"a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3":["ded1061b23ea6e38a649b2342334ad8c9000a855"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["8429ddf2214f2bf8abcbb5484fefef6aaf5c417e"],"ded1061b23ea6e38a649b2342334ad8c9000a855":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["52c5280f6286c7546ab75b72c663f7bb1dc10e96","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"8429ddf2214f2bf8abcbb5484fefef6aaf5c417e":["56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d","30b6ad849a21206db510322a3f583ca70ae20a2f"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"93dd449115a9247533e44bab47e8429e5dccbc6d":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","56572ec06f1407c066d6b7399413178b33176cd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a9cc184ce59bfe09f739d9aaa34fdb28ddc738c3"],"30b6ad849a21206db510322a3f583ca70ae20a2f":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}