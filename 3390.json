{"path":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","commits":[{"id":"ca7f1e1bca680a7b42df6b2ff6f81ec3774dd749","date":1132609906,"type":0,"author":"Daniel Naber","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    assertEquals(\"short\", filter.next().termText());\n    assertEquals(\"ab\", filter.next().termText());\n    assertEquals(\"foo\", filter.next().termText());\n    assertNull(filter.next());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    final Token reusableToken = new Token();\n    assertEquals(\"short\", filter.next(reusableToken).term());\n    assertEquals(\"ab\", filter.next(reusableToken).term());\n    assertEquals(\"foo\", filter.next(reusableToken).term());\n    assertNull(filter.next(reusableToken));\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    assertEquals(\"short\", filter.next().termText());\n    assertEquals(\"ab\", filter.next().termText());\n    assertEquals(\"foo\", filter.next().termText());\n    assertNull(filter.next());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    final Token reusableToken = new Token();\n    assertEquals(\"short\", filter.next(reusableToken).term());\n    assertEquals(\"ab\", filter.next(reusableToken).term());\n    assertEquals(\"foo\", filter.next(reusableToken).term());\n    assertNull(filter.next(reusableToken));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe0932c1d340f83fb0a611e5829b3046a1cc1152","date":1264946739,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, \n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6","date":1265808957,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, \n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, \n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestLengthFilter#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, \n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, \n        new StringReader(\"short toolong evenmuchlongertext a ab toolong foo\"));\n    LengthFilter filter = new LengthFilter(stream, 2, 6);\n    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);\n\n    assertTrue(filter.incrementToken());\n    assertEquals(\"short\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"ab\", termAtt.term());\n    assertTrue(filter.incrementToken());\n    assertEquals(\"foo\", termAtt.term());\n    assertFalse(filter.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["ca7f1e1bca680a7b42df6b2ff6f81ec3774dd749"],"ca7f1e1bca680a7b42df6b2ff6f81ec3774dd749":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d78f014fded44fbde905f4f84cdc21907b371e8":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"ca7f1e1bca680a7b42df6b2ff6f81ec3774dd749":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ca7f1e1bca680a7b42df6b2ff6f81ec3774dd749"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}