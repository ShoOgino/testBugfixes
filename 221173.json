{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","commits":[{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incomning indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0fcdcf196523675146a4df3193e91413533857ab","4d3e8520fd031bab31fd0e4d480e55958bc45efe","c48871ed951104729f5e17a8ee1091b43fa18980","2a186ae8733084223c22044e935e4ef848a143d1","2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"60ec5d047bcf0598ca7c56ff3615fc560d861f23","date":1277133311,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incomning indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incomning indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStoreSegment(newDsName);\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93bbf12a2e13137d385f5bd63f77f33339478e7c","date":1282563549,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6","date":1288888250,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a186ae8733084223c22044e935e4ef848a143d1","date":1289694819,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c498d3f8d75170b121f5eda2c6210ac5beb5d411","date":1289726298,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7","date":1292688238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(true, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          if (infoStream != null) {\n            message(\"process segment=\" + info.name);\n          }\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          String newDsName = newSegName;\n          boolean docStoreCopied = false;\n          if (dsNames.containsKey(dsName)) {\n            newDsName = dsNames.get(dsName);\n            docStoreCopied = true;\n          } else if (dsName != null) {\n            dsNames.put(dsName, newSegName);\n            docStoreCopied = false;\n          }\n\n          // Copy the segment files\n          for (String file : info.files()) {\n            if (docStoreCopied && IndexFileNames.isDocStoreFile(file)) {\n              continue;\n            } \n            dir.copy(directory, file, newSegName + IndexFileNames.stripSegmentName(file));\n          }\n\n          // Update SI appropriately\n          info.setDocStoreSegment(newDsName);\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3161e3ffcf20c09a22504a589d4d9bd273e11e33","date":1295142360,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          // if this call is removed in the future we need to make\n          // sure that info.clearFiles() is called here\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n          \n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n          \n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          // if this call is removed in the future we need to make\n          // sure that info.clearFiles() is called here\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n          \n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n          \n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments \n   * are copied as-is, meaning they are not converted to CFS if they aren't, \n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge} \n   * or {@link #optimize} afterwards.\n   * \n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n          \n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }      \n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f","date":1306408552,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, the segments\n   * are copied as-is, meaning they are not converted to CFS if they aren't,\n   * and vice-versa. If you wish to do that, you can call {@link #maybeMerge}\n   * or {@link #optimize} afterwards.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // Determine if the doc store of this segment needs to be copied. It's\n          // only relevant for segments who share doc store with others, because\n          // the DS might have been copied already, in which case we just want\n          // to update the DS name of this SegmentInfo.\n          // NOTE: pre-3x segments include a null DSName if they don't share doc\n          // store. So the following code ensures we don't accidentally insert\n          // 'null' to the map.\n          final String newDsName;\n          if (dsName != null) {\n            if (dsNames.containsKey(dsName)) {\n              newDsName = dsNames.get(dsName);\n            } else {\n              dsNames.put(dsName, newSegName);\n              newDsName = newSegName;\n            }\n          } else {\n            newDsName = newSegName;\n          }\n\n          // Copy the segment files\n          for (String file: info.files()) {\n            final String newFileName;\n            if (IndexFileNames.isDocStoreFile(file)) {\n              newFileName = newDsName + IndexFileNames.stripSegmentName(file);\n              if (dsFilesCopied.contains(newFileName)) {\n                continue;\n              }\n              dsFilesCopied.add(newFileName);\n            } else {\n              newFileName = newSegName + IndexFileNames.stripSegmentName(file);\n            }\n            assert !directory.fileExists(newFileName): \"file \\\"\" + newFileName + \"\\\" already exists\";\n            dir.copy(directory, file, newFileName);\n          }\n\n          // Update SI appropriately\n          info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n          info.dir = directory;\n          info.name = newSegName;\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n\n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(codecs); // read infos from dir\n        sis.read(dir, codecs);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          message(\"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            message(\"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, -1));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, -1));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a6a3fe8668125a7bb217ab4b515c348a6d21ddf9","date":1322493662,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones. Also, if the merge \n   * policy allows compound files, then any segment that is not compound is \n   * converted to such. However, if the segment is compound, it is copied as-is\n   * even if the merge policy does not allow compound files.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      int docCount = 0;\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      Comparator<String> versionComparator = StringHelper.getVersionComparator();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          docCount += info.docCount;\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          // create CFS only if the source segment is not CFS, and MP agrees it\n          // should be CFS.\n          boolean createCFS;\n          synchronized (this) { // Guard segmentInfos\n            createCFS = !info.getUseCompoundFile()\n                && mergePolicy.useCompoundFile(segmentInfos, info)\n                // optimize case only for segments that don't share doc stores\n                && versionComparator.compare(info.getVersion(), \"3.1\") >= 0;\n          }\n          \n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          if (createCFS) {\n            copySegmentIntoCFS(info, newSegName, context);\n          } else {\n            copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n          }\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream != null) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5df35ab57c223ea11aec64b53bf611904f3dced","date":1323640545,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory...).mjava","sourceNew":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing. A large document\n   * collection can be broken into sub-collections. Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine. The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>\n   * <b>NOTE:</b> the index in each {@link Directory} must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.\n   *\n   * <p>Note that this requires temporary free space in the\n   * {@link Directory} up to 2X the sum of all input indexes\n   * (including the starting index). If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #forceMerge(int)} for details).\n   *\n   * <p>\n   * <b>NOTE:</b> this method only copies the segments of the incoming indexes\n   * and does not merge them. Therefore deleted documents are not removed and\n   * the new segments are not merged with the existing ones.\n   *\n   * <p>This requires this index not be among those to be added.\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer. See <a\n   * href=\"#OOME\">above</a> for details.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(Directory...)\");\n      }\n\n      flush(false, true);\n\n      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();\n      for (Directory dir : dirs) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"addIndexes: process directory \" + dir);\n        }\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dir);\n        final Set<String> dsFilesCopied = new HashSet<String>();\n        final Map<String, String> dsNames = new HashMap<String, String>();\n        for (SegmentInfo info : sis) {\n          assert !infos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n\n          String newSegName = newSegmentName();\n          String dsName = info.getDocStoreSegment();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"addIndexes: process segment origName=\" + info.name + \" newName=\" + newSegName + \" dsName=\" + dsName + \" info=\" + info);\n          }\n\n          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));\n          \n          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context);\n\n          infos.add(info);\n        }\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        segmentInfos.addAll(infos);\n        checkpoint();\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["a6a3fe8668125a7bb217ab4b515c348a6d21ddf9"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["2a186ae8733084223c22044e935e4ef848a143d1"],"a6a3fe8668125a7bb217ab4b515c348a6d21ddf9":["3cc749c053615f5871f3b95715fe292f34e70a53"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c5df35ab57c223ea11aec64b53bf611904f3dced"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["58c6bbc222f074c844e736e6fb23647e3db9cfe3","c5df35ab57c223ea11aec64b53bf611904f3dced"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["3161e3ffcf20c09a22504a589d4d9bd273e11e33","1224a4027481acce15495b03bce9b48b93b42722"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"60ec5d047bcf0598ca7c56ff3615fc560d861f23":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"1224a4027481acce15495b03bce9b48b93b42722":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f"],"2a186ae8733084223c22044e935e4ef848a143d1":["2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6"],"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["85a883878c0af761245ab048babc63d099f835f3","2a186ae8733084223c22044e935e4ef848a143d1"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"85a883878c0af761245ab048babc63d099f835f3":["93bbf12a2e13137d385f5bd63f77f33339478e7c","2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1224a4027481acce15495b03bce9b48b93b42722","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"93bbf12a2e13137d385f5bd63f77f33339478e7c":["334c1175813aea771a71728cd2c4ee4754fd0603"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["60ec5d047bcf0598ca7c56ff3615fc560d861f23"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7b91922b55d15444d554721b352861d028eb8278":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"3161e3ffcf20c09a22504a589d4d9bd273e11e33":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"8fe956d65251358d755c56f14fe8380644790e47":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6":["93bbf12a2e13137d385f5bd63f77f33339478e7c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","ddc4c914be86e34b54f70023f45a60fa7f04e929","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["c5df35ab57c223ea11aec64b53bf611904f3dced","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["2ed5989ef2bc7bbd155dbff1e023ea849003dbf7"],"a6a3fe8668125a7bb217ab4b515c348a6d21ddf9":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3cc749c053615f5871f3b95715fe292f34e70a53"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3161e3ffcf20c09a22504a589d4d9bd273e11e33"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"60ec5d047bcf0598ca7c56ff3615fc560d861f23":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","7b91922b55d15444d554721b352861d028eb8278"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1224a4027481acce15495b03bce9b48b93b42722","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[],"2a186ae8733084223c22044e935e4ef848a143d1":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"2ed5989ef2bc7bbd155dbff1e023ea849003dbf7":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"85a883878c0af761245ab048babc63d099f835f3":["c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["32dd6d3e87d9e4f05e3e9de40bebf1ff1482771f","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"3cc749c053615f5871f3b95715fe292f34e70a53":["a6a3fe8668125a7bb217ab4b515c348a6d21ddf9"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"334c1175813aea771a71728cd2c4ee4754fd0603":["93bbf12a2e13137d385f5bd63f77f33339478e7c"],"93bbf12a2e13137d385f5bd63f77f33339478e7c":["85a883878c0af761245ab048babc63d099f835f3","2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["60ec5d047bcf0598ca7c56ff3615fc560d861f23","8fe956d65251358d755c56f14fe8380644790e47"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"3161e3ffcf20c09a22504a589d4d9bd273e11e33":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"2248ea99d1f1e5ae6d67d1547acfe3e29576b8a6":["2a186ae8733084223c22044e935e4ef848a143d1","85a883878c0af761245ab048babc63d099f835f3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","5d004d0e0b3f65bb40da76d476d659d7888270e8","2e10cb22a8bdb44339e282925a29182bb2f3174d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}