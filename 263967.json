{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,FieldMetadata.Serializer,String,int,int,String,String).mjava","commits":[{"id":"d28da8a459f5f0c930da7185c56d0c25edd3fbd1","date":1577783695,"type":1,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,FieldMetadata.Serializer,String,int,int,String,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,String,int,int,String,String).mjava","sourceNew":"  /**\n   * @param blockDecoder Optional block decoder, may be null if none.\n   *                     It can be used for decompression or decryption.\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state,\n                                    BlockDecoder blockDecoder, FieldMetadata.Serializer fieldMetadataReader,\n                                     String codecName, int versionStart, int versionCurrent,\n                                    String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, fieldMetadataReader, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, blockDecoder, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","sourceOld":"  /**\n   * @param blockDecoder Optional block decoder, may be null if none.\n   *                     It can be used for decompression or decryption.\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, BlockDecoder blockDecoder,\n                                     String codecName, int versionStart, int versionCurrent, String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, blockDecoder, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c606b777c4250f3f3f6f66d659c7c4c403679b71","date":1577958559,"type":1,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,FieldMetadata.Serializer,String,int,int,String,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,String,int,int,String,String).mjava","sourceNew":"  /**\n   * @param blockDecoder Optional block decoder, may be null if none.\n   *                     It can be used for decompression or decryption.\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state,\n                                    BlockDecoder blockDecoder, FieldMetadata.Serializer fieldMetadataReader,\n                                     String codecName, int versionStart, int versionCurrent,\n                                    String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, fieldMetadataReader, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, blockDecoder, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","sourceOld":"  /**\n   * @param blockDecoder Optional block decoder, may be null if none.\n   *                     It can be used for decompression or decryption.\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, BlockDecoder blockDecoder,\n                                     String codecName, int versionStart, int versionCurrent, String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, blockDecoder, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"97ee06ea0335fd2077527d81c4c993c86e06f0da","date":1583768142,"type":5,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,boolean,FieldMetadata.Serializer,String,int,int,String,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/uniformsplit/UniformSplitTermsReader#UniformSplitTermsReader(PostingsReaderBase,SegmentReadState,BlockDecoder,FieldMetadata.Serializer,String,int,int,String,String).mjava","sourceNew":"  /**\n   * @see #UniformSplitTermsReader(PostingsReaderBase, SegmentReadState, BlockDecoder, boolean)\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, BlockDecoder blockDecoder,\n                                    boolean dictionaryOnHeap, FieldMetadata.Serializer fieldMetadataReader,\n                                    String codecName, int versionStart, int versionCurrent,\n                                    String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, fieldMetadataReader, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, state, blockDecoder, dictionaryOnHeap, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","sourceOld":"  /**\n   * @param blockDecoder Optional block decoder, may be null if none.\n   *                     It can be used for decompression or decryption.\n   */\n  protected UniformSplitTermsReader(PostingsReaderBase postingsReader, SegmentReadState state,\n                                    BlockDecoder blockDecoder, FieldMetadata.Serializer fieldMetadataReader,\n                                     String codecName, int versionStart, int versionCurrent,\n                                    String termsBlocksExtension, String dictionaryExtension) throws IOException {\n     IndexInput dictionaryInput = null;\n     IndexInput blockInput = null;\n     boolean success = false;\n     try {\n       this.postingsReader = postingsReader;\n       String segmentName = state.segmentInfo.name;\n       String termsName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, termsBlocksExtension);\n       blockInput = state.directory.openInput(termsName, state.context);\n\n       int version = CodecUtil.checkIndexHeader(blockInput, codecName, versionStart,\n           versionCurrent, state.segmentInfo.getId(), state.segmentSuffix);\n       String indexName = IndexFileNames.segmentFileName(segmentName, state.segmentSuffix, dictionaryExtension);\n       dictionaryInput = state.directory.openInput(indexName, state.context);\n\n       CodecUtil.checkIndexHeader(dictionaryInput, codecName, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n       CodecUtil.checksumEntireFile(dictionaryInput);\n\n       postingsReader.init(blockInput, state);\n       CodecUtil.retrieveChecksum(blockInput);\n\n       seekFieldsMetadata(blockInput);\n       Collection<FieldMetadata> fieldMetadataCollection = parseFieldsMetadata(blockInput, state.fieldInfos, fieldMetadataReader, state.segmentInfo.maxDoc());\n\n       fieldToTermsMap = new HashMap<>();\n       this.blockInput = blockInput;\n       this.dictionaryInput = dictionaryInput;\n\n       fillFieldMap(postingsReader, blockDecoder, dictionaryInput, blockInput, fieldMetadataCollection, state.fieldInfos);\n\n       List<String> fieldNames = new ArrayList<>(fieldToTermsMap.keySet());\n       Collections.sort(fieldNames);\n       sortedFieldNames = Collections.unmodifiableList(fieldNames);\n\n       success = true;\n     } finally {\n       if (!success) {\n         IOUtils.closeWhileHandlingException(blockInput, dictionaryInput);\n       }\n     }\n   }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c606b777c4250f3f3f6f66d659c7c4c403679b71":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d28da8a459f5f0c930da7185c56d0c25edd3fbd1"],"97ee06ea0335fd2077527d81c4c993c86e06f0da":["d28da8a459f5f0c930da7185c56d0c25edd3fbd1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d28da8a459f5f0c930da7185c56d0c25edd3fbd1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["97ee06ea0335fd2077527d81c4c993c86e06f0da"]},"commit2Childs":{"c606b777c4250f3f3f6f66d659c7c4c403679b71":[],"97ee06ea0335fd2077527d81c4c993c86e06f0da":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c606b777c4250f3f3f6f66d659c7c4c403679b71","d28da8a459f5f0c930da7185c56d0c25edd3fbd1"],"d28da8a459f5f0c930da7185c56d0c25edd3fbd1":["c606b777c4250f3f3f6f66d659c7c4c403679b71","97ee06ea0335fd2077527d81c4c993c86e06f0da"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c606b777c4250f3f3f6f66d659c7c4c403679b71","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}