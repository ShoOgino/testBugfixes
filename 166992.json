{"path":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","commits":[{"id":"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b","date":1313816278,"type":1,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3aecf04c2d454cf9e42c4dc50241a8df41f8e6ab","date":1324934316,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength, false);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["f9fdc0777b84633cc8cfa8995ff5b0d411e4515b"],"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["3aecf04c2d454cf9e42c4dc50241a8df41f8e6ab"],"3aecf04c2d454cf9e42c4dc50241a8df41f8e6ab":["e6e919043fa85ee891123768dd655a98edbbf63c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e6e919043fa85ee891123768dd655a98edbbf63c":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f9fdc0777b84633cc8cfa8995ff5b0d411e4515b"],"3aecf04c2d454cf9e42c4dc50241a8df41f8e6ab":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"e6e919043fa85ee891123768dd655a98edbbf63c":["3aecf04c2d454cf9e42c4dc50241a8df41f8e6ab"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["e6e919043fa85ee891123768dd655a98edbbf63c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}