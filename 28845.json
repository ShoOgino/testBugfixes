{"path":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","pathOld":"/dev/null","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","sourceNew":null,"sourceOld":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","pathOld":"src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","pathOld":"/dev/null","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c8b12bda3f5864b27e3e04df1be4f6736ec067a","date":1270088127,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d085fb336a7208eea2214e5ffcc803960819b60b","date":1270981894,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setEmpty().append(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          TermAttribute termAtt = addAttribute(TermAttribute.class);\n          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setTermBuffer(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42fd6cf39f8ec69cf682e3f653bdbdeed8504e85","date":1272966943,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory#testDups(String,Token...).mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter#testDups(String,Token...).mjava","sourceNew":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setEmpty().append(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","sourceOld":"  public void testDups(final String expected, final Token... tokens)\n    throws Exception {\n\n    final Iterator<Token> toks = Arrays.asList(tokens).iterator();\n    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();\n    final TokenStream ts = factory.create\n      (new TokenStream() {\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n          public boolean incrementToken() {\n            if (toks.hasNext()) {\n              clearAttributes();\n              Token tok = toks.next();\n              termAtt.setEmpty().append(tok.term());\n              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());\n              posIncAtt.setPositionIncrement(tok.getPositionIncrement());\n              return true;\n            } else {\n              return false;\n            }\n          }\n        });\n    \n    assertTokenStreamContents(ts, expected.split(\"\\\\s\"));   \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["1da8d55113b689b06716246649de6f62430f15c0"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"42fd6cf39f8ec69cf682e3f653bdbdeed8504e85":["d085fb336a7208eea2214e5ffcc803960819b60b"],"d085fb336a7208eea2214e5ffcc803960819b60b":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["42fd6cf39f8ec69cf682e3f653bdbdeed8504e85"]},"commit2Childs":{"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["d085fb336a7208eea2214e5ffcc803960819b60b"],"1da8d55113b689b06716246649de6f62430f15c0":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"42fd6cf39f8ec69cf682e3f653bdbdeed8504e85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d085fb336a7208eea2214e5ffcc803960819b60b":["42fd6cf39f8ec69cf682e3f653bdbdeed8504e85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}