{"path":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzer_custom_with_confdir().mjava","commits":[{"id":"f554f2d9b5456248ab6467b9d4f6015686797a6c","date":1554891357,"type":0,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/luke/src/test/org/apache/lucene/luke/models/analysis/AnalysisImplTest#testAnalyzer_custom_with_confdir().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testAnalyzer_custom_with_confdir() throws Exception {\n    Path confDir = createTempDir(\"conf\");\n    Path stopFile = Files.createFile(Paths.get(confDir.toString(), \"stop.txt\"));\n    Files.write(stopFile, \"of\\nthe\\nby\\nfor\\n\".getBytes(StandardCharsets.UTF_8));\n\n    AnalysisImpl analysis = new AnalysisImpl();\n    Map<String, String> tkParams = new HashMap<>();\n    tkParams.put(\"maxTokenLen\", \"128\");\n    Map<String, String> tfParams = new HashMap<>();\n    tfParams.put(\"ignoreCase\", \"true\");\n    tfParams.put(\"words\", \"stop.txt\");\n    tfParams.put(\"format\", \"wordset\");\n    CustomAnalyzerConfig.Builder builder = new CustomAnalyzerConfig.Builder(\n        \"whitespace\", tkParams)\n        .configDir(confDir.toString())\n        .addTokenFilterConfig(\"lowercase\", Collections.emptyMap())\n        .addTokenFilterConfig(\"stop\", tfParams);\n    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());\n    assertEquals(\"org.apache.lucene.analysis.custom.CustomAnalyzer\", analyzer.getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.WhitespaceTokenizerFactory\", analyzer.getTokenizerFactory().getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.LowerCaseFilterFactory\", analyzer.getTokenFilterFactories().get(0).getClass().getName());\n    assertEquals(\"org.apache.lucene.analysis.core.StopFilterFactory\", analyzer.getTokenFilterFactories().get(1).getClass().getName());\n\n    String text = \"Government of the People, by the People, for the People\";\n    List<Analysis.Token> tokens = analysis.analyze(text);\n    assertNotNull(tokens);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f554f2d9b5456248ab6467b9d4f6015686797a6c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f554f2d9b5456248ab6467b9d4f6015686797a6c"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f554f2d9b5456248ab6467b9d4f6015686797a6c"],"f554f2d9b5456248ab6467b9d4f6015686797a6c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}