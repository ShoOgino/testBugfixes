{"path":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","commits":[{"id":"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","date":1316747797,"type":0,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766","2b7d23fd7761cfb5055ce47fc02aee3e155ac202","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","sourceNew":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }\n\n","sourceOld":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }\n\n","bugFix":null,"bugIntro":["190bb9bef51c64832a51cf820b20992d4cf60c51"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","sourceNew":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericField.DataType numericType = fieldType().numericType();\n    if (numericType != null) {\n      if (numericTokenStream == null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val = (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }\n\n","sourceOld":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }\n\n","bugFix":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","2b7d23fd7761cfb5055ce47fc02aee3e155ac202"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/document/Field#tokenStream(Analyzer).mjava","sourceNew":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericField.DataType numericType = fieldType().numericType();\n    if (numericType != null) {\n      if (numericTokenStream == null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val = (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }\n\n","sourceOld":"  /**\n   * {@inheritDoc}\n   */\n  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericField.DataType numericType = fieldType().numericType();\n    if (numericType != null) {\n      if (numericTokenStream == null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val = (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() == null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used = true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used = false;\n        }\n      };\n    }\n\n    if (tokenStream != null) {\n      return tokenStream;\n    } else if (readerValue() != null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() != null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["fa0f44f887719e97183771e977cfc4bfb485b766"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"fa0f44f887719e97183771e977cfc4bfb485b766":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["fa0f44f887719e97183771e977cfc4bfb485b766"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}