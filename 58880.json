{"path":"src/java/org/apache/lucene/analysis/WordlistLoader#getWordSet(Class[#],String,String).mjava","commits":[{"id":"ddc5fbfd4c64963aba52713a4496522540294d69","date":1262508497,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/WordlistLoader#getWordSet(Class[#],String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Loads a text file associated with a given class (See\n   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry\n   * to a {@link Set} (omitting leading and trailing whitespace). Every line of\n   * the file should contain only one word. The words need to be in lower-case if\n   * you make use of an Analyzer which uses LowerCaseFilter (like\n   * StandardAnalyzer).\n   * \n   * @param aClass\n   *          a class that is associated with the given stopwordResource\n   * @param stopwordResource\n   *          name of the resource file associated with the given class\n   * @param comment\n   *          the comment string to ignore\n   * @return a {@link Set} with the file's words\n   */\n  public static Set<String> getWordSet(Class<?> aClass,\n      String stopwordResource, String comment) throws IOException {\n    final Reader reader = new BufferedReader(new InputStreamReader(aClass\n        .getResourceAsStream(stopwordResource), \"UTF-8\"));\n    try {\n      return getWordSet(reader, comment);\n    } finally {\n      reader.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/analysis/WordlistLoader#getWordSet(Class[#],String,String).mjava","pathOld":"src/java/org/apache/lucene/analysis/WordlistLoader#getWordSet(Class[#],String,String).mjava","sourceNew":"  /**\n   * Loads a text file associated with a given class (See\n   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry\n   * to a {@link Set} (omitting leading and trailing whitespace). Every line of\n   * the file should contain only one word. The words need to be in lower-case if\n   * you make use of an Analyzer which uses LowerCaseFilter (like\n   * StandardAnalyzer).\n   * \n   * @param aClass\n   *          a class that is associated with the given stopwordResource\n   * @param stopwordResource\n   *          name of the resource file associated with the given class\n   * @param comment\n   *          the comment string to ignore\n   * @return a {@link Set} with the file's words\n   */\n  public static Set<String> getWordSet(Class<?> aClass,\n      String stopwordResource, String comment) throws IOException {\n    final Reader reader = new BufferedReader(new InputStreamReader(aClass\n        .getResourceAsStream(stopwordResource), \"UTF-8\"));\n    try {\n      return getWordSet(reader, comment);\n    } finally {\n      reader.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Loads a text file associated with a given class (See\n   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry\n   * to a {@link Set} (omitting leading and trailing whitespace). Every line of\n   * the file should contain only one word. The words need to be in lower-case if\n   * you make use of an Analyzer which uses LowerCaseFilter (like\n   * StandardAnalyzer).\n   * \n   * @param aClass\n   *          a class that is associated with the given stopwordResource\n   * @param stopwordResource\n   *          name of the resource file associated with the given class\n   * @param comment\n   *          the comment string to ignore\n   * @return a {@link Set} with the file's words\n   */\n  public static Set<String> getWordSet(Class<?> aClass,\n      String stopwordResource, String comment) throws IOException {\n    final Reader reader = new BufferedReader(new InputStreamReader(aClass\n        .getResourceAsStream(stopwordResource), \"UTF-8\"));\n    try {\n      return getWordSet(reader, comment);\n    } finally {\n      reader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ddc5fbfd4c64963aba52713a4496522540294d69":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["ddc5fbfd4c64963aba52713a4496522540294d69"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ddc5fbfd4c64963aba52713a4496522540294d69"],"ddc5fbfd4c64963aba52713a4496522540294d69":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}