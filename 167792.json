{"path":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(MutableFieldInfos).mjava","commits":[{"id":"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","date":1337136355,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(MutableFieldInfos).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(MutableFieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a851824c09818632c94eba41e60ef5e72e323c8e","date":1337355760,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(MutableFieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(MutableFieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a851824c09818632c94eba41e60ef5e72e323c8e":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a851824c09818632c94eba41e60ef5e72e323c8e":[],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["a851824c09818632c94eba41e60ef5e72e323c8e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a851824c09818632c94eba41e60ef5e72e323c8e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}