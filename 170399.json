{"path":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,InvertedFieldsConsumer,SegmentWriteState).mjava","commits":[{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,InvertedFieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, InvertedFieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,InvertedFieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, InvertedFieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}