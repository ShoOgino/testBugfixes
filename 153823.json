{"path":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","commits":[{"id":"134a24d0cb66520908d88384f1a559875704ed25","date":1445326601,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"/dev/null","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(c, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, c, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(BytesRef.deepCopyOf(c), classScore));\n    }\n    ArrayList<ClassificationResult<BytesRef>> assignedClassesNorm = normClassificationResults(assignedClasses);\n    return assignedClassesNorm;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0e0e52292aa6f946f8944886fa7f080d3f14ff2c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38b5bd3ae837751f57f363e9a41b833794222814","date":1445342257,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(c, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, c, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(BytesRef.deepCopyOf(c), classScore));\n    }\n    ArrayList<ClassificationResult<BytesRef>> assignedClassesNorm = normClassificationResults(assignedClasses);\n    return assignedClassesNorm;\n  }\n\n","bugFix":null,"bugIntro":["0e0e52292aa6f946f8944886fa7f080d3f14ff2c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"444d4b906d0e3398f87d6a5c4967c508f11a7f0b","date":1466507434,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e0e52292aa6f946f8944886fa7f080d3f14ff2c","date":1504356239,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":["38b5bd3ae837751f57f363e9a41b833794222814","134a24d0cb66520908d88384f1a559875704ed25"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21283ed01203901a7257aa4b7f0a0899c86e56e","date":1504689720,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7","date":1504848000,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum classesEnum = classes.iterator();\n    BytesRef c;\n\n    analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n    int docsWithClassSize = countDocsWithClass();\n    while ((c = classesEnum.next()) != null) {\n      double classScore = 0;\n      Term term = new Term(this.classFieldName, c);\n      for (String fieldName : textFieldNames) {\n        List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n        double fieldScore = 0;\n        for (String[] fieldTokensArray : tokensArrays) {\n          fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n        }\n        classScore += fieldScore;\n      }\n      assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/document/SimpleNaiveBayesDocumentClassifier#assignNormClasses(Document).mjava","sourceNew":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiTerms.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","sourceOld":"  private List<ClassificationResult<BytesRef>> assignNormClasses(Document inputDocument) throws IOException {\n    List<ClassificationResult<BytesRef>> assignedClasses = new ArrayList<>();\n    Map<String, List<String[]>> fieldName2tokensArray = new LinkedHashMap<>();\n    Map<String, Float> fieldName2boost = new LinkedHashMap<>();\n    Terms classes = MultiFields.getTerms(indexReader, classFieldName);\n    if (classes != null) {\n      TermsEnum classesEnum = classes.iterator();\n      BytesRef c;\n\n      analyzeSeedDocument(inputDocument, fieldName2tokensArray, fieldName2boost);\n\n      int docsWithClassSize = countDocsWithClass();\n      while ((c = classesEnum.next()) != null) {\n        double classScore = 0;\n        Term term = new Term(this.classFieldName, c);\n        for (String fieldName : textFieldNames) {\n          List<String[]> tokensArrays = fieldName2tokensArray.get(fieldName);\n          double fieldScore = 0;\n          for (String[] fieldTokensArray : tokensArrays) {\n            fieldScore += calculateLogPrior(term, docsWithClassSize) + calculateLogLikelihood(fieldTokensArray, fieldName, term, docsWithClassSize) * fieldName2boost.get(fieldName);\n          }\n          classScore += fieldScore;\n        }\n        assignedClasses.add(new ClassificationResult<>(term.bytes(), classScore));\n      }\n    }\n    return normClassificationResults(assignedClasses);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["38b5bd3ae837751f57f363e9a41b833794222814"],"b21283ed01203901a7257aa4b7f0a0899c86e56e":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","0e0e52292aa6f946f8944886fa7f080d3f14ff2c"],"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","b21283ed01203901a7257aa4b7f0a0899c86e56e"],"0e0e52292aa6f946f8944886fa7f080d3f14ff2c":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","0e0e52292aa6f946f8944886fa7f080d3f14ff2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"38b5bd3ae837751f57f363e9a41b833794222814":["134a24d0cb66520908d88384f1a559875704ed25"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["104a3f62ee393d48b5596de76ed4d9a4e0ea6de7"],"134a24d0cb66520908d88384f1a559875704ed25":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["38b5bd3ae837751f57f363e9a41b833794222814","444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["b21283ed01203901a7257aa4b7f0a0899c86e56e","104a3f62ee393d48b5596de76ed4d9a4e0ea6de7","0e0e52292aa6f946f8944886fa7f080d3f14ff2c","3a7809d1d753b67f48b1a706e17034bf8b624ea3","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"b21283ed01203901a7257aa4b7f0a0899c86e56e":["104a3f62ee393d48b5596de76ed4d9a4e0ea6de7"],"0e0e52292aa6f946f8944886fa7f080d3f14ff2c":["b21283ed01203901a7257aa4b7f0a0899c86e56e","3a7809d1d753b67f48b1a706e17034bf8b624ea3"],"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["134a24d0cb66520908d88384f1a559875704ed25"],"38b5bd3ae837751f57f363e9a41b833794222814":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"134a24d0cb66520908d88384f1a559875704ed25":["38b5bd3ae837751f57f363e9a41b833794222814"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}