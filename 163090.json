{"path":"src/java/org/apache/lucene/search/MultiSearcher#prepareWeight(Query).mjava","commits":[{"id":"402061809f3a4629ea0c449e33e9f94a9772f3c3","date":1113967712,"type":0,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#prepareWeight(Query).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  private Weight prepareWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["94b168f99ebebb8d23639b67c3f4f8939cd4f5be","94b168f99ebebb8d23639b67c3f4f8939cd4f5be"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ceaa9738cad9616d1831286111af106e13e0e4b","date":1114543820,"type":5,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#prepareWeight(Query).mjava","sourceNew":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  private Weight prepareWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"4ceaa9738cad9616d1831286111af106e13e0e4b":["402061809f3a4629ea0c449e33e9f94a9772f3c3"],"402061809f3a4629ea0c449e33e9f94a9772f3c3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4ceaa9738cad9616d1831286111af106e13e0e4b"]},"commit2Childs":{"4ceaa9738cad9616d1831286111af106e13e0e4b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"402061809f3a4629ea0c449e33e9f94a9772f3c3":["4ceaa9738cad9616d1831286111af106e13e0e4b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["402061809f3a4629ea0c449e33e9f94a9772f3c3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}