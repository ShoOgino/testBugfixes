{"path":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter#flush().mjava","commits":[{"id":"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6","date":1411857884,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    if (bufferedDocs.length >= 2 * chunkSize) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    if (bufferedDocs.length >= 2 * chunkSize) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter#flush().mjava","sourceNew":null,"sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    if (bufferedDocs.length >= 2 * chunkSize) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["9bb9a29a5e71a90295f175df8919802993142c9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"9bb9a29a5e71a90295f175df8919802993142c9a":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["9bb9a29a5e71a90295f175df8919802993142c9a"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}