{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","date":1337136355,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new MutableFieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2dee33619431ada2a7a07f5fe2dbd94bac6a460","date":1337274029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null);\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new MutableFieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null);\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null);\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6dce319de558e8b80705326dd04d578f74767d9","date":1337618331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null);\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22b3128eea8c61f8f1f387dac6b3e9504bc8036e","date":1337625491,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    // nocommit\n    // merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"034b8e37ade96af2cef0172233d24b652b432f99","date":1337636665,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    // nocommit\n    // merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, codec, details);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.docCount;\n        final double delRatio = ((double) delCount)/info.info.docCount;\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    merge.info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, 0, codec, details);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63caed6eb28209e181e97822c4c8fdf808884c3b","date":1337712793,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.docCount;\n        final double delRatio = ((double) delCount)/info.info.docCount;\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, codec, details);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.docCount;\n        final double delRatio = ((double) delCount)/info.info.docCount;\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"203d7d3cb7712e10ef33009a63247ae40c302d7a","date":1337798111,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, -1, mergeSegmentName, false, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, 0, -1, mergeSegmentName, false, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.docCount;\n        final double delRatio = ((double) delCount)/info.info.docCount;\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a917aca07a305ab70118a83e84d931503441271","date":1337826487,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, -1, mergeSegmentName, false, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"764b942fd30efcae6e532c19771f32eeeb0037b2","date":1337868546,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, null, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f","date":1338915218,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, details, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9eae2a56dc810a17cf807d831f720dec931a03de","date":1349262073,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b47dabfbaff6449eedcd4321017ab2f73dfa06ab","date":1360797548,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n        merge.totalMergeBytes += info.sizeInBytes();\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b41f996b22bd5518650f897d050088ff808ec03","date":1360969107,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n        merge.totalMergeBytes += info.sizeInBytes();\n      }\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06698263c5125d11a04df79637e84372de4ac797","date":1361452578,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfoPerCommit info : merge.segments) {\n      if (info.info.getDocCount() > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.info.getDocCount();\n        final double delRatio = ((double) delCount)/info.info.getDocCount();\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n        merge.totalMergeBytes += info.sizeInBytes();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d1c249f01722fe2de6d60de2f0aade417fbb638","date":1365517193,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L));\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    merge.info = new SegmentInfoPerCommit(si, 0, -1L);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e072d0b1fc19e0533d8ce432eed245196bca6fde","date":1379265112,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] _mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L));\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"75e4e08ceec867127dcd9913a5ebbc46cf85a28d","date":1379651991,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] _mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77f264c55cbf75404f8601ae7290d69157273a56","date":1380484282,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe","date":1381909398,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes || result.anyNumericDVUpdates) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfoPerCommit info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentInfoPerCommit(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0567bdc5c86c94ced64201187cfcef2417d76dda","date":1400678298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6b7c6630218ed9693cdb8643276513f9f0043f4","date":1406648084,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"057a1793765d068ea9302f1a29e21734ee58d41e","date":1408130117,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3cc329405ce41b8ef462b4cd30611eca1567620","date":1408661910,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null);\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"949847c0040cd70a68222d526cb0da7bf6cbb3c2","date":1410997182,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","date":1414017220,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8521d944f9dfb45692ec28235dbf116d47ef69ba","date":1417535150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.getDocCount());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"618635065f043788c9e034f96ca5cd5cea1b4592","date":1433442044,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","date":1434062028,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c","date":1477166077,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1ee9437ba5a8297220428d48a6bb823d1fcd57b","date":1489137809,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    if (merge.rateLimiter.getAbort()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":["636c73dfa97dd282a3089d4239620475f2633519"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Lock order: IW -> BD\n    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);\n    \n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentCommitInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        pendingNumDocs.addAndGet(-info.info.maxDoc());\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n//    System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW._mergeInit: \" + segString(merge.segments) + \" into \" + si);\n\n    // Lock order: IW -> BD\n    bufferedUpdatesStream.prune(segmentInfos);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"636c73dfa97dd282a3089d4239620475f2633519","date":1499025533,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":["f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","date":1499066739,"type":3,"author":"Adrien Grand","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdates(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy);\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    readerPool.writeDocValuesUpdatesForMerge(merge.segments);\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68ba24d6f9380e2463dbe5130d27502647f64904","date":1554881362,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec,\n        Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14654be3f7a82c9a3c52169e365baa55bfe64f66","date":1587212697,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec,\n        Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L, StringHelper.randomId()));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec,\n        Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  private synchronized void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, config.getCodec(),\n        Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L, StringHelper.randomId()));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == UNBOUNDED_MAX_MERGE_SEGMENTS || merge.maxNumSegments > 0;\n\n    if (tragedy.get() != null) {\n      throw new IllegalStateException(\"this writer hit an unrecoverable error; cannot merge\", tragedy.get());\n    }\n\n    if (merge.info != null) {\n      // mergeInit already done\n      return;\n    }\n\n    merge.mergeInit();\n\n    if (merge.isAborted()) {\n      return;\n    }\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"now apply deletes for \" + merge.segments.size() + \" merging segments\");\n    }\n\n    // Must move the pending doc values updates to disk now, else the newly merged segment will not see them:\n    // TODO: we could fix merging to pull the merged DV iterator so we don't have to move these updates to disk first, i.e. just carry them\n    // in memory:\n    if (readerPool.writeDocValuesUpdatesForMerge(merge.segments)) {\n      checkpoint();\n    }\n    \n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    final String mergeSegmentName = newSegmentName();\n    // We set the min version to null for now, it will be set later by SegmentMerger\n    SegmentInfo si = new SegmentInfo(directoryOrig, Version.LATEST, null, mergeSegmentName, -1, false, codec,\n        Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n    Map<String,String> details = new HashMap<>();\n    details.put(\"mergeMaxNumSegments\", \"\" + merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(si, SOURCE_MERGE, details);\n    merge.setMergeInfo(new SegmentCommitInfo(si, 0, 0, -1L, -1L, -1L, StringHelper.randomId()));\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.info.name + \" \" + segString(merge.segments));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["06698263c5125d11a04df79637e84372de4ac797"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa":["618635065f043788c9e034f96ca5cd5cea1b4592"],"636c73dfa97dd282a3089d4239620475f2633519":["28288370235ed02234a64753cdbf0c6ec096304a"],"3b41f996b22bd5518650f897d050088ff808ec03":["9eae2a56dc810a17cf807d831f720dec931a03de","b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"b70042a8a492f7054d480ccdd2be9796510d4327":["1926100d9b67becc9701c54266fee3ba7878a5f0","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"79700663e164dece87bed4adfd3e28bab6cb1385":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"77f264c55cbf75404f8601ae7290d69157273a56":["75e4e08ceec867127dcd9913a5ebbc46cf85a28d"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"299a2348fa24151d150182211b6208a38e5e3450":["5faf65b6692f15cca0f87bf8666c87899afc619f","79700663e164dece87bed4adfd3e28bab6cb1385"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"06698263c5125d11a04df79637e84372de4ac797":["b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"b0267c69e2456a3477a1ad785723f2135da3117e":["79700663e164dece87bed4adfd3e28bab6cb1385"],"30c8e5574b55d57947e989443dfde611646530ee":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","636c73dfa97dd282a3089d4239620475f2633519"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["92212fd254551a0b1156aafc3a1a6ed1a43932ad","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b"],"e3cc329405ce41b8ef462b4cd30611eca1567620":["057a1793765d068ea9302f1a29e21734ee58d41e"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["68ba24d6f9380e2463dbe5130d27502647f64904"],"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","0567bdc5c86c94ced64201187cfcef2417d76dda"],"f592209545c71895260367152601e9200399776d":["1926100d9b67becc9701c54266fee3ba7878a5f0","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["636c73dfa97dd282a3089d4239620475f2633519"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"6a917aca07a305ab70118a83e84d931503441271":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["6a917aca07a305ab70118a83e84d931503441271"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c"],"0ad30c6a479e764150a3316e57263319775f1df2":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","3d33e731a93d4b57e662ff094f64f94a745422d4"],"b7605579001505896d48b07160075a5c8b8e128e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","0567bdc5c86c94ced64201187cfcef2417d76dda"],"057a1793765d068ea9302f1a29e21734ee58d41e":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["5faf65b6692f15cca0f87bf8666c87899afc619f","b0267c69e2456a3477a1ad785723f2135da3117e"],"4356000e349e38c9fb48034695b7c309abd54557":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["77f264c55cbf75404f8601ae7290d69157273a56"],"b06445ae1731e049327712db0454e5643ca9b7fe":["299a2348fa24151d150182211b6208a38e5e3450","b0267c69e2456a3477a1ad785723f2135da3117e"],"68ba24d6f9380e2463dbe5130d27502647f64904":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"22b3128eea8c61f8f1f387dac6b3e9504bc8036e":["b6dce319de558e8b80705326dd04d578f74767d9"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":["28288370235ed02234a64753cdbf0c6ec096304a","636c73dfa97dd282a3089d4239620475f2633519"],"b47dabfbaff6449eedcd4321017ab2f73dfa06ab":["9eae2a56dc810a17cf807d831f720dec931a03de"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"75e4e08ceec867127dcd9913a5ebbc46cf85a28d":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"9eae2a56dc810a17cf807d831f720dec931a03de":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["034b8e37ade96af2cef0172233d24b652b432f99"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"28288370235ed02234a64753cdbf0c6ec096304a":["31741cf1390044e38a2ec3127cf302ba841bfd75","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","d470c8182e92b264680e34081b75e70a9f2b3c89"],"618635065f043788c9e034f96ca5cd5cea1b4592":["b0267c69e2456a3477a1ad785723f2135da3117e"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","764b942fd30efcae6e532c19771f32eeeb0037b2"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"b6dce319de558e8b80705326dd04d578f74767d9":["4356000e349e38c9fb48034695b7c309abd54557"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"034b8e37ade96af2cef0172233d24b652b432f99":["22b3128eea8c61f8f1f387dac6b3e9504bc8036e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"636c73dfa97dd282a3089d4239620475f2633519":["30c8e5574b55d57947e989443dfde611646530ee","845b760a99e5f369fcd0a5d723a87b8def6a3f56","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35"],"3b41f996b22bd5518650f897d050088ff808ec03":[],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["299a2348fa24151d150182211b6208a38e5e3450","b0267c69e2456a3477a1ad785723f2135da3117e"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["057a1793765d068ea9302f1a29e21734ee58d41e"],"77f264c55cbf75404f8601ae7290d69157273a56":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"299a2348fa24151d150182211b6208a38e5e3450":["b06445ae1731e049327712db0454e5643ca9b7fe"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["6a917aca07a305ab70118a83e84d931503441271"],"06698263c5125d11a04df79637e84372de4ac797":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"b0267c69e2456a3477a1ad785723f2135da3117e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","618635065f043788c9e034f96ca5cd5cea1b4592"],"30c8e5574b55d57947e989443dfde611646530ee":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["30c8e5574b55d57947e989443dfde611646530ee"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["4356000e349e38c9fb48034695b7c309abd54557"],"e3cc329405ce41b8ef462b4cd30611eca1567620":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f":["9eae2a56dc810a17cf807d831f720dec931a03de"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"f592209545c71895260367152601e9200399776d":[],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"6a917aca07a305ab70118a83e84d931503441271":["764b942fd30efcae6e532c19771f32eeeb0037b2"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"b7605579001505896d48b07160075a5c8b8e128e":[],"057a1793765d068ea9302f1a29e21734ee58d41e":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","28288370235ed02234a64753cdbf0c6ec096304a"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"4356000e349e38c9fb48034695b7c309abd54557":["b6dce319de558e8b80705326dd04d578f74767d9"],"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"68ba24d6f9380e2463dbe5130d27502647f64904":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"22b3128eea8c61f8f1f387dac6b3e9504bc8036e":["034b8e37ade96af2cef0172233d24b652b432f99"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1"],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":[],"b47dabfbaff6449eedcd4321017ab2f73dfa06ab":["3b41f996b22bd5518650f897d050088ff808ec03","06698263c5125d11a04df79637e84372de4ac797"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["75e4e08ceec867127dcd9913a5ebbc46cf85a28d"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e","0567bdc5c86c94ced64201187cfcef2417d76dda"],"75e4e08ceec867127dcd9913a5ebbc46cf85a28d":["77f264c55cbf75404f8601ae7290d69157273a56"],"9eae2a56dc810a17cf807d831f720dec931a03de":["3b41f996b22bd5518650f897d050088ff808ec03","b47dabfbaff6449eedcd4321017ab2f73dfa06ab"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["d6b7c6630218ed9693cdb8643276513f9f0043f4","a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"28288370235ed02234a64753cdbf0c6ec096304a":["636c73dfa97dd282a3089d4239620475f2633519","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"618635065f043788c9e034f96ca5cd5cea1b4592":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","68ba24d6f9380e2463dbe5130d27502647f64904"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"b6dce319de558e8b80705326dd04d578f74767d9":["22b3128eea8c61f8f1f387dac6b3e9504bc8036e"],"034b8e37ade96af2cef0172233d24b652b432f99":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3b41f996b22bd5518650f897d050088ff808ec03","b70042a8a492f7054d480ccdd2be9796510d4327","30c8e5574b55d57947e989443dfde611646530ee","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","a656b32c3aa151037a8c52e9b134acc3cbf482bc","f592209545c71895260367152601e9200399776d","b7605579001505896d48b07160075a5c8b8e128e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}