{"path":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);\n    customType2.setStoreTermVectors(true);\n    customType2.setStoreTermVectorPositions(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType2));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType2));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType2));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);\n    customType3.setStoreTermVectors(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType3));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType3));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6eb141f80638abdb6ffaa5149877f36ea39b6ad5","date":1315714072,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);\n    customType2.setStoreTermVectors(true);\n    customType2.setStoreTermVectorPositions(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType2));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType2));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType2));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);\n    customType3.setStoreTermVectors(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType3));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType3));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc;\n\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);\n    customType2.setStoreTermVectors(true);\n    customType2.setStoreTermVectorPositions(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType2));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType2));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType2));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);\n    customType3.setStoreTermVectors(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType3));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType3));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);\n    customType2.setStoreTermVectors(true);\n    customType2.setStoreTermVectorPositions(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType2));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType2));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType2));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);\n    customType3.setStoreTermVectors(true);\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType3));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", customType3));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", TextField.TYPE_UNSTORED));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(newField(\"a\", \"a b a c a d a e a f a g a h a\", customType));\n    doc.add(newField(\"b\", \"a b c b d b e b f b g b h b\", TextField.TYPE_UNSTORED));\n    doc.add(newField(\"c\", \"a c b c d c e c f c g c h c\", customType3));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0].utf8ToString());\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2].utf8ToString());\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["d572389229127c297dd1fa5ce4758e1cec41e799"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["132903c28af3aa6f67284b78de91c0f0a99488c2","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cc749c053615f5871f3b95715fe292f34e70a53":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"d572389229127c297dd1fa5ce4758e1cec41e799":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"5f4e87790277826a2aea119328600dfb07761f32":["d572389229127c297dd1fa5ce4758e1cec41e799","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"962d04139994fce5193143ef35615499a9a96d78":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","132903c28af3aa6f67284b78de91c0f0a99488c2"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["132903c28af3aa6f67284b78de91c0f0a99488c2","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["3cc749c053615f5871f3b95715fe292f34e70a53"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"d572389229127c297dd1fa5ce4758e1cec41e799":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"962d04139994fce5193143ef35615499a9a96d78":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["962d04139994fce5193143ef35615499a9a96d78"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d572389229127c297dd1fa5ce4758e1cec41e799"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}