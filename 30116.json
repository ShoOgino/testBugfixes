{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","sourceNew":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","sourceOld":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool#randomizeManyInputFiles(Configuration,Path,Path,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * To uniformly spread load across all mappers we randomize fullInputList\n   * with a separate small Mapper & Reducer preprocessing step. This way\n   * each input line ends up on a random position in the output file list.\n   * Each mapper indexes a disjoint consecutive set of files such that each\n   * set has roughly the same size, at least from a probabilistic\n   * perspective.\n   * \n   * For example an input file with the following input list of URLs:\n   * \n   * A\n   * B\n   * C\n   * D\n   * \n   * might be randomized into the following output list of URLs:\n   * \n   * C\n   * A\n   * D\n   * B\n   * \n   * The implementation sorts the list of lines by randomly generated numbers.\n   */\n  private Job randomizeManyInputFiles(Configuration baseConfig, Path fullInputList, Path outputStep2Dir, int numLinesPerSplit) \n      throws IOException {\n    \n    Job job2 = Job.getInstance(baseConfig);\n    job2.setJarByClass(getClass());\n    job2.setJobName(getClass().getName() + \"/\" + Utils.getShortClassName(LineRandomizerMapper.class));\n    job2.setInputFormatClass(NLineInputFormat.class);\n    NLineInputFormat.addInputPath(job2, fullInputList);\n    NLineInputFormat.setNumLinesPerSplit(job2, numLinesPerSplit);          \n    job2.setMapperClass(LineRandomizerMapper.class);\n    job2.setReducerClass(LineRandomizerReducer.class);\n    job2.setOutputFormatClass(TextOutputFormat.class);\n    FileOutputFormat.setOutputPath(job2, outputStep2Dir);\n    job2.setNumReduceTasks(1);\n    job2.setOutputKeyClass(LongWritable.class);\n    job2.setOutputValueClass(Text.class);\n    return job2;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["12109b652e9210b8d58fca47f6c4a725d058a58e","74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}