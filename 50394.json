{"path":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["7d16aff6229cca84309d03d047cd718946bd4b43"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.shutdown();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.shutdown();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.shutdown();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.shutdown();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbaae1c00d39df2c872bbe043af26d02d3818313","date":1409657064,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses pulsing codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1862266772deb28cdcb7d996b64d2177022687","date":1453077824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"470eaac3a77cf637b62126a5408b178d7be93eb1","date":1531830722,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.count(new TermQuery(new Term(\"field1\", \"standard\"))));\n    assertEquals(NUM_DOCS-1, s.count(new TermQuery(new Term(\"field2\", \"memory\"))));\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.count(new TermQuery(new Term(\"field1\", \"standard\"))));\n    assertEquals(NUM_DOCS-2, s.count(new TermQuery(new Term(\"field2\", \"memory\"))));\n    assertEquals(1, s.count(new TermQuery(new Term(\"id\", \"76\"))));\n    assertEquals(0, s.count(new TermQuery(new Term(\"id\", \"77\"))));\n    assertEquals(0, s.count(new TermQuery(new Term(\"id\", \"44\"))));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.count(new TermQuery(new Term(\"field1\", \"standard\"))));\n    assertEquals(NUM_DOCS-1, s.count(new TermQuery(new Term(\"field2\", \"memory\"))));\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.count(new TermQuery(new Term(\"field1\", \"standard\"))));\n    assertEquals(NUM_DOCS-2, s.count(new TermQuery(new Term(\"field2\", \"memory\"))));\n    assertEquals(1, s.count(new TermQuery(new Term(\"id\", \"76\"))));\n    assertEquals(0, s.count(new TermQuery(new Term(\"id\", \"77\"))));\n    assertEquals(0, s.count(new TermQuery(new Term(\"id\", \"44\"))));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    BaseDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newTextField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO));\n    // uses memory codec:\n    Field field2 = newTextField(\"field2\", \"this field uses the memory codec as the test\", Field.Store.NO);\n    doc.add(field2);\n    \n    Field idField = newStringField(\"id\", \"\", Field.Store.NO);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setStringValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = DirectoryReader.open(w);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = DirectoryReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"memory\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"fbaae1c00d39df2c872bbe043af26d02d3818313":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["2a1862266772deb28cdcb7d996b64d2177022687","470eaac3a77cf637b62126a5408b178d7be93eb1"],"2a1862266772deb28cdcb7d996b64d2177022687":["fbaae1c00d39df2c872bbe043af26d02d3818313"],"aba371508186796cc6151d8223a5b4e16d02e26e":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"470eaac3a77cf637b62126a5408b178d7be93eb1":["2a1862266772deb28cdcb7d996b64d2177022687"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["470eaac3a77cf637b62126a5408b178d7be93eb1"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"fbaae1c00d39df2c872bbe043af26d02d3818313":["2a1862266772deb28cdcb7d996b64d2177022687"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"2a1862266772deb28cdcb7d996b64d2177022687":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","470eaac3a77cf637b62126a5408b178d7be93eb1"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["aba371508186796cc6151d8223a5b4e16d02e26e","ae14298f4eec6d5faee6a149f88ba57d14a6f21a","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["fbaae1c00d39df2c872bbe043af26d02d3818313"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"470eaac3a77cf637b62126a5408b178d7be93eb1":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}