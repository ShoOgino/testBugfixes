{"path":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","commits":[{"id":"4b3d16cba9355e2e97962eb1c441bbd0b6735c15","date":1357426290,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"116a468ed771d87fd94eb1350dd2d42bbf0b262f","date":1365791134,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.shutdown();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.shutdown();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"381618eac2691bb34ab9a3fca76ad55c6274517e","date":1495564791,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":null,"sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":null,"sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["d0ef034a4f10871667ae75181537775ddcf8ade4","381618eac2691bb34ab9a3fca76ad55c6274517e"],"6613659748fe4411a7dcf85266e55db1f95f7315":["116a468ed771d87fd94eb1350dd2d42bbf0b262f"],"116a468ed771d87fd94eb1350dd2d42bbf0b262f":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["381618eac2691bb34ab9a3fca76ad55c6274517e"]},"commit2Childs":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","116a468ed771d87fd94eb1350dd2d42bbf0b262f"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"116a468ed771d87fd94eb1350dd2d42bbf0b262f":["6613659748fe4411a7dcf85266e55db1f95f7315"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["381618eac2691bb34ab9a3fca76ad55c6274517e","e9017cf144952056066919f1ebc7897ff9bd71b1"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}