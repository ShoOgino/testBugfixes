{"path":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0158ced21948b6626f733c1c42c1e18d94449789","date":1462994341,"type":3,"author":"Bartosz Krasiński","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback intersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback intersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#intersectionSize(int[],int[]).mjava","sourceNew":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback intersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","sourceOld":"  public static int intersectionSize(int[] smallerSortedList, int[] biggerSortedList) {\n    final int a[] = smallerSortedList;\n    final int b[] = biggerSortedList;\n\n    // The next doc we are looking for will be much closer to the last position we tried\n    // than it will be to the midpoint between last and high... so probe ahead using\n    // a function of the ratio of the sizes of the sets.\n    int step = (b.length/a.length)+1;\n\n    // Since the majority of probes should be misses, we'll already be above the last probe\n    // and shouldn't need to move larger than the step size on average to step over our target (and thus lower\n    // the high upper bound a lot.)... but if we don't go over our target, it's a big miss... so double it.\n    step = step + step;\n\n    // FUTURE: come up with a density such that target * density == likely position?\n    // then check step on one side or the other?\n    // (density could be cached in the DocSet)... length/maxDoc\n\n    // FUTURE: try partitioning like a sort algorithm.  Pick the midpoint of the big\n    // array, find where that should be in the small array, and then recurse with\n    // the top and bottom half of both arrays until they are small enough to use\n    // a fallback insersection method.\n    // NOTE: I tried this and it worked, but it was actually slower than this current\n    // highly optimized approach.\n\n    int icount = 0;\n    int low = 0;\n    int max = b.length-1;\n\n    for (int i=0; i<a.length; i++) {\n      int doca = a[i];\n\n      int high = max;\n\n      int probe = low + step;     // 40% improvement!\n\n      // short linear probe to see if we can drop the high pointer in one big jump.\n      if (probe<high) {\n        if (b[probe]>=doca) {\n          // success!  we cut down the upper bound by a lot in one step!\n          high=probe;\n        } else {\n          // relative failure... we get to move the low pointer, but not my much\n          low=probe+1;\n\n          // reprobe worth it? it appears so!\n          probe = low + step;\n          if (probe<high) {\n            if (b[probe]>=doca) {\n              high=probe;\n            } else {\n              low=probe+1;\n            }\n          }\n        }\n      }\n\n      // binary search the rest of the way\n      while (low <= high) {\n        int mid = (low+high) >>> 1;\n        int docb = b[mid];\n\n        if (docb < doca) {\n          low = mid+1;\n        }\n        else if (docb > doca) {\n          high = mid-1;\n        }\n        else {\n          icount++;\n          low = mid+1;  // found it, so start at next element\n          break;\n        }\n      }\n      // Didn't find it... low is now positioned on the insertion point,\n      // which is higher than what we were looking for, so continue using\n      // the same low point.\n    }\n\n    return icount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["c26f00b574427b55127e869b935845554afde1fa","19275ba31e621f6da1b83bf13af75233876fd3d4"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"19275ba31e621f6da1b83bf13af75233876fd3d4":["c26f00b574427b55127e869b935845554afde1fa"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["19275ba31e621f6da1b83bf13af75233876fd3d4","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["19275ba31e621f6da1b83bf13af75233876fd3d4","0158ced21948b6626f733c1c42c1e18d94449789"],"0158ced21948b6626f733c1c42c1e18d94449789":["19275ba31e621f6da1b83bf13af75233876fd3d4"]},"commit2Childs":{"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"c26f00b574427b55127e869b935845554afde1fa":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","19275ba31e621f6da1b83bf13af75233876fd3d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","c26f00b574427b55127e869b935845554afde1fa","a258fbb26824fd104ed795e5d9033d2d040049ee"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","0158ced21948b6626f733c1c42c1e18d94449789"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"0158ced21948b6626f733c1c42c1e18d94449789":["d470c8182e92b264680e34081b75e70a9f2b3c89"]},"heads":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","a258fbb26824fd104ed795e5d9033d2d040049ee","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}