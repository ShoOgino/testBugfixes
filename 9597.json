{"path":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","commits":[{"id":"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","date":1349450075,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"/dev/null","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene40Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene40Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4ce24aa081e44190692bbebc8aead342ad7060e8"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"879f5e74b68e6faa45251db337bb1e1a9e9be647","date":1350008268,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene40Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene40Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7492bcb52be51e55d596134b95b2e53cc4ffb91","date":1350223278,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene40Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene40Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db4fdbf3d262768eabc027cd8321edca0cd11fa8","date":1350574784,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene40Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene40Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb04ae9ea4b411d2adefa88bcd40141cfe9a711d","date":1351706460,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    iwConf.setCodec(CompressingCodec.randomInstance(random()));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final byte[] arr = new byte[length];\n        final int max = rarely() ? 256 : 2;\n        for (int k = 0; k < length; ++k) {\n          arr[k] = (byte) random().nextInt(max);\n        }\n        data[i][j] = arr;\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab","date":1354403647,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":null,"sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7492bcb52be51e55d596134b95b2e53cc4ffb91":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","879f5e74b68e6faa45251db337bb1e1a9e9be647"],"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["fb04ae9ea4b411d2adefa88bcd40141cfe9a711d","1a51ec81f1fd009bf893bd88ec1c7b964fae6fab"],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","c7492bcb52be51e55d596134b95b2e53cc4ffb91"],"fb04ae9ea4b411d2adefa88bcd40141cfe9a711d":["c7492bcb52be51e55d596134b95b2e53cc4ffb91"],"879f5e74b68e6faa45251db337bb1e1a9e9be647":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1a51ec81f1fd009bf893bd88ec1c7b964fae6fab"],"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab":["fb04ae9ea4b411d2adefa88bcd40141cfe9a711d"]},"commit2Childs":{"c7492bcb52be51e55d596134b95b2e53cc4ffb91":["db4fdbf3d262768eabc027cd8321edca0cd11fa8","fb04ae9ea4b411d2adefa88bcd40141cfe9a711d"],"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa":["c7492bcb52be51e55d596134b95b2e53cc4ffb91","db4fdbf3d262768eabc027cd8321edca0cd11fa8","879f5e74b68e6faa45251db337bb1e1a9e9be647"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":[],"fb04ae9ea4b411d2adefa88bcd40141cfe9a711d":["407687e67faf6e1f02a211ca078d8e3eed631027","1a51ec81f1fd009bf893bd88ec1c7b964fae6fab"],"879f5e74b68e6faa45251db337bb1e1a9e9be647":["c7492bcb52be51e55d596134b95b2e53cc4ffb91"],"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","db4fdbf3d262768eabc027cd8321edca0cd11fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}