{"path":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","commits":[{"id":"6620df8541b174097b1133a4fc370adb2e570524","date":1319544675,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    DocIdSet docIdSet = cache.get(reader, coreKey);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      // cache miss: we use no acceptDocs here\n      // (this saves time on building DocIdSet, the acceptDocs will be applied on the cached set)\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, null/**!!!*/), reader);\n      cache.put(coreKey, docIdSet);\n    }\n    \n    return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc0528274cde759b2d3f75b55794edeae6093533","date":1323799309,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future we don't want to over-cache:\n    final boolean doCacheSubAcceptDocs = recacheDeletes && acceptDocs == reader.getLiveDocs();\n\n    final Bits subAcceptDocs;\n    if (doCacheSubAcceptDocs) {\n      subAcceptDocs = acceptDocs;\n    } else {\n      subAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(reader, coreKey, subAcceptDocs);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, subAcceptDocs), reader);\n      cache.put(coreKey, subAcceptDocs, docIdSet);\n    }\n\n    if (doCacheSubAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    DocIdSet docIdSet = cache.get(reader, coreKey);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      // cache miss: we use no acceptDocs here\n      // (this saves time on building DocIdSet, the acceptDocs will be applied on the cached set)\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, null/**!!!*/), reader);\n      cache.put(coreKey, docIdSet);\n    }\n    \n    return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ceb6a6c707ada1df8bde804e25c98668e699a18","date":1323800602,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future we don't want to over-cache:\n    final boolean doCacheSubAcceptDocs = recacheDeletes && acceptDocs == reader.getLiveDocs();\n\n    final Bits subAcceptDocs;\n    if (doCacheSubAcceptDocs) {\n      subAcceptDocs = acceptDocs;\n    } else {\n      subAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(reader, coreKey, subAcceptDocs);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, subAcceptDocs), reader);\n      cache.put(coreKey, subAcceptDocs, docIdSet);\n    }\n\n    if (doCacheSubAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    DocIdSet docIdSet = cache.get(reader, coreKey);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      // cache miss: we use no acceptDocs here\n      // (this saves time on building DocIdSet, the acceptDocs will be applied on the cached set)\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, null/**!!!*/), reader);\n      cache.put(coreKey, docIdSet);\n    }\n    \n    return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd8a5ae76e8111ea49a39aef28be5fbd3dab382b","date":1323874359,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future we don't want to over-cache:\n    final boolean doCacheSubAcceptDocs = recacheDeletes && acceptDocs == reader.getLiveDocs();\n\n    final Bits subAcceptDocs;\n    if (doCacheSubAcceptDocs) {\n      subAcceptDocs = acceptDocs;\n    } else {\n      subAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(reader, subAcceptDocs);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, subAcceptDocs), reader);\n      cache.put(reader, subAcceptDocs, docIdSet);\n    }\n\n    if (doCacheSubAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n    final Object coreKey = reader.getCoreCacheKey();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future we don't want to over-cache:\n    final boolean doCacheSubAcceptDocs = recacheDeletes && acceptDocs == reader.getLiveDocs();\n\n    final Bits subAcceptDocs;\n    if (doCacheSubAcceptDocs) {\n      subAcceptDocs = acceptDocs;\n    } else {\n      subAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(reader, coreKey, subAcceptDocs);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, subAcceptDocs), reader);\n      cache.put(coreKey, subAcceptDocs, docIdSet);\n    }\n\n    if (doCacheSubAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7d9cdceb051035b069ec36719809d0cd837777b","date":1323905856,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future we don't want to over-cache:\n    final boolean doCacheSubAcceptDocs = recacheDeletes && acceptDocs == reader.getLiveDocs();\n\n    final Bits subAcceptDocs;\n    if (doCacheSubAcceptDocs) {\n      subAcceptDocs = acceptDocs;\n    } else {\n      subAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(reader, subAcceptDocs);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, subAcceptDocs), reader);\n      cache.put(reader, subAcceptDocs, docIdSet);\n    }\n\n    if (doCacheSubAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2725b2d479964ea5aaea0ba4ae2634716f3ec26c","date":1327188170,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicIndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6fff8f4b218bd0626afcdce82027bafeb84a50a4","date":1327229950,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicIndexReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicIndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicIndexReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final IndexReader reader = context.reader;\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/CachingWrapperFilter#getDocIdSet(AtomicReaderContext,Bits).mjava","sourceNew":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {\n    final AtomicReader reader = context.reader();\n\n    // Only cache if incoming acceptDocs is == live docs;\n    // if Lucene passes in more interesting acceptDocs in\n    // the future (@UweSays: it already does when you chain FilteredQuery) we don't want to over-cache:\n    final Bits liveDocs = reader.getLiveDocs();\n    final boolean doCacheAcceptDocs = (recacheDeletes && acceptDocs == liveDocs);\n\n    final Object key;\n    final Bits cacheAcceptDocs;\n    if (doCacheAcceptDocs) {\n      assert acceptDocs == liveDocs;\n      key = reader.getCombinedCoreAndDeletesKey();\n      cacheAcceptDocs = acceptDocs;\n    } else {\n      key = reader.getCoreCacheKey();\n      cacheAcceptDocs = null;\n    }\n\n    DocIdSet docIdSet = cache.get(key);\n    if (docIdSet != null) {\n      hitCount++;\n    } else {\n      missCount++;\n      docIdSet = docIdSetToCache(filter.getDocIdSet(context, cacheAcceptDocs), reader);\n      cache.put(key, docIdSet);\n    }\n\n    if (doCacheAcceptDocs) {\n      return docIdSet;\n    } else {\n      return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4ceb6a6c707ada1df8bde804e25c98668e699a18":["6620df8541b174097b1133a4fc370adb2e570524","bc0528274cde759b2d3f75b55794edeae6093533"],"6620df8541b174097b1133a4fc370adb2e570524":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bc0528274cde759b2d3f75b55794edeae6093533":["6620df8541b174097b1133a4fc370adb2e570524"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["6fff8f4b218bd0626afcdce82027bafeb84a50a4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6fff8f4b218bd0626afcdce82027bafeb84a50a4":["2725b2d479964ea5aaea0ba4ae2634716f3ec26c"],"cd8a5ae76e8111ea49a39aef28be5fbd3dab382b":["bc0528274cde759b2d3f75b55794edeae6093533"],"b7d9cdceb051035b069ec36719809d0cd837777b":["cd8a5ae76e8111ea49a39aef28be5fbd3dab382b"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["b7d9cdceb051035b069ec36719809d0cd837777b","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2725b2d479964ea5aaea0ba4ae2634716f3ec26c":["b7d9cdceb051035b069ec36719809d0cd837777b"]},"commit2Childs":{"4ceb6a6c707ada1df8bde804e25c98668e699a18":[],"6620df8541b174097b1133a4fc370adb2e570524":["4ceb6a6c707ada1df8bde804e25c98668e699a18","bc0528274cde759b2d3f75b55794edeae6093533"],"bc0528274cde759b2d3f75b55794edeae6093533":["4ceb6a6c707ada1df8bde804e25c98668e699a18","cd8a5ae76e8111ea49a39aef28be5fbd3dab382b"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6620df8541b174097b1133a4fc370adb2e570524"],"6fff8f4b218bd0626afcdce82027bafeb84a50a4":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd8a5ae76e8111ea49a39aef28be5fbd3dab382b":["b7d9cdceb051035b069ec36719809d0cd837777b"],"b7d9cdceb051035b069ec36719809d0cd837777b":["5cab9a86bd67202d20b6adc463008c8e982b070a","2725b2d479964ea5aaea0ba4ae2634716f3ec26c"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2725b2d479964ea5aaea0ba4ae2634716f3ec26c":["6fff8f4b218bd0626afcdce82027bafeb84a50a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4ceb6a6c707ada1df8bde804e25c98668e699a18","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}