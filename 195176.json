{"path":"solr/core/src/test/org/apache/solr/handler/TestStressThreadBackup#testSnapshotsAndBackupsDuringConcurrentCommitsAndOptimizes(BackupAPIImpl).mjava","commits":[{"id":"19f02bb04467ed179738a398a7da80bbbe161c16","date":1573660732,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/TestStressThreadBackup#testSnapshotsAndBackupsDuringConcurrentCommitsAndOptimizes(BackupAPIImpl).mjava","pathOld":"/dev/null","sourceNew":"  public void testSnapshotsAndBackupsDuringConcurrentCommitsAndOptimizes(final BackupAPIImpl impl) throws Exception {\n    final int numBackupIters = 20; // don't use 'atLeast', we don't want to blow up on nightly\n    \n    final AtomicReference<Throwable> heavyCommitFailure = new AtomicReference<>();\n    final AtomicBoolean keepGoing = new AtomicBoolean(true);\n\n    // this thread will do nothing but add/commit new 'dummy' docs over and over again as fast as possible\n    // to create a lot of index churn w/ segment merging\n    final Thread heavyCommitting = new Thread() {\n      public void run() {\n        try {\n          int docIdCounter = 0;\n          while (keepGoing.get()) {\n            docIdCounter++;\n\n            final UpdateRequest req = new UpdateRequest().add(makeDoc(\"dummy_\" + docIdCounter, \"dummy\"));\n            // always commit to force lots of new segments\n            req.setParam(UpdateParams.COMMIT,\"true\");\n            req.setParam(UpdateParams.OPEN_SEARCHER,\"false\");           // we don't care about searching\n\n            // frequently forceMerge to ensure segments are frequently deleted\n            if (0 == (docIdCounter % 13)) {                             // arbitrary\n              req.setParam(UpdateParams.OPTIMIZE, \"true\");\n              req.setParam(UpdateParams.MAX_OPTIMIZE_SEGMENTS, \"5\");    // arbitrary\n            }\n            \n            log.info(\"Heavy Committing #{}: {}\", docIdCounter, req);\n            final UpdateResponse rsp = req.process(coreClient);\n            assertEquals(\"Dummy Doc#\" + docIdCounter + \" add status: \" + rsp.toString(), 0, rsp.getStatus());\n                   \n          }\n        } catch (Throwable t) {\n          heavyCommitFailure.set(t);\n        }\n      }\n    };\n    \n    heavyCommitting.start();\n    try {\n      // now have the \"main\" test thread try to take a serious of backups/snapshots\n      // while adding other \"real\" docs\n      \n      final Queue<String> namedSnapshots = new LinkedList<>();\n\n      // NOTE #1: start at i=1 for 'id' & doc counting purposes...\n      // NOTE #2: abort quickly if the oher thread reports a heavyCommitFailure...\n      for (int i = 1; (i <= numBackupIters && null == heavyCommitFailure.get()); i++) {\n        \n        // in each iteration '#i', the commit we create should have exactly 'i' documents in\n        // it with the term 'type_s:real' (regardless of what the other thread does with dummy docs)\n        \n        // add & commit a doc #i\n        final UpdateRequest req = new UpdateRequest().add(makeDoc(\"doc_\" + i, \"real\"));\n        req.setParam(UpdateParams.COMMIT,\"true\"); // make immediately available for backup\n        req.setParam(UpdateParams.OPEN_SEARCHER,\"false\"); // we don't care about searching\n        \n        final UpdateResponse rsp = req.process(coreClient);\n        assertEquals(\"Real Doc#\" + i + \" add status: \" + rsp.toString(), 0, rsp.getStatus());\n\n        // create a backup of the 'current' index\n        impl.makeBackup(\"backup_currentAt_\" + i);\n        \n        // verify backup is valid and has the number of 'real' docs we expect...\n        validateBackup(\"backup_currentAt_\" + i);\n\n        // occasionally make a \"snapshot_i\", add it to 'namedSnapshots'\n        // NOTE: we don't want to do this too often, or the SnapShotMetadataManager will protect\n        // too many segment files \"long term\".  It's more important to stress the thread contention\n        // between backups calling save/release vs the DelPolicy trying to delete segments\n        if ( 0 == random().nextInt(7 + namedSnapshots.size()) ) {\n          final String snapshotName = \"snapshot_\" + i;\n          log.info(\"Creating snapshot: {}\", snapshotName);\n          impl.makeSnapshot(snapshotName);\n          namedSnapshots.add(snapshotName);\n        }\n\n        // occasionally make a backup of a snapshot and remove it\n        // the odds of doing this increase based on how many snapshots currently exist,\n        // and how few iterations we have left\n        if (3 < namedSnapshots.size() &&\n            random().nextInt(3 + numBackupIters - i) < random().nextInt(namedSnapshots.size())) {\n\n          assert 0 < namedSnapshots.size() : \"Someone broke the conditionl\";\n          final String snapshotName = namedSnapshots.poll();\n          final String backupName = \"backup_as_of_\" + snapshotName;\n          log.info(\"Creating {} from {} in iter={}\", backupName, snapshotName, i);\n          impl.makeBackup(backupName, snapshotName);\n          log.info(\"Deleting {} in iter={}\", snapshotName, i);\n          impl.deleteSnapshot(snapshotName);\n\n          validateBackup(backupName);\n\n          // NOTE: we can't directly compare our backups, because the stress thread\n          // may have added/committed documents\n          // ie: backup_as_of_snapshot_4 and backup_currentAt_4 should have the same 4 \"real\"\n          // documents, but they may have other commits that affect the data files\n          // between when the backup was taken and when the snapshot was taken\n\n        }\n      }\n      \n    } finally {\n      keepGoing.set(false);\n      heavyCommitting.join();\n    }\n    assertNull(heavyCommitFailure.get());\n\n    { log.info(\"Done with (concurrent) updates, Deleting all docs...\");\n      final UpdateRequest delAll = new UpdateRequest().deleteByQuery(\"*:*\");\n      delAll.setParam(UpdateParams.COMMIT,\"true\");\n      delAll.setParam(UpdateParams.OPTIMIZE, \"true\");\n      delAll.setParam(UpdateParams.MAX_OPTIMIZE_SEGMENTS, \"1\"); // purge as many files as possible\n      final UpdateResponse delRsp = delAll.process(coreClient);\n      assertEquals(\"dellAll status: \" + delRsp.toString(), 0, delRsp.getStatus());\n    }\n\n    { // Validate some backups at random...\n      final int numBackupsToCheck = atLeast(1);\n      log.info(\"Validating {} random backups to ensure they are un-affected by deleting all docs...\",\n               numBackupsToCheck);\n      final List<File> allBackups = Arrays.asList(backupDir.listFiles());\n      // insure consistent (arbitrary) ordering before shuffling\n      Collections.sort(allBackups); \n      Collections.shuffle(allBackups, random());\n      for (int i = 0; i < numBackupsToCheck; i++) {\n        final File backup = allBackups.get(i);\n        validateBackup(backup);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"19f02bb04467ed179738a398a7da80bbbe161c16":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["19f02bb04467ed179738a398a7da80bbbe161c16"]},"commit2Childs":{"19f02bb04467ed179738a398a7da80bbbe161c16":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["19f02bb04467ed179738a398a7da80bbbe161c16"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}