{"path":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":["4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, 0);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, 0);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, 0);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, 0);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, 0);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5","date":1376515204,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2fb55c0777755badd3b46d8140f3d4301febed","date":1398881584,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a16b40feb4e6e0d55c1716733bde48296bedd20","date":1400540388,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        if (docsEnum == null) {\n          for(int docID : docs.get(term)) {\n            assert deleted.contains(docID);\n          }\n        } else {\n          for(int docID : docs.get(term)) {\n            if (!deleted.contains(docID)) {\n              assertEquals(docID, docsEnum.nextDoc());\n            }\n          }\n          assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n        }\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45a621dd071a902e1fd30367200d7bbbea037706","date":1400686915,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        if (docsEnum == null) {\n          for(int docID : docs.get(term)) {\n            assert deleted.contains(docID);\n          }\n        } else {\n          for(int docID : docs.get(term)) {\n            if (!deleted.contains(docID)) {\n              assertEquals(docID, docsEnum.nextDoc());\n            }\n          }\n          assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n        }\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.shutdown();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, PostingsEnum.FLAG_NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, postingsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, DocsEnum.FLAG_NONE);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, postingsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, PostingsEnum.FLAG_NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, postingsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, liveDocs, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, postingsEnum.nextDoc());\n          }\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b51ee14e04858fe1f47f241ac29486de23b215b5","date":1456734355,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af2638813028b254a88b418ebeafb541afb49653","date":1456804822,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ee0394b8176abd7c90a4be8c05465be1879db79","date":1522842314,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new MergePolicyWrapper(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(CodecReader reader) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(NoMergePolicy.INSTANCE));\n      // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n      w.setKeepFullyDeletedSegments(true);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8","date":1523648719,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(CodecReader reader) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new MergePolicyWrapper(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(CodecReader reader) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15e716649e2bd79a98b5e68c464154ea4c44677a","date":1523975212,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(CodecReader reader) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiBits.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                             .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n                                               @Override\n                                               public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                                                 // we can do this because we use NoMergePolicy (and dont merge to \"nothing\")\n                                                 return true;\n                                               }\n                                             }));\n      Map<BytesRef,List<Integer>> docs = new HashMap<>();\n      Set<Integer> deleted = new HashSet<>();\n      List<BytesRef> terms = new ArrayList<>();\n\n      int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newStringField(\"field\", \"\", Field.Store.NO);\n      doc.add(f);\n      Field id = newStringField(\"id\", \"\", Field.Store.NO);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random().nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random().nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setStringValue(term.utf8ToString());\n        } else {\n          String s = TestUtil.randomUnicodeString(random(), 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setStringValue(s);\n        }\n        id.setStringValue(\"\"+i);\n        w.addDocument(doc);\n        if (random().nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random().nextInt(20) == 1) {\n          int delID = random().nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);\n        Collections.sort(termsList);\n        System.out.println(\"TEST: terms in UTF-8 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random().nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        PostingsEnum postingsEnum = TestUtil.docs(random(), reader, \"field\", term, null, PostingsEnum.NONE);\n        assertNotNull(postingsEnum);\n\n        for(int docID : docs.get(term)) {\n          assertEquals(docID, postingsEnum.nextDoc());\n        }\n        assertEquals(DocIdSetIterator.NO_MORE_DOCS, postingsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["02331260bb246364779cb6f04919ca47900d01bb","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"af2638813028b254a88b418ebeafb541afb49653":["0f4464508ee83288c8c4585b533f9faaa93aa314","b51ee14e04858fe1f47f241ac29486de23b215b5"],"45a621dd071a902e1fd30367200d7bbbea037706":["0a16b40feb4e6e0d55c1716733bde48296bedd20"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a78a90fc9701e511308346ea29f4f5e548bb39fe","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"6613659748fe4411a7dcf85266e55db1f95f7315":["da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640","da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["af2638813028b254a88b418ebeafb541afb49653"],"51f5280f31484820499077f41fcdfe92d527d9dc":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b51ee14e04858fe1f47f241ac29486de23b215b5":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["02331260bb246364779cb6f04919ca47900d01bb"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"15e716649e2bd79a98b5e68c464154ea4c44677a":["6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"02331260bb246364779cb6f04919ca47900d01bb":["04f07771a2a7dd3a395700665ed839c3dae2def2"]},"commit2Childs":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["0a16b40feb4e6e0d55c1716733bde48296bedd20","54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5":["6613659748fe4411a7dcf85266e55db1f95f7315","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"af2638813028b254a88b418ebeafb541afb49653":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"45a621dd071a902e1fd30367200d7bbbea037706":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["45a621dd071a902e1fd30367200d7bbbea037706"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"5ee0394b8176abd7c90a4be8c05465be1879db79":["6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"b51ee14e04858fe1f47f241ac29486de23b215b5":["af2638813028b254a88b418ebeafb541afb49653"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["af2638813028b254a88b418ebeafb541afb49653","b51ee14e04858fe1f47f241ac29486de23b215b5"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"15e716649e2bd79a98b5e68c464154ea4c44677a":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"02331260bb246364779cb6f04919ca47900d01bb":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","15250ca94ba8ab3bcdd476daf6bf3f3febb92640","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","45a621dd071a902e1fd30367200d7bbbea037706","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}