{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","commits":[{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":5,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","sourceNew":"  void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (hasHitAbortingException == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      abortingExceptionConsumer.accept(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"680b6449f09827f58fe987aff279e014c311d966":["0dcf8f79417865e5028d753e669fae06457e8369","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["49f1924bd448393fbdfef8b5ebed799f938169d3"],"0dcf8f79417865e5028d753e669fae06457e8369":["f97270426d92300e08ac1bd1a4ef499ae02e88b7","49f1924bd448393fbdfef8b5ebed799f938169d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["7a6f8af01d9b3067b143bbdc0a492720e2af97cf","0dcf8f79417865e5028d753e669fae06457e8369"],"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"0dcf8f79417865e5028d753e669fae06457e8369":["680b6449f09827f58fe987aff279e014c311d966"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}