{"path":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","commits":[{"id":"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc","date":1359570667,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61d5f95d14e5b9b046998c51e16709a398c15226","date":1359603451,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"571abba77e55fea386a38c0024f72ffa5b37a9ad","date":1360272747,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets(Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(fsp, indexReader, taxo);\n    FacetsCollector fc = FacetsCollector.create(accumulator);\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, fc));\n        \n    // Obtain facets results and print them\n    List<FacetResult> res = fc.getFacetResults();\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31703f98041a9e7086254ed134d8f91c22ac933f","date":1360500940,"type":4,"author":"Shai Erera","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":null,"sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets(Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(fsp, indexReader, taxo);\n    FacetsCollector fc = FacetsCollector.create(accumulator);\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, fc));\n        \n    // Obtain facets results and print them\n    List<FacetResult> res = fc.getFacetResults();\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"61d5f95d14e5b9b046998c51e16709a398c15226":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"],"31703f98041a9e7086254ed134d8f91c22ac933f":["571abba77e55fea386a38c0024f72ffa5b37a9ad"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"571abba77e55fea386a38c0024f72ffa5b37a9ad":["1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["31703f98041a9e7086254ed134d8f91c22ac933f"]},"commit2Childs":{"61d5f95d14e5b9b046998c51e16709a398c15226":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["61d5f95d14e5b9b046998c51e16709a398c15226","1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"],"31703f98041a9e7086254ed134d8f91c22ac933f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc":["61d5f95d14e5b9b046998c51e16709a398c15226","571abba77e55fea386a38c0024f72ffa5b37a9ad"],"571abba77e55fea386a38c0024f72ffa5b37a9ad":["31703f98041a9e7086254ed134d8f91c22ac933f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["61d5f95d14e5b9b046998c51e16709a398c15226","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}