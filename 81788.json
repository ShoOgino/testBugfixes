{"path":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","commits":[{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"/dev/null","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, 10).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, 20); // shouldn't take 20 secs but just to be safe\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, 20);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["877f1e09b9299ce0757f4d83768da944803baf04"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aea1d78da2c058b98e64569bcd37981c733b52a8","date":1400551646,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, 10).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, 20); // shouldn't take 20 secs but just to be safe\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, 20);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, 10).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, 20); // shouldn't take 20 secs but just to be safe\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, 20);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9043cd220362869f58e50f635c13c362f8377da","date":1404227796,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3f5be45b5f54f240a9e1485e92e33a094299659","date":1405328334,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d14328dee83c3ec0478e7d711f7af48560ad5ef","date":1412617800,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d","date":1419896224,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37e7502644cd23597431d66e301299b1ead2fb9b","date":1422636984,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollection(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"182acd29cf4cb1644a02b8517f3a5b867c0d7cce","date":1432665213,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"877f1e09b9299ce0757f4d83768da944803baf04","date":1433276115,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    SolrDispatchFilter filter = (SolrDispatchFilter)replicaJetty.getDispatchFilter().getFilter();\n    CoreContainer coreContainer = filter.getCores();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n        \n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1088b72b3b4cc45316b7595bd09023c859cd2327","date":1447150009,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    SolrDispatchFilter filter = (SolrDispatchFilter)replicaJetty.getDispatchFilter().getFilter();\n    CoreContainer coreContainer = filter.getCores();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21527799e803fb77d646c5612716d40a48a30335","date":1487775419,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = 1000;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 100 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 100) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":["ce70cdb9511a671e0e15d4dc93b8334c86b6e681"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac97ea104d893f16aab430d9904473bc1f233f3c","date":1496249396,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3217321f3e1d7922898c6c633d17acfa840d6875","date":1496257480,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f","date":1496281877,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      new CollectionAdminRequest.Delete()\n              .setCollectionName(testCollectionName).process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    \n    proxy.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    // Have the partition last at least 1 sec\n    // While this gives the impression that recovery is timing related, this is\n    // really only\n    // to give time for the state to be written to ZK before the test completes.\n    // In other words,\n    // without a brief pause, the test finishes so quickly that it doesn't give\n    // time for the recovery process to kick-in\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    sendDoc(3);\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      sendDoc(d + 4); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ce70cdb9511a671e0e15d4dc93b8334c86b6e681","date":1542017422,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 105;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 100;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":["21527799e803fb77d646c5612716d40a48a30335"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a966532d92cf9ba2856f15a8140151bb6b518e4b","date":1588290631,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed {} for core {}\", maxVersionBefore, coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 105;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed {} for core {}, was: {}\"\n          , currentMaxVersion, coreName, maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed \"+maxVersionBefore+\" for core \"+coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 105;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed \" + currentMaxVersion +\n          \" for core \" + coreName + \", was: \" + maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf2().mjava","sourceNew":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed {} for core {}\", maxVersionBefore, coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 105;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed {} for core {}, was: {}\"\n          , currentMaxVersion, coreName, maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf2() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x2\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 2, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    Replica notLeader = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive).get(0);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy = getProxyForReplica(notLeader);\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy.close();\n    leaderProxy.close();\n\n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    // replica should publish itself as DOWN if the network is not healed after some amount time\n    waitForState(testCollectionName, notLeader.getName(), DOWN, 10000);\n    \n    proxy.reopen();\n    leaderProxy.reopen();\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    \n    int achievedRf = sendDoc(3);\n    if (achievedRf == 1) {\n      // this case can happen when leader reuse an connection get established before network partition\n      // TODO: Remove when SOLR-11776 get committed\n      ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n    }\n    \n    // sent 3 docs in so far, verify they are on the leader and replica\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n\n    // Get the max version from the replica core to make sure it gets updated after recovery (see SOLR-7625)\n    JettySolrRunner replicaJetty = getJettyOnPort(getReplicaPort(notLeader));\n    CoreContainer coreContainer = replicaJetty.getCoreContainer();\n    ZkCoreNodeProps replicaCoreNodeProps = new ZkCoreNodeProps(notLeader);\n    String coreName = replicaCoreNodeProps.getCoreName();\n    Long maxVersionBefore = null;\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\"+coreName+\"' not found for replica: \"+notLeader.getName(), core);\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      maxVersionBefore = ulog.getCurrentMaxVersion();\n    }\n    assertNotNull(\"max version bucket seed not set for core \" + coreName, maxVersionBefore);\n    log.info(\"Looked up max version bucket seed {} for core {}\", maxVersionBefore, coreName);\n\n    // now up the stakes and do more docs\n    int numDocs = TEST_NIGHTLY ? 1000 : 105;\n    boolean hasPartition = false;\n    for (int d = 0; d < numDocs; d++) {\n      // create / restore partition every 100 docs\n      if (d % 10 == 0) {\n        if (hasPartition) {\n          proxy.reopen();\n          leaderProxy.reopen();\n          hasPartition = false;\n        } else {\n          if (d >= 10) {\n            proxy.close();\n            leaderProxy.close();\n            hasPartition = true;\n            Thread.sleep(sleepMsBeforeHealPartition);\n          }\n        }\n      }\n      // always send doc directly to leader without going through proxy\n      sendDoc(d + 4, null, leaderJetty); // 4 is offset as we've already indexed 1-3\n    }\n    \n    // restore connectivity if lost\n    if (hasPartition) {\n      proxy.reopen();\n      leaderProxy.reopen();\n    }\n    \n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 2, maxWaitSecsToSeeAllActive);\n\n    try (SolrCore core = coreContainer.getCore(coreName)) {\n      assertNotNull(\"Core '\" + coreName + \"' not found for replica: \" + notLeader.getName(), core);\n      Long currentMaxVersion = core.getUpdateHandler().getUpdateLog().getCurrentMaxVersion();\n      log.info(\"After recovery, looked up NEW max version bucket seed {} for core {}, was: {}\"\n          , currentMaxVersion, coreName, maxVersionBefore);\n      assertTrue(\"max version bucket seed not updated after recovery!\", currentMaxVersion > maxVersionBefore);\n    }\n\n    // verify all docs received\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, numDocs + 3);\n\n    log.info(\"testRf2 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"197bbedf08450ade98a11f4a0001448059666bec":["42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"21527799e803fb77d646c5612716d40a48a30335":["1088b72b3b4cc45316b7595bd09023c859cd2327"],"84f20f331d8001864545c7021812d8c6509c7593":["28288370235ed02234a64753cdbf0c6ec096304a"],"2d14328dee83c3ec0478e7d711f7af48560ad5ef":["b3f5be45b5f54f240a9e1485e92e33a094299659"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["2d14328dee83c3ec0478e7d711f7af48560ad5ef"],"b7605579001505896d48b07160075a5c8b8e128e":["14d5815ecbef89580f5c48990bcd433f04f8563a","aea1d78da2c058b98e64569bcd37981c733b52a8"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["ce70cdb9511a671e0e15d4dc93b8334c86b6e681"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["21527799e803fb77d646c5612716d40a48a30335","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["e9017cf144952056066919f1ebc7897ff9bd71b1","197bbedf08450ade98a11f4a0001448059666bec"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"877f1e09b9299ce0757f4d83768da944803baf04":["182acd29cf4cb1644a02b8517f3a5b867c0d7cce"],"55980207f1977bd1463465de1659b821347e2fa8":["b3f5be45b5f54f240a9e1485e92e33a094299659","2d14328dee83c3ec0478e7d711f7af48560ad5ef"],"ac97ea104d893f16aab430d9904473bc1f233f3c":["21527799e803fb77d646c5612716d40a48a30335"],"ce70cdb9511a671e0e15d4dc93b8334c86b6e681":["84f20f331d8001864545c7021812d8c6509c7593"],"3217321f3e1d7922898c6c633d17acfa840d6875":["21527799e803fb77d646c5612716d40a48a30335","ac97ea104d893f16aab430d9904473bc1f233f3c"],"f9043cd220362869f58e50f635c13c362f8377da":["aea1d78da2c058b98e64569bcd37981c733b52a8"],"28288370235ed02234a64753cdbf0c6ec096304a":["3217321f3e1d7922898c6c633d17acfa840d6875","197bbedf08450ade98a11f4a0001448059666bec"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["21527799e803fb77d646c5612716d40a48a30335","3217321f3e1d7922898c6c633d17acfa840d6875"],"1088b72b3b4cc45316b7595bd09023c859cd2327":["877f1e09b9299ce0757f4d83768da944803baf04"],"b3f5be45b5f54f240a9e1485e92e33a094299659":["f9043cd220362869f58e50f635c13c362f8377da"],"37e7502644cd23597431d66e301299b1ead2fb9b":["d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d"],"182acd29cf4cb1644a02b8517f3a5b867c0d7cce":["37e7502644cd23597431d66e301299b1ead2fb9b"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"197bbedf08450ade98a11f4a0001448059666bec":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","28288370235ed02234a64753cdbf0c6ec096304a"],"21527799e803fb77d646c5612716d40a48a30335":["e9017cf144952056066919f1ebc7897ff9bd71b1","ac97ea104d893f16aab430d9904473bc1f233f3c","3217321f3e1d7922898c6c633d17acfa840d6875","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"84f20f331d8001864545c7021812d8c6509c7593":["ce70cdb9511a671e0e15d4dc93b8334c86b6e681"],"2d14328dee83c3ec0478e7d711f7af48560ad5ef":["d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d","55980207f1977bd1463465de1659b821347e2fa8"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["37e7502644cd23597431d66e301299b1ead2fb9b"],"b7605579001505896d48b07160075a5c8b8e128e":[],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":[],"14d5815ecbef89580f5c48990bcd433f04f8563a":["b7605579001505896d48b07160075a5c8b8e128e","aea1d78da2c058b98e64569bcd37981c733b52a8"],"877f1e09b9299ce0757f4d83768da944803baf04":["1088b72b3b4cc45316b7595bd09023c859cd2327"],"55980207f1977bd1463465de1659b821347e2fa8":[],"ac97ea104d893f16aab430d9904473bc1f233f3c":["3217321f3e1d7922898c6c633d17acfa840d6875"],"ce70cdb9511a671e0e15d4dc93b8334c86b6e681":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"3217321f3e1d7922898c6c633d17acfa840d6875":["28288370235ed02234a64753cdbf0c6ec096304a","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"f9043cd220362869f58e50f635c13c362f8377da":["b3f5be45b5f54f240a9e1485e92e33a094299659"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["197bbedf08450ade98a11f4a0001448059666bec","e9017cf144952056066919f1ebc7897ff9bd71b1"],"28288370235ed02234a64753cdbf0c6ec096304a":["84f20f331d8001864545c7021812d8c6509c7593"],"1088b72b3b4cc45316b7595bd09023c859cd2327":["21527799e803fb77d646c5612716d40a48a30335"],"b3f5be45b5f54f240a9e1485e92e33a094299659":["2d14328dee83c3ec0478e7d711f7af48560ad5ef","55980207f1977bd1463465de1659b821347e2fa8"],"37e7502644cd23597431d66e301299b1ead2fb9b":["182acd29cf4cb1644a02b8517f3a5b867c0d7cce"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["b7605579001505896d48b07160075a5c8b8e128e","f9043cd220362869f58e50f635c13c362f8377da"],"182acd29cf4cb1644a02b8517f3a5b867c0d7cce":["877f1e09b9299ce0757f4d83768da944803baf04"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","55980207f1977bd1463465de1659b821347e2fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}