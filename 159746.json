{"path":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","commits":[{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setStartOffset(start);\n    offsetAtt.setEndOffset(start+length);\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0c17d12803da6cadc96b3cdf15b0b940eddb28de","0c17d12803da6cadc96b3cdf15b0b940eddb28de","0c17d12803da6cadc96b3cdf15b0b940eddb28de"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"10855b393afd8884613d82de3a4fff773d4e5334","date":1240953458,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(start, start+length);\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setStartOffset(start);\n    offsetAtt.setEndOffset(start+length);\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2","date":1245784531,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(start, start+length);\n    return true;\n  }\n\n","bugFix":null,"bugIntro":["a82fda1447250ff156ff3b862d94a99bf0a3c23c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"457c790b0d3d5883da64fb842ea54813004bb796","date":1248495093,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0833fee1ce16a2b8e10f21cbccd2e93f3d8ccf31","date":1249940086,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    int length = 0;\n    int start = bufferIndex;\n    termAtt.clear();\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"91d0e16ae1a83f5658ad4d16453fb88650460140","date":1250287302,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    int length = 0;\n    int start = bufferIndex;\n    termAtt.clear();\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a82fda1447250ff156ff3b862d94a99bf0a3c23c","date":1252649533,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":["cd27af5c226d98a7c6378c388a67a3bff7c0b3a2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a9e385641d717e641408d8fbbc62be8fc766357","date":1256746606,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n  }\n\n","sourceOld":"  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"966b079690131e434b39530d82cc413f5aff4cd5","date":1264751025,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    if(useOldAPI) // TODO remove this in LUCENE 4.0\n      return incrementTokenOld();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        if(!charUtils.fill(ioBuffer, input)) { // read supplementary char aware with CharacterUtils\n          dataLen = 0; // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        dataLen = ioBuffer.getLength();\n        bufferIndex = 0;\n      }\n      // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone\n      final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);\n      bufferIndex += Character.charCount(c);\n\n      if (isTokenChar(c)) {               // if it's a token char\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length >= buffer.length-1) // check if a supplementary could run out of bounds\n          buffer = termAtt.resizeTermBuffer(2+length); // make sure a supplementary fits in the buffer\n        length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized\n        if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test\n          break;\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n    \n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          dataLen = 0;                            // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = termAtt.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)      // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":["0c17d12803da6cadc96b3cdf15b0b940eddb28de","0c17d12803da6cadc96b3cdf15b0b940eddb28de","0c17d12803da6cadc96b3cdf15b0b940eddb28de"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    if(useOldAPI) // TODO remove this in LUCENE 4.0\n      return incrementTokenOld();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        if(!charUtils.fill(ioBuffer, input)) { // read supplementary char aware with CharacterUtils\n          dataLen = 0; // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        dataLen = ioBuffer.getLength();\n        bufferIndex = 0;\n      }\n      // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone\n      final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);\n      bufferIndex += Character.charCount(c);\n\n      if (isTokenChar(c)) {               // if it's a token char\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length >= buffer.length-1) // check if a supplementary could run out of bounds\n          buffer = termAtt.resizeTermBuffer(2+length); // make sure a supplementary fits in the buffer\n        length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized\n        if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test\n          break;\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n    \n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n    if(useOldAPI) // TODO remove this in LUCENE 4.0\n      return incrementTokenOld();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = termAtt.termBuffer();\n    while (true) {\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        if(!charUtils.fill(ioBuffer, input)) { // read supplementary char aware with CharacterUtils\n          dataLen = 0; // so next offset += dataLen won't decrement offset\n          if (length > 0)\n            break;\n          else\n            return false;\n        }\n        dataLen = ioBuffer.getLength();\n        bufferIndex = 0;\n      }\n      // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone\n      final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);\n      bufferIndex += Character.charCount(c);\n\n      if (isTokenChar(c)) {               // if it's a token char\n        if (length == 0)                 // start of token\n          start = offset + bufferIndex - 1;\n        else if (length >= buffer.length-1) // check if a supplementary could run out of bounds\n          buffer = termAtt.resizeTermBuffer(2+length); // make sure a supplementary fits in the buffer\n        length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized\n        if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test\n          break;\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    termAtt.setTermLength(length);\n    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));\n    return true;\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"966b079690131e434b39530d82cc413f5aff4cd5":["8a9e385641d717e641408d8fbbc62be8fc766357"],"a82fda1447250ff156ff3b862d94a99bf0a3c23c":["91d0e16ae1a83f5658ad4d16453fb88650460140"],"457c790b0d3d5883da64fb842ea54813004bb796":["cd27af5c226d98a7c6378c388a67a3bff7c0b3a2"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"91d0e16ae1a83f5658ad4d16453fb88650460140":["0833fee1ce16a2b8e10f21cbccd2e93f3d8ccf31"],"8a9e385641d717e641408d8fbbc62be8fc766357":["a82fda1447250ff156ff3b862d94a99bf0a3c23c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"10855b393afd8884613d82de3a4fff773d4e5334":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["966b079690131e434b39530d82cc413f5aff4cd5"],"0833fee1ce16a2b8e10f21cbccd2e93f3d8ccf31":["457c790b0d3d5883da64fb842ea54813004bb796"],"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2":["10855b393afd8884613d82de3a4fff773d4e5334"]},"commit2Childs":{"966b079690131e434b39530d82cc413f5aff4cd5":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a82fda1447250ff156ff3b862d94a99bf0a3c23c":["8a9e385641d717e641408d8fbbc62be8fc766357"],"457c790b0d3d5883da64fb842ea54813004bb796":["0833fee1ce16a2b8e10f21cbccd2e93f3d8ccf31"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["10855b393afd8884613d82de3a4fff773d4e5334"],"91d0e16ae1a83f5658ad4d16453fb88650460140":["a82fda1447250ff156ff3b862d94a99bf0a3c23c"],"8a9e385641d717e641408d8fbbc62be8fc766357":["966b079690131e434b39530d82cc413f5aff4cd5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"10855b393afd8884613d82de3a4fff773d4e5334":["cd27af5c226d98a7c6378c388a67a3bff7c0b3a2"],"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2":["457c790b0d3d5883da64fb842ea54813004bb796"],"0833fee1ce16a2b8e10f21cbccd2e93f3d8ccf31":["91d0e16ae1a83f5658ad4d16453fb88650460140"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}