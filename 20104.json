{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","commits":[{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"/dev/null","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n    \n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new DocsAndPositionsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        DocsAndPositionsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n    \n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new DocsAndPositionsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        DocsAndPositionsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc","date":1427826153,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":["fd152d7a93506b8b937e483676fe721e3cb857a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b4e3cd382d0d075a0f1725649c084bb6510c483","date":1428096423,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        if (t != null) {\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        if (t != null) {\n          termsEnum = t.iterator(null);\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05c52ac194342b760b830342ee8423fcf00e54d0","date":1429197275,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        termsEnum = t.iterator();\n        postings = new PostingsEnum[terms.length];\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        if (t != null) {\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":["fd152d7a93506b8b937e483676fe721e3cb857a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd152d7a93506b8b937e483676fe721e3cb857a9","date":1429913980,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          if (!t.hasOffsets()) {\n            // no offsets available\n            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n          }\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        } else {\n          termsEnum = null;\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (!t.hasOffsets()) {\n          // no offsets available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        termsEnum = t.iterator();\n        postings = new PostingsEnum[terms.length];\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":["05c52ac194342b760b830342ee8423fcf00e54d0","1530e3091abcc262e32b7d5b7ba8675aa3fff9cc"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad0d09e969f4763b0df4230f8e3f74357872a4e4","date":1459188769,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter must not be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          if (!t.hasOffsets()) {\n            // no offsets available\n            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n          }\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        } else {\n          termsEnum = null;\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          if (!t.hasOffsets()) {\n            // no offsets available\n            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n          }\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        } else {\n          termsEnum = null;\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"381618eac2691bb34ab9a3fca76ad55c6274517e","date":1495564791,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":null,"sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter must not be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          if (!t.hasOffsets()) {\n            // no offsets available\n            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n          }\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        } else {\n          termsEnum = null;\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[LeafReaderContext],int,Query).mjava","sourceNew":null,"sourceOld":"  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  \n    Map<Integer,Object> highlights = new HashMap<>();\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter must not be null\");\n    }\n    \n    // check if we should do any multiterm processing\n    Analyzer analyzer = getIndexAnalyzer(field);\n    CharacterRunAutomaton automata[] = new CharacterRunAutomaton[0];\n    if (analyzer != null) {\n      automata = MultiTermHighlighting.extractAutomata(query, field);\n    }\n    \n    // resize 'terms', where the last term is the multiterm matcher\n    if (automata.length > 0) {\n      BytesRef newTerms[] = new BytesRef[terms.length + 1];\n      System.arraycopy(terms, 0, newTerms, 0, terms.length);\n      terms = newTerms;\n    }\n\n    // we are processing in increasing docid order, so we only need to reinitialize stuff on segment changes\n    // otherwise, we will just advance() existing enums to the new document in the same segment.\n    PostingsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      LeafReaderContext subContext = leaves.get(leaf);\n      LeafReader r = subContext.reader();\n      \n      assert leaf >= lastLeaf; // increasing order\n      \n      // if the segment has changed, we must initialize new enums.\n      if (leaf != lastLeaf) {\n        Terms t = r.terms(field);\n        if (t != null) {\n          if (!t.hasOffsets()) {\n            // no offsets available\n            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n          }\n          termsEnum = t.iterator();\n          postings = new PostingsEnum[terms.length];\n        } else {\n          termsEnum = null;\n        }\n      }\n      if (termsEnum == null) {\n        continue; // no terms for this field, nothing to do\n      }\n      \n      // if there are multi-term matches, we have to initialize the \"fake\" enum for each document\n      if (automata.length > 0) {\n        PostingsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);\n        dp.advance(doc - subContext.docBase);\n        postings[terms.length-1] = dp; // last term is the multiterm matcher\n      }\n      \n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      \n      if (passages.length == 0) {\n        // no passages were returned, so ask for a default summary\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n\n      if (passages.length > 0) {\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      \n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["ad0d09e969f4763b0df4230f8e3f74357872a4e4","381618eac2691bb34ab9a3fca76ad55c6274517e"],"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc":["51f5280f31484820499077f41fcdfe92d527d9dc"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["51f5280f31484820499077f41fcdfe92d527d9dc","1530e3091abcc262e32b7d5b7ba8675aa3fff9cc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ad0d09e969f4763b0df4230f8e3f74357872a4e4":["fd152d7a93506b8b937e483676fe721e3cb857a9"],"05c52ac194342b760b830342ee8423fcf00e54d0":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"fd152d7a93506b8b937e483676fe721e3cb857a9":["05c52ac194342b760b830342ee8423fcf00e54d0"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["381618eac2691bb34ab9a3fca76ad55c6274517e"]},"commit2Childs":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["05c52ac194342b760b830342ee8423fcf00e54d0"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ad0d09e969f4763b0df4230f8e3f74357872a4e4":["381618eac2691bb34ab9a3fca76ad55c6274517e","e9017cf144952056066919f1ebc7897ff9bd71b1"],"51f5280f31484820499077f41fcdfe92d527d9dc":["1530e3091abcc262e32b7d5b7ba8675aa3fff9cc","6b4e3cd382d0d075a0f1725649c084bb6510c483"],"05c52ac194342b760b830342ee8423fcf00e54d0":["fd152d7a93506b8b937e483676fe721e3cb857a9"],"fd152d7a93506b8b937e483676fe721e3cb857a9":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}