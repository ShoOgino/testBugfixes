{"path":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","commits":[{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      assertEquals(docID, s.nextDoc());\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.binaryValue();\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      assertEquals(docID, s.nextDoc());\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.binaryValue();\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"/dev/null","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      assertEquals(docID, s.nextDoc());\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.binaryValue();\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":4,"author":"Dawid Weiss","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":null,"sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      assertEquals(docID, s.nextDoc());\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.binaryValue();\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"24f89e8a6aac05753cde4c83d62a74356098200d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["0e121d43b5a10f2df530f406f935102656e9c4e8","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"83870855d82aba6819217abeff5a40779dbb28b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0e121d43b5a10f2df530f406f935102656e9c4e8"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["24f89e8a6aac05753cde4c83d62a74356098200d"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d","83870855d82aba6819217abeff5a40779dbb28b4"],"24f89e8a6aac05753cde4c83d62a74356098200d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["24f89e8a6aac05753cde4c83d62a74356098200d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}