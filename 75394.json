{"path":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da59c6d3748d1a2a9d1a58a69f70383622d68379","date":1433873890,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294","42d384b06aa87eae925b668b65f3246154f0b0fa"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"102da6baafc0f534a59f31729343dbab9d3b9e9a","date":1438410244,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bcf9886c8ff537aafde14de48ebf744f5673f08b","date":1439041198,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69f91b8bcd249b2eb41d6dfa163332a5842f6fe","date":1440511898,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 5);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 5);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=12\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 7, 3, 9);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05834cb6cab7637131bac081d141cdc023ce2a38","date":1440514846,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 5);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 5);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","date":1460069869,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = getHttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","date":1460110033,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = getHttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":null,"sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = getHttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","sourceNew":null,"sourceOld":"  @Nightly\n  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    String[] args = new String[]{};\n    List<String> argList = new ArrayList<>();\n\n    try (HttpSolrClient server = getHttpSolrClient(cloudJettys.get(0).url)) {\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n          \"--mappers=3\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n          \"--verbose\",\n          \"--go-live\"\n      };\n      args = prependInitialArgs(args);\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n        assertEquals(20, results.getResults().getNumFound());\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      assertTrue(fs.mkdirs(inDir));\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n      args = new String[]{\n          \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n      };\n      args = prependInitialArgs(args);\n\n      getShardUrlArgs(argList);\n      args = concat(args, argList.toArray(new String[0]));\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n        results = server.query(new SolrQuery(\"*:*\"));\n\n        assertEquals(22, results.getResults().getNumFound());\n      }\n\n      // try using zookeeper\n      String collection = \"collection1\";\n      if (random().nextBoolean()) {\n        // sometimes, use an alias\n        createAlias(\"updatealias\", \"collection1\");\n        collection = \"updatealias\";\n      }\n\n      fs.delete(inDir, true);\n      fs.delete(outDir, true);\n      fs.delete(dataDir, true);\n      INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n\n      args = new String[]{\n          \"--output-dir=\" + outDir.toString(),\n          \"--mappers=3\",\n          \"--reducers=6\",\n          \"--fanout=2\",\n          \"--verbose\",\n          \"--go-live\",\n          random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n          \"--zk-host\", zkServer.getZkAddress(),\n          \"--collection\", collection\n      };\n      args = prependInitialArgs(args);\n\n      if (true) {\n        tool = new MapReduceIndexerTool();\n        res = ToolRunner.run(jobConf, tool, args);\n        assertEquals(0, res);\n        assertTrue(tool.job.isComplete());\n        assertTrue(tool.job.isSuccessful());\n\n        SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs.size());\n\n        // perform updates\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n            update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n        }\n        cloudClient.commit();\n\n        // verify updates\n        SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");\n        assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n        assertEquals(RECORD_COUNT, resultDocs2.size());\n        for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n\n          // perform delete\n          cloudClient.deleteById((String) doc.getFirstValue(\"id\"));\n        }\n        cloudClient.commit();\n\n        // verify deletes\n        assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n      }\n\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (timeout.hasTimedOut()) {\n         throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 3, 3, 3);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a69f91b8bcd249b2eb41d6dfa163332a5842f6fe":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"da59c6d3748d1a2a9d1a58a69f70383622d68379":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["da59c6d3748d1a2a9d1a58a69f70383622d68379"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["05834cb6cab7637131bac081d141cdc023ce2a38"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"05834cb6cab7637131bac081d141cdc023ce2a38":["a69f91b8bcd249b2eb41d6dfa163332a5842f6fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","e3c94a8b8bf47db4f968d9ae510ec8bbe1372088"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["abb23fcc2461782ab204e61213240feb77d355aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"a69f91b8bcd249b2eb41d6dfa163332a5842f6fe":["05834cb6cab7637131bac081d141cdc023ce2a38"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"da59c6d3748d1a2a9d1a58a69f70383622d68379":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"abb23fcc2461782ab204e61213240feb77d355aa":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"05834cb6cab7637131bac081d141cdc023ce2a38":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["a69f91b8bcd249b2eb41d6dfa163332a5842f6fe"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["da59c6d3748d1a2a9d1a58a69f70383622d68379"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}