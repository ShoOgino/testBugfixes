{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","commits":[{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,int).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  private void testNGrams(int minGram, int maxGram, int length) throws IOException {\n    final String s = RandomStrings.randomAsciiOfLength(random(), length);\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram);\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < s.length(); ++start) {\n      for (int end = start + minGram; end <= start + maxGram && end <= s.length(); ++end) {\n        assertTrue(grams.incrementToken());\n        assertEquals(s.substring(start, end), termAtt.toString());\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(start, offsetAtt.startOffset());\n        assertEquals(end, offsetAtt.endOffset());\n      }\n    }\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(TEST_VERSION_CURRENT, minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(TEST_VERSION_CURRENT, minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9798d0818e7a880546802b509792d3f3d57babd2","date":1528358901,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(ArrayUtil.copyOfSubArray(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(ArrayUtil.copyOfSubArray(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(ArrayUtil.copyOfSubArray(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final Tokenizer grams = new NGramTokenizer(minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    grams.setReader(new StringReader(s));\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["379db3ad24c4f0214f30a122265a6d6be003a99d","9798d0818e7a880546802b509792d3f3d57babd2"],"9798d0818e7a880546802b509792d3f3d57babd2":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9798d0818e7a880546802b509792d3f3d57babd2"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["379db3ad24c4f0214f30a122265a6d6be003a99d","9798d0818e7a880546802b509792d3f3d57babd2"]},"commit2Childs":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["b70042a8a492f7054d480ccdd2be9796510d4327","9798d0818e7a880546802b509792d3f3d57babd2","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"9798d0818e7a880546802b509792d3f3d57babd2":["b70042a8a492f7054d480ccdd2be9796510d4327","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}