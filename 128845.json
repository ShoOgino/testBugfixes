{"path":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        // TODO (Facet): avoid String creation/use bytes?\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","sourceOld":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        // TODO (Facet): avoid String creation/use bytes?\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"533890d1266aa8169162ec556395d0c5d0377566","date":1337173867,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    TermsEnum termsEnum = null;\n    DocsEnum docsEnum = null;\n    int base = 0;\n    for (AtomicReader r : reader.getSequentialSubReaders()) {\n      Terms terms = r.terms(Consts.FULL);\n      if (terms != null) { // cannot really happen, but be on the safe side\n        termsEnum = terms.iterator(termsEnum);\n        while (termsEnum.next() != null) {\n          BytesRef t = termsEnum.term();\n          // Since we guarantee uniqueness of categories, each term has exactly\n          // one document. Also, since we do not allow removing categories (and\n          // hence documents), there are no deletions in the index. Therefore, it\n          // is sufficient to call next(), and then doc(), exactly once with no\n          // 'validation' checks.\n          cp.clear();\n          cp.add(t.utf8ToString(), delimiter);\n          docsEnum = termsEnum.docs(null, docsEnum, false);\n          cache.put(cp, docsEnum.nextDoc() + base);\n        }\n      }\n      base += r.maxDoc(); // we don't have any deletions, so we're ok\n    }\n    /*Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }*/\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","sourceOld":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        // TODO (Facet): avoid String creation/use bytes?\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"119bc02554a192b2954b73d79389ec441257b624","date":1337232699,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      openInternalReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    TermsEnum termsEnum = null;\n    DocsEnum docsEnum = null;\n    int base = 0;\n    for (AtomicReader r : reader.getSequentialSubReaders()) {\n      Terms terms = r.terms(Consts.FULL);\n      if (terms != null) { // cannot really happen, but be on the safe side\n        termsEnum = terms.iterator(termsEnum);\n        while (termsEnum.next() != null) {\n          BytesRef t = termsEnum.term();\n          // Since we guarantee uniqueness of categories, each term has exactly\n          // one document. Also, since we do not allow removing categories (and\n          // hence documents), there are no deletions in the index. Therefore, it\n          // is sufficient to call next(), and then doc(), exactly once with no\n          // 'validation' checks.\n          cp.clear();\n          cp.add(t.utf8ToString(), delimiter);\n          docsEnum = termsEnum.docs(null, docsEnum, false);\n          cache.put(cp, docsEnum.nextDoc() + base);\n        }\n      }\n      base += r.maxDoc(); // we don't have any deletions, so we're ok\n    }\n    /*Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }*/\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","sourceOld":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    TermsEnum termsEnum = null;\n    DocsEnum docsEnum = null;\n    int base = 0;\n    for (AtomicReader r : reader.getSequentialSubReaders()) {\n      Terms terms = r.terms(Consts.FULL);\n      if (terms != null) { // cannot really happen, but be on the safe side\n        termsEnum = terms.iterator(termsEnum);\n        while (termsEnum.next() != null) {\n          BytesRef t = termsEnum.term();\n          // Since we guarantee uniqueness of categories, each term has exactly\n          // one document. Also, since we do not allow removing categories (and\n          // hence documents), there are no deletions in the index. Therefore, it\n          // is sufficient to call next(), and then doc(), exactly once with no\n          // 'validation' checks.\n          cp.clear();\n          cp.add(t.utf8ToString(), delimiter);\n          docsEnum = termsEnum.docs(null, docsEnum, false);\n          cache.put(cp, docsEnum.nextDoc() + base);\n        }\n      }\n      base += r.maxDoc(); // we don't have any deletions, so we're ok\n    }\n    /*Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }*/\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef0d8a69209261514c5739c770bba706c2308450","date":1337607597,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      openInternalReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    TermsEnum termsEnum = null;\n    DocsEnum docsEnum = null;\n    int base = 0;\n    for (AtomicReader r : reader.getSequentialSubReaders()) {\n      Terms terms = r.terms(Consts.FULL);\n      if (terms != null) { // cannot really happen, but be on the safe side\n        termsEnum = terms.iterator(termsEnum);\n        while (termsEnum.next() != null) {\n          BytesRef t = termsEnum.term();\n          // Since we guarantee uniqueness of categories, each term has exactly\n          // one document. Also, since we do not allow removing categories (and\n          // hence documents), there are no deletions in the index. Therefore, it\n          // is sufficient to call next(), and then doc(), exactly once with no\n          // 'validation' checks.\n          cp.clear();\n          cp.add(t.utf8ToString(), delimiter);\n          docsEnum = termsEnum.docs(null, docsEnum, false);\n          cache.put(cp, docsEnum.nextDoc() + base);\n        }\n      }\n      base += r.maxDoc(); // we don't have any deletions, so we're ok\n    }\n    /*Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }*/\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","sourceOld":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      reader = openReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        // TODO (Facet): avoid String creation/use bytes?\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b7a1bc6030c258e47d63eff3455a2b1bbf32683","date":1339494023,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, false);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  private boolean perhapsFillCache() throws IOException {\n    // Note: we assume that we're only called when cacheIsComplete==false.\n    // TODO (Facet): parametrize this criterion:\n    if (cacheMisses < cacheMissesUntilFill) {\n      return false;\n    }\n    // If the cache was already filled (or we decided not to fill it because\n    // there was no room), there is no sense in trying it again.\n    if (alreadyCalledFillCache) {\n      return false;\n    }\n    alreadyCalledFillCache = true;\n    // TODO (Facet): we should probably completely clear the cache before starting\n    // to read it?\n    if (reader == null) {\n      openInternalReader();\n    }\n\n    if (!cache.hasRoom(reader.numDocs())) {\n      return false;\n    }\n\n    CategoryPath cp = new CategoryPath();\n    TermsEnum termsEnum = null;\n    DocsEnum docsEnum = null;\n    int base = 0;\n    for (AtomicReader r : reader.getSequentialSubReaders()) {\n      Terms terms = r.terms(Consts.FULL);\n      if (terms != null) { // cannot really happen, but be on the safe side\n        termsEnum = terms.iterator(termsEnum);\n        while (termsEnum.next() != null) {\n          BytesRef t = termsEnum.term();\n          // Since we guarantee uniqueness of categories, each term has exactly\n          // one document. Also, since we do not allow removing categories (and\n          // hence documents), there are no deletions in the index. Therefore, it\n          // is sufficient to call next(), and then doc(), exactly once with no\n          // 'validation' checks.\n          cp.clear();\n          cp.add(t.utf8ToString(), delimiter);\n          docsEnum = termsEnum.docs(null, docsEnum, false);\n          cache.put(cp, docsEnum.nextDoc() + base);\n        }\n      }\n      base += r.maxDoc(); // we don't have any deletions, so we're ok\n    }\n    /*Terms terms = MultiFields.getTerms(reader, Consts.FULL);\n    // The check is done here to avoid checking it on every iteration of the\n    // below loop. A null term wlil be returned if there are no terms in the\n    // lexicon, or after the Consts.FULL term. However while the loop is\n    // executed we're safe, because we only iterate as long as there are next()\n    // terms.\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator(null);\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docsEnum = null;\n      while (termsEnum.next() != null) {\n        BytesRef t = termsEnum.term();\n        // Since we guarantee uniqueness of categories, each term has exactly\n        // one document. Also, since we do not allow removing categories (and\n        // hence documents), there are no deletions in the index. Therefore, it\n        // is sufficient to call next(), and then doc(), exactly once with no\n        // 'validation' checks.\n        docsEnum = termsEnum.docs(liveDocs, docsEnum, false);\n        docsEnum.nextDoc();\n        cp.clear();\n        cp.add(t.utf8ToString(), delimiter);\n        cache.put(cp, docsEnum.docID());\n      }\n    }*/\n\n    cacheIsComplete = true;\n    // No sense to keep the reader open - we will not need to read from it\n    // if everything is in the cache.\n    reader.close();\n    reader = null;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766","02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, false);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":["8b7a1bc6030c258e47d63eff3455a2b1bbf32683"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, false);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, false);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ced66195b26fdb1f77ee00e2a77ec6918dedd766","date":1344948886,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":["8b7a1bc6030c258e47d63eff3455a2b1bbf32683"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      int base = 0;\n      for (AtomicReader r : reader.getSequentialSubReaders()) {\n        Terms terms = r.terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + base);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n        base += r.maxDoc(); // we don't have any deletions, so we're ok\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d58d08788c3fd51172ba34474cca42499d6391b","date":1354802133,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d90771c07d45c6ad884c5ef9cb3a6eeb257238d1","date":1357499264,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              CategoryPath cp = new CategoryPath(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              CategoryPath cp = new CategoryPath(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      CategoryPath cp = new CategoryPath();\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              cp.clear();\n              cp.add(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, 0);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c190847801a50f4dd20fd639bdc29b54ea3b288b","date":1384461522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              CategoryPath cp = new CategoryPath(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bbe9946d3cd818d329255e4c325597ac3480503e","date":1385580921,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              CategoryPath cp = new CategoryPath(t.utf8ToString(), delimiter);\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (AtomicReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.FLAG_NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          // TODO: share per-segment TermsEnum here!\n          TermsEnum termsEnum = terms.iterator();\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          termsEnum = terms.iterator(termsEnum);\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter#perhapsFillCache().mjava","sourceNew":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          // TODO: share per-segment TermsEnum here!\n          TermsEnum termsEnum = terms.iterator();\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","sourceOld":"  // we need to guarantee that if several threads call this concurrently, only\n  // one executes it, and after it returns, the cache is updated and is either\n  // complete or not.\n  private synchronized void perhapsFillCache() throws IOException {\n    if (cacheMisses.get() < cacheMissesUntilFill) {\n      return;\n    }\n    \n    if (!shouldFillCache) {\n      // we already filled the cache once, there's no need to re-fill it\n      return;\n    }\n    shouldFillCache = false;\n    \n    initReaderManager();\n\n    boolean aborted = false;\n    DirectoryReader reader = readerManager.acquire();\n    try {\n      PostingsEnum postingsEnum = null;\n      for (LeafReaderContext ctx : reader.leaves()) {\n        Terms terms = ctx.reader().terms(Consts.FULL);\n        if (terms != null) { // cannot really happen, but be on the safe side\n          // TODO: share per-segment TermsEnum here!\n          TermsEnum termsEnum = terms.iterator();\n          while (termsEnum.next() != null) {\n            if (!cache.isFull()) {\n              BytesRef t = termsEnum.term();\n              // Since we guarantee uniqueness of categories, each term has exactly\n              // one document. Also, since we do not allow removing categories (and\n              // hence documents), there are no deletions in the index. Therefore, it\n              // is sufficient to call next(), and then doc(), exactly once with no\n              // 'validation' checks.\n              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n              boolean res = cache.put(cp, postingsEnum.nextDoc() + ctx.docBase);\n              assert !res : \"entries should not have been evicted from the cache\";\n            } else {\n              // the cache is full and the next put() will evict entries from it, therefore abort the iteration.\n              aborted = true;\n              break;\n            }\n          }\n        }\n        if (aborted) {\n          break;\n        }\n      }\n    } finally {\n      readerManager.release(reader);\n    }\n\n    cacheIsComplete = !aborted;\n    if (cacheIsComplete) {\n      synchronized (this) {\n        // everything is in the cache, so no need to keep readerManager open.\n        // this block is executed in a sync block so that it works well with\n        // initReaderManager called in parallel.\n        readerManager.close();\n        readerManager = null;\n        initializedReaderManager = false;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["407687e67faf6e1f02a211ca078d8e3eed631027","d90771c07d45c6ad884c5ef9cb3a6eeb257238d1"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"3d58d08788c3fd51172ba34474cca42499d6391b":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d6f074e73200c07d54f242d3880a8da5a35ff97b","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["d90771c07d45c6ad884c5ef9cb3a6eeb257238d1"],"d90771c07d45c6ad884c5ef9cb3a6eeb257238d1":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["d90771c07d45c6ad884c5ef9cb3a6eeb257238d1","bbe9946d3cd818d329255e4c325597ac3480503e"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["02331260bb246364779cb6f04919ca47900d01bb"],"ef0d8a69209261514c5739c770bba706c2308450":["b89678825b68eccaf09e6ab71675fc0b0af1e099","119bc02554a192b2954b73d79389ec441257b624"],"bbe9946d3cd818d329255e4c325597ac3480503e":["c190847801a50f4dd20fd639bdc29b54ea3b288b"],"119bc02554a192b2954b73d79389ec441257b624":["533890d1266aa8169162ec556395d0c5d0377566"],"407687e67faf6e1f02a211ca078d8e3eed631027":["ced66195b26fdb1f77ee00e2a77ec6918dedd766","3d58d08788c3fd51172ba34474cca42499d6391b"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["3d58d08788c3fd51172ba34474cca42499d6391b"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["8b7a1bc6030c258e47d63eff3455a2b1bbf32683","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["8b7a1bc6030c258e47d63eff3455a2b1bbf32683","02331260bb246364779cb6f04919ca47900d01bb"],"8b7a1bc6030c258e47d63eff3455a2b1bbf32683":["119bc02554a192b2954b73d79389ec441257b624"],"533890d1266aa8169162ec556395d0c5d0377566":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"02331260bb246364779cb6f04919ca47900d01bb":["8b7a1bc6030c258e47d63eff3455a2b1bbf32683"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3d58d08788c3fd51172ba34474cca42499d6391b":["407687e67faf6e1f02a211ca078d8e3eed631027","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["bbe9946d3cd818d329255e4c325597ac3480503e"],"d90771c07d45c6ad884c5ef9cb3a6eeb257238d1":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","c190847801a50f4dd20fd639bdc29b54ea3b288b","3cc728b07df73b197e6d940d27f9b08b63918f13"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","3d58d08788c3fd51172ba34474cca42499d6391b","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","407687e67faf6e1f02a211ca078d8e3eed631027"],"ef0d8a69209261514c5739c770bba706c2308450":[],"bbe9946d3cd818d329255e4c325597ac3480503e":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"119bc02554a192b2954b73d79389ec441257b624":["ef0d8a69209261514c5739c770bba706c2308450","8b7a1bc6030c258e47d63eff3455a2b1bbf32683"],"407687e67faf6e1f02a211ca078d8e3eed631027":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["ef0d8a69209261514c5739c770bba706c2308450","533890d1266aa8169162ec556395d0c5d0377566"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d90771c07d45c6ad884c5ef9cb3a6eeb257238d1"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"8b7a1bc6030c258e47d63eff3455a2b1bbf32683":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"533890d1266aa8169162ec556395d0c5d0377566":["119bc02554a192b2954b73d79389ec441257b624"],"02331260bb246364779cb6f04919ca47900d01bb":["ced66195b26fdb1f77ee00e2a77ec6918dedd766","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","ef0d8a69209261514c5739c770bba706c2308450","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}