{"path":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","commits":[{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d47f68d60cbff5718136b945ba8c55982342f38","date":1285583375,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1f653cfcf159baeaafe5d01682a911e95bba4012":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"8d47f68d60cbff5718136b945ba8c55982342f38":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8d47f68d60cbff5718136b945ba8c55982342f38"]},"commit2Childs":{"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["8d47f68d60cbff5718136b945ba8c55982342f38"],"8d47f68d60cbff5718136b945ba8c55982342f38":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}