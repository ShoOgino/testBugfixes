{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[],boolean).mjava","commits":[{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":1,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[]).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, true)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, true);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, true);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).leaderZkNodeParentVersion;\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":["f291d2d430e8149d24fdd06b0bcdab0941ec9144"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"22859cb40e09867e7da8de84a31956c07259f82f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["22859cb40e09867e7da8de84a31956c07259f82f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"]},"commit2Childs":{"22859cb40e09867e7da8de84a31956c07259f82f":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["22859cb40e09867e7da8de84a31956c07259f82f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}