{"path":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = new BytesRef(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15","date":1322511317,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !term.startsWith(startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0445bcd8433e331f296f5502fc089b336cbac3a6","date":1322630375,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                int base = sub.slice.start;\n                for (;;) {\n                  int nDocs = sub.docsEnum.read();\n                  if (nDocs == 0) break;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    if (fastForRandomSet.exists(docArr[i]+base)) c++;\n                  }\n                }\n              }\n            } else {\n\n              // this should be the same bulk result object if sharing of the docsEnum succeeded\n              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n              for (;;) {\n                int nDocs = docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i])) c++;\n                }\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getIndexReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = MultiFields.getFields(r);\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = MultiFields.getLiveDocs(r);\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    InvertedFields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    InvertedFields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, 0);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, 0);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, 0);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, false);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, 0);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, 0);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes, true) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<Integer>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRef charsRef = new CharsRef(10);\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    AtomicReader r = searcher.getAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.postingsEnum = postingsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            postingsEnum = deState.postingsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.FLAG_NONE);\n            c=0;\n\n            if (postingsEnum instanceof MultiPostingsEnum) {\n              MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n              int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.postingsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    DocsEnum docsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.docsEnum = docsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            docsEnum = deState.docsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);\n            c=0;\n\n            if (docsEnum instanceof MultiDocsEnum) {\n              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.docsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.postingsEnum = postingsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            postingsEnum = deState.postingsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n            c=0;\n\n            if (postingsEnum instanceof MultiPostingsEnum) {\n              MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n              int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.postingsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.postingsEnum = postingsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            postingsEnum = deState.postingsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.FLAG_NONE);\n            c=0;\n\n            if (postingsEnum instanceof MultiPostingsEnum) {\n              MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n              int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.postingsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c5280f6286c7546ab75b72c663f7bb1dc10e96","date":1427372570,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix, String contains, boolean ignoreCase)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n    \n    BytesRef containsTermBytes = null;\n    if (contains != null) {\n      String indexedContains = ft.toInternal(contains);\n      containsTermBytes = new BytesRef(indexedContains);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (containsTermBytes == null || StringHelper.contains(term, containsTermBytes, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              c = searcher.numDocs(docs, deState);\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) c++;\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) c++;\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.postingsEnum = postingsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            postingsEnum = deState.postingsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n            c=0;\n\n            if (postingsEnum instanceof MultiPostingsEnum) {\n              MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n              int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.postingsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["aa62c79a7afa4d7f22e1f71b883659f3213d7db1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef startTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes))\n          break;\n\n        int df = termsEnum.docFreq();\n\n        // If we are sorting, we can use df>min (rather than >=) since we\n        // are going in index order.  For certain term distributions this can\n        // make a large difference (for example, many terms with df=1).\n        if (df>0 && df>min) {\n          int c;\n\n          if (df >= minDfFilterCache) {\n            // use the filter cache\n\n            if (deState==null) {\n              deState = new SolrIndexSearcher.DocsEnumState();\n              deState.fieldName = field;\n              deState.liveDocs = r.getLiveDocs();\n              deState.termsEnum = termsEnum;\n              deState.postingsEnum = postingsEnum;\n            }\n\n            c = searcher.numDocs(docs, deState);\n\n            postingsEnum = deState.postingsEnum;\n          } else {\n            // iterate over TermDocs to calculate the intersection\n\n            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n            postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n            c=0;\n\n            if (postingsEnum instanceof MultiPostingsEnum) {\n              MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n              int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n              for (int subindex = 0; subindex<numSubs; subindex++) {\n                MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                if (sub.postingsEnum == null) continue;\n                int base = sub.slice.start;\n                int docid;\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid+base)) c++;\n                }\n              }\n            } else {\n              int docid;\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            }\n            \n\n          }\n\n          if (sortByCount) {\n            if (c>min) {\n              BytesRef termCopy = BytesRef.deepCopyOf(term);\n              queue.add(new CountPair<>(termCopy, c));\n              if (queue.size()>=maxsize) min=queue.last().val;\n            }\n          } else {\n            if (c >= mincount && --off<0) {\n              if (--lim<0) break;\n              ft.indexedToReadable(term, charsRef);\n              res.add(charsRef.toString(), c);\n            }\n          }\n        }\n\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["02331260bb246364779cb6f04919ca47900d01bb","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["e6e919043fa85ee891123768dd655a98edbbf63c"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4","52c5280f6286c7546ab75b72c663f7bb1dc10e96"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"e6e919043fa85ee891123768dd655a98edbbf63c":["3cc749c053615f5871f3b95715fe292f34e70a53"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"0445bcd8433e331f296f5502fc089b336cbac3a6":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"3cc749c053615f5871f3b95715fe292f34e70a53":["c26f00b574427b55127e869b935845554afde1fa"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["872cff1d3a554e0cd64014cd97f88d3002b0f491","96d207426bd26fa5c1014e26d21d87603aea68b7"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["02331260bb246364779cb6f04919ca47900d01bb"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["0445bcd8433e331f296f5502fc089b336cbac3a6","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["52c5280f6286c7546ab75b72c663f7bb1dc10e96"],"02331260bb246364779cb6f04919ca47900d01bb":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c26f00b574427b55127e869b935845554afde1fa":["3cc749c053615f5871f3b95715fe292f34e70a53"],"61f30939a6ca0891c7b0c0f34aa43800bd4c9a15":["0445bcd8433e331f296f5502fc089b336cbac3a6"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"e6e919043fa85ee891123768dd655a98edbbf63c":["61f30939a6ca0891c7b0c0f34aa43800bd4c9a15"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"0445bcd8433e331f296f5502fc089b336cbac3a6":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"3cc749c053615f5871f3b95715fe292f34e70a53":["e6e919043fa85ee891123768dd655a98edbbf63c"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["52c5280f6286c7546ab75b72c663f7bb1dc10e96","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec"],"02331260bb246364779cb6f04919ca47900d01bb":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","15250ca94ba8ab3bcdd476daf6bf3f3febb92640","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","b65b350ca9588f9fc76ce7d6804160d06c45ff42","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}