{"path":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5c4531fadbecf73a7716fdf5cd742463e866e84e","date":1273851662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":["2123bddbd65dea198cac380540636ce43a880403"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.next();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        writer.optimize();\n        FlexTestUtil.verifyFlexVsPreFlex(rnd, writer);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d47f68d60cbff5718136b945ba8c55982342f38","date":1285583375,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Random,Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Random random, Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0638cded77f278f667f33442bd4a132d484068d1","date":1286833084,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a13a126d15299d5c1e117ea99ddae6fb0fa3f209","date":1291909583,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                    offset += br.length;\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newInOrderLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a81d4e03e5fe1327c36803b2b698afcbc96eca0b","date":1309376774,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getDeletedDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getDeletedDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getDeletedDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d","date":1315818042,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["a81d4e03e5fe1327c36803b2b698afcbc96eca0b"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["dca8d536ba2e4aab4623a172a22cc2885ec7cb3d"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"c19f985e36a65cc969e8e564fe337a0d41512075":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["c19f985e36a65cc969e8e564fe337a0d41512075"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","c19f985e36a65cc969e8e564fe337a0d41512075"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["5c4531fadbecf73a7716fdf5cd742463e866e84e"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["0638cded77f278f667f33442bd4a132d484068d1"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a81d4e03e5fe1327c36803b2b698afcbc96eca0b":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","01e5948db9a07144112d2f08f28ca2e3cd880348"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["0638cded77f278f667f33442bd4a132d484068d1","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"5f4e87790277826a2aea119328600dfb07761f32":["5c4531fadbecf73a7716fdf5cd742463e866e84e","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"8d47f68d60cbff5718136b945ba8c55982342f38":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["01e5948db9a07144112d2f08f28ca2e3cd880348","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c19f985e36a65cc969e8e564fe337a0d41512075","01e5948db9a07144112d2f08f28ca2e3cd880348"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"5c4531fadbecf73a7716fdf5cd742463e866e84e":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"0638cded77f278f667f33442bd4a132d484068d1":["8d47f68d60cbff5718136b945ba8c55982342f38"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["1509f151d7692d84fae414b2b799ac06ba60fcb4","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["5c4531fadbecf73a7716fdf5cd742463e866e84e"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["a81d4e03e5fe1327c36803b2b698afcbc96eca0b","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233","45669a651c970812a680841b97a77cce06af559f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["dca8d536ba2e4aab4623a172a22cc2885ec7cb3d"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["8d47f68d60cbff5718136b945ba8c55982342f38"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["b21422ff1d1d56499dec481f193b402e5e8def5b","5f4e87790277826a2aea119328600dfb07761f32"],"dca8d536ba2e4aab4623a172a22cc2885ec7cb3d":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"a81d4e03e5fe1327c36803b2b698afcbc96eca0b":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"8d47f68d60cbff5718136b945ba8c55982342f38":["0638cded77f278f667f33442bd4a132d484068d1"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","31f025ae60076ae95274433f3fe8e6ace2857a87"],"5c4531fadbecf73a7716fdf5cd742463e866e84e":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"45669a651c970812a680841b97a77cce06af559f":[],"0638cded77f278f667f33442bd4a132d484068d1":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}