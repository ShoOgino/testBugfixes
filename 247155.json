{"path":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","commits":[{"id":"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3","date":1373907993,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","pathOld":"/dev/null","sourceNew":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3aad8246db872dc16fbe6109f893457496b0240","date":1373920172,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","sourceNew":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","pathOld":"/dev/null","sourceNew":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7acd952b8ec320606434716bd02faaec540c885","date":1376495743,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":["ec083aa3f3ecd55f91c47009d49e45553f99bd77","4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":4,"author":"Han Jiang","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","sourceNew":null,"sourceOld":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b7acd952b8ec320606434716bd02faaec540c885":["b3aad8246db872dc16fbe6109f893457496b0240"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3aad8246db872dc16fbe6109f893457496b0240"],"b3aad8246db872dc16fbe6109f893457496b0240":["33ba398fa7984fdcb45fd76b87504d5adf7ca5e3"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["37a0f60745e53927c4c876cfe5b5a58170f0646c","b7acd952b8ec320606434716bd02faaec540c885"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b7acd952b8ec320606434716bd02faaec540c885"]},"commit2Childs":{"b7acd952b8ec320606434716bd02faaec540c885":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"b3aad8246db872dc16fbe6109f893457496b0240":["b7acd952b8ec320606434716bd02faaec540c885","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["37a0f60745e53927c4c876cfe5b5a58170f0646c","33ba398fa7984fdcb45fd76b87504d5adf7ca5e3"],"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3":["b3aad8246db872dc16fbe6109f893457496b0240"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}