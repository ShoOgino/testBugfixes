{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","pathOld":"/dev/null","sourceNew":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastDocID == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastDocID = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < hashSize/2 && hashSize >= 2)\n              hashSize /= 2;\n            hashSize *= 2;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"290ddf70ef4230015d4b5ff6758c630a466d757c","date":1183739680,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","sourceNew":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastDocID == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastDocID = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n              hashSize >>= 1;\n            hashSize <<= 1;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","sourceOld":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastDocID == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastDocID = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < hashSize/2 && hashSize >= 2)\n              hashSize /= 2;\n            hashSize *= 2;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","sourceNew":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastGen == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastGen = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n              hashSize >>= 1;\n            hashSize <<= 1;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","sourceOld":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastDocID == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastDocID = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n              hashSize >>= 1;\n            hashSize <<= 1;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#trimFields().mjava","sourceNew":null,"sourceOld":"    /** If there are fields we've seen but did not see again\n     *  in the last run, then free them up.  Also reduce\n     *  postings hash size. */\n    void trimFields() {\n\n      int upto = 0;\n      for(int i=0;i<numAllFieldData;i++) {\n        FieldData fp = allFieldDataArray[i];\n        if (fp.lastGen == -1) {\n          // This field was not seen since the previous\n          // flush, so, free up its resources now\n\n          // Unhash\n          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n          FieldData last = null;\n          FieldData fp0 = fieldDataHash[hashPos];\n          while(fp0 != fp) {\n            last = fp0;\n            fp0 = fp0.next;\n          }\n          assert fp0 != null;\n\n          if (last == null)\n            fieldDataHash[hashPos] = fp.next;\n          else\n            last.next = fp.next;\n\n          if (infoStream != null)\n            infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n        } else {\n          // Reset\n          fp.lastGen = -1;\n          allFieldDataArray[upto++] = fp;\n          \n          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n            int hashSize = fp.postingsHashSize;\n\n            // Reduce hash so it's between 25-50% full\n            while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n              hashSize >>= 1;\n            hashSize <<= 1;\n\n            if (hashSize != fp.postingsHash.length)\n              fp.rehashPostings(hashSize);\n          }\n        }\n      }\n\n      // If we didn't see any norms for this field since\n      // last flush, free it\n      for(int i=0;i<norms.length;i++) {\n        BufferedNorms n = norms[i];\n        if (n != null && n.upto == 0)\n          norms[i] = null;\n      }\n\n      numAllFieldData = upto;\n\n      // Also pare back PostingsVectors if it's excessively\n      // large\n      if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n        final int newSize;\n        if (0 == maxPostingsVectors)\n          newSize = 1;\n        else\n          newSize = (int) (1.5*maxPostingsVectors);\n        PostingVector[] newArray = new PostingVector[newSize];\n        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n        postingsVectors = newArray;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["290ddf70ef4230015d4b5ff6758c630a466d757c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a0af3a442be522899177e5e11384a45a6784a3f":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"290ddf70ef4230015d4b5ff6758c630a466d757c":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a0af3a442be522899177e5e11384a45a6784a3f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"290ddf70ef4230015d4b5ff6758c630a466d757c":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["290ddf70ef4230015d4b5ff6758c630a466d757c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}