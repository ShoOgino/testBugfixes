{"path":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","commits":[{"id":"87ac1f6aeca005a4532bea348c9a2c416eb0b4ec","date":1363331426,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"/dev/null","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // TODO: use a stable sort instead?\n    sorter.quickSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d666f5b596b5ce43296739e9453ff761c7ddde3f","date":1363451610,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.timSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // TODO: use a stable sort instead?\n    sorter.quickSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d1c249f01722fe2de6d60de2f0aade417fbb638","date":1365517193,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.timSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.timSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dcc555744b1a581a4beccd0b75f8d3fe49735a2f","date":1367588265,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    SorterTemplate sorter = new DocValueSorterTemplate(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.timSort(0, docs.length - 1); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","date":1373959221,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b221df109c657b2edfbf4b24f58ed360f1750e00","date":1394123961,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4637747f71df783fc2014ef1f1e0418466e3bed6","date":1394196311,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":["87ac1f6aeca005a4532bea348c9a2c416eb0b4ec"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  protected static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd","date":1404226546,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final PackedLongValues.Builder newToOldBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOldBuilder.add(docs[i]);\n    }\n    final PackedLongValues newToOld = newToOldBuilder.build();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final PackedLongValues.Builder oldToNewBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNewBuilder.add(docs[i]);\n    }\n    final PackedLongValues oldToNew = oldToNewBuilder.build();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final MonotonicAppendingLongBuffer newToOld = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOld.add(docs[i]);\n    }\n    newToOld.freeze();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final MonotonicAppendingLongBuffer oldToNew = new MonotonicAppendingLongBuffer();\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNew.add(docs[i]);\n    }\n    oldToNew.freeze();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e","date":1419346542,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/Sorter#sort(int,DocComparator).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter#sort(int,DocComparator).mjava","sourceNew":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final PackedLongValues.Builder newToOldBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOldBuilder.add(docs[i]);\n    }\n    final PackedLongValues newToOld = newToOldBuilder.build();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final PackedLongValues.Builder oldToNewBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNewBuilder.add(docs[i]);\n    }\n    final PackedLongValues oldToNew = oldToNewBuilder.build();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","sourceOld":"  /** Computes the old-to-new permutation over the given comparator. */\n  private static Sorter.DocMap sort(final int maxDoc, DocComparator comparator) {\n    // check if the index is sorted\n    boolean sorted = true;\n    for (int i = 1; i < maxDoc; ++i) {\n      if (comparator.compare(i-1, i) > 0) {\n        sorted = false;\n        break;\n      }\n    }\n    if (sorted) {\n      return null;\n    }\n\n    // sort doc IDs\n    final int[] docs = new int[maxDoc];\n    for (int i = 0; i < maxDoc; i++) {\n      docs[i] = i;\n    }\n    \n    DocValueSorter sorter = new DocValueSorter(docs, comparator);\n    // It can be common to sort a reader, add docs, sort it again, ... and in\n    // that case timSort can save a lot of time\n    sorter.sort(0, docs.length); // docs is now the newToOld mapping\n\n    // The reason why we use MonotonicAppendingLongBuffer here is that it\n    // wastes very little memory if the index is in random order but can save\n    // a lot of memory if the index is already \"almost\" sorted\n    final PackedLongValues.Builder newToOldBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      newToOldBuilder.add(docs[i]);\n    }\n    final PackedLongValues newToOld = newToOldBuilder.build();\n\n    for (int i = 0; i < maxDoc; ++i) {\n      docs[(int) newToOld.get(i)] = i;\n    } // docs is now the oldToNew mapping\n\n    final PackedLongValues.Builder oldToNewBuilder = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    for (int i = 0; i < maxDoc; ++i) {\n      oldToNewBuilder.add(docs[i]);\n    }\n    final PackedLongValues oldToNew = oldToNewBuilder.build();\n    \n    return new Sorter.DocMap() {\n\n      @Override\n      public int oldToNew(int docID) {\n        return (int) oldToNew.get(docID);\n      }\n\n      @Override\n      public int newToOld(int docID) {\n        return (int) newToOld.get(docID);\n      }\n\n      @Override\n      public int size() {\n        return maxDoc;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"b221df109c657b2edfbf4b24f58ed360f1750e00":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","4637747f71df783fc2014ef1f1e0418466e3bed6"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f","3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa"],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["d666f5b596b5ce43296739e9453ff761c7ddde3f"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"d666f5b596b5ce43296739e9453ff761c7ddde3f":["87ac1f6aeca005a4532bea348c9a2c416eb0b4ec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"87ac1f6aeca005a4532bea348c9a2c416eb0b4ec":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","b221df109c657b2edfbf4b24f58ed360f1750e00"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["4637747f71df783fc2014ef1f1e0418466e3bed6"]},"commit2Childs":{"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa":["b221df109c657b2edfbf4b24f58ed360f1750e00","96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","4637747f71df783fc2014ef1f1e0418466e3bed6"],"b221df109c657b2edfbf4b24f58ed360f1750e00":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d666f5b596b5ce43296739e9453ff761c7ddde3f":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["87ac1f6aeca005a4532bea348c9a2c416eb0b4ec"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["96ea64d994d340044e0d57aeb6a5871539d10ca5","aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"87ac1f6aeca005a4532bea348c9a2c416eb0b4ec":["d666f5b596b5ce43296739e9453ff761c7ddde3f"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}