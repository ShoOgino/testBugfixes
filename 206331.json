{"path":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","commits":[{"id":"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8","date":1252682465,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n    StatsValues allstats = new StatsValues();\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n    }\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n    }\n    if (allstats.getCount() > 0) {\n      if (finfo.length > 0) {\n        allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n        for (FieldFacetStats f : finfo) {\n          allstats.facets.put(f.name, f.facetStatsValues);\n        }\n      }\n      return allstats;\n    } else {\n      return null;\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","1e210ae1e604402eb4eeff2a52e56d189cd4f2f1","a56da5612f2fd2e0e08ecc709069a20adc9ab234","5e62bfc99fea332bfdcdca0f73a821428d533279"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1c2dfb6aa7b4d342a493c0982be96b76f595add","date":1252705594,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n    StatsValues allstats = new StatsValues();\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n    }\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n    }\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        allstats.facets.put(f.name, f.facetStatsValues);\n      }\n    }\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n    StatsValues allstats = new StatsValues();\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n    }\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n    }\n    if (allstats.getCount() > 0) {\n      if (finfo.length > 0) {\n        allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n        for (FieldFacetStats f : finfo) {\n          allstats.facets.put(f.name, f.facetStatsValues);\n        }\n      }\n      return allstats;\n    } else {\n      return null;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["a56da5612f2fd2e0e08ecc709069a20adc9ab234"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a56da5612f2fd2e0e08ecc709069a20adc9ab234","date":1254490277,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n      FieldType ft = searcher.getSchema().getFieldType(field);\n\n      int i = 0;\n      final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n      //Initialize facetstats, if facets have been passed in\n      FieldCache.StringIndex si;\n      for (String f : facet) {\n        ft = searcher.getSchema().getFieldType(f);\n        try {\n          si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n        }\n        catch (IOException e) {\n          throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n        }\n        finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n      }\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n      if (finfo.length > 0) {\n        allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n        for (FieldFacetStats f : finfo) {\n          allstats.facets.put(f.name, f.facetStatsValues);\n        }\n      }\n    }\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n    StatsValues allstats = new StatsValues();\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n    }\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n    }\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        allstats.facets.put(f.name, f.facetStatsValues);\n      }\n    }\n    return allstats;\n\n  }\n\n","bugFix":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8","c1c2dfb6aa7b4d342a493c0982be96b76f595add"],"bugIntro":["5e62bfc99fea332bfdcdca0f73a821428d533279"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5e62bfc99fea332bfdcdca0f73a821428d533279","date":1254838400,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize > 0) {\n      FieldType ft = searcher.getSchema().getFieldType(field);\n\n      int i = 0;\n      final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n      //Initialize facetstats, if facets have been passed in\n      FieldCache.StringIndex si;\n      for (String f : facet) {\n        ft = searcher.getSchema().getFieldType(f);\n        try {\n          si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n        }\n        catch (IOException e) {\n          throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n        }\n        finfo[i++] = new FieldFacetStats(f, si, ft, numTermsInField);\n      }\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n      boolean doNegative = false;\n      if (finfo.length == 0) {\n        //if we're collecting statistics with a facet field, can't do inverted counting\n        doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n                && docs instanceof BitDocSet;\n      }\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n          if (finfo.length == 0) {\n            counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n          } else {\n            //COULD BE VERY SLOW\n            //if we're collecting stats for facet fields, we need to iterate on all matching documents\n            DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n            DocIterator iter = bigTermDocSet.iterator();\n            while (iter.hasNext()) {\n              int doc = iter.nextDoc();\n              counts[tt.termNum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tt.termNum);\n              }\n            }\n          }\n        }\n      }\n\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff) == 1) {\n            int pos = code >>> 8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for (; ;) {\n              int delta = 0;\n              for (; ;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (; ;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80) == 0) {\n                if (delta == 0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                for (FieldFacetStats f : finfo) {\n                  f.facetTermNum(doc, tnum);\n                }\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      // add results in index order\n\n      for (i = 0; i < numTermsInField; i++) {\n        int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n        if (c == 0) {\n          continue;\n        }\n        Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n        allstats.accumulate(value, c);\n        //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n        for (FieldFacetStats f : finfo) {\n          f.accumulateTermNum(i, value);\n        }\n      }\n      te.close();\n      int c = SimpleFacets.getFieldMissingCount(searcher, baseDocs, field);\n      if (c > 0) {\n        allstats.addMissing(c);\n      }\n      if (finfo.length > 0) {\n        allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n        for (FieldFacetStats f : finfo) {\n          allstats.facets.put(f.name, f.facetStatsValues);\n        }\n      }\n    }\n    return allstats;\n\n  }\n\n","bugFix":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8","a56da5612f2fd2e0e08ecc709069a20adc9ab234"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a56da5612f2fd2e0e08ecc709069a20adc9ab234":["c1c2dfb6aa7b4d342a493c0982be96b76f595add"],"c1c2dfb6aa7b4d342a493c0982be96b76f595add":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"5e62bfc99fea332bfdcdca0f73a821428d533279":["a56da5612f2fd2e0e08ecc709069a20adc9ab234"],"ad94625fb8d088209f46650c8097196fec67f00c":["5e62bfc99fea332bfdcdca0f73a821428d533279"],"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a56da5612f2fd2e0e08ecc709069a20adc9ab234":["5e62bfc99fea332bfdcdca0f73a821428d533279"],"c1c2dfb6aa7b4d342a493c0982be96b76f595add":["a56da5612f2fd2e0e08ecc709069a20adc9ab234"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8"],"5e62bfc99fea332bfdcdca0f73a821428d533279":["ad94625fb8d088209f46650c8097196fec67f00c"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8":["c1c2dfb6aa7b4d342a493c0982be96b76f595add"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}