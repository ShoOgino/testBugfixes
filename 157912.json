{"path":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#saveDfsUsed().mjava","commits":[{"id":"44ca189138a5b6e1989d12ab992fab60e235ddc7","date":1549051496,"type":0,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#saveDfsUsed().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Write the current dfsUsed to the cache file.\n   */\n  void saveDfsUsed() {\n    File outFile = new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n          outFile.getParent());\n    }\n\n    try {\n      long used = getDfsUsed();\n      try (Writer out = new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won't be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        // This is only called as part of the volume shutdown.\n        // We explicitly avoid calling flush with fileIoProvider which triggers\n        // volume check upon io exception to avoid cyclic volume checks.\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bdf107cf16be0f22504ae184fed81596665a244","date":1576012524,"type":4,"author":"Kevin Risden","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#saveDfsUsed().mjava","sourceNew":null,"sourceOld":"  /**\n   * Write the current dfsUsed to the cache file.\n   */\n  void saveDfsUsed() {\n    File outFile = new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n          outFile.getParent());\n    }\n\n    try {\n      long used = getDfsUsed();\n      try (Writer out = new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won't be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        // This is only called as part of the volume shutdown.\n        // We explicitly avoid calling flush with fileIoProvider which triggers\n        // volume check upon io exception to avoid cyclic volume checks.\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a229cb50768e988c50a2106bdae3a92154f428bf","date":1576051038,"type":4,"author":"Dawid Weiss","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#saveDfsUsed().mjava","sourceNew":null,"sourceOld":"  /**\n   * Write the current dfsUsed to the cache file.\n   */\n  void saveDfsUsed() {\n    File outFile = new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n          outFile.getParent());\n    }\n\n    try {\n      long used = getDfsUsed();\n      try (Writer out = new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won't be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        // This is only called as part of the volume shutdown.\n        // We explicitly avoid calling flush with fileIoProvider which triggers\n        // volume check upon io exception to avoid cyclic volume checks.\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a229cb50768e988c50a2106bdae3a92154f428bf":["44ca189138a5b6e1989d12ab992fab60e235ddc7","6bdf107cf16be0f22504ae184fed81596665a244"],"6bdf107cf16be0f22504ae184fed81596665a244":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6bdf107cf16be0f22504ae184fed81596665a244"]},"commit2Childs":{"a229cb50768e988c50a2106bdae3a92154f428bf":[],"6bdf107cf16be0f22504ae184fed81596665a244":["a229cb50768e988c50a2106bdae3a92154f428bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["a229cb50768e988c50a2106bdae3a92154f428bf","6bdf107cf16be0f22504ae184fed81596665a244"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a229cb50768e988c50a2106bdae3a92154f428bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}