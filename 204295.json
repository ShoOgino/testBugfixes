{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eafa8c5eabc3dacd34680054e6a33bda024080ac","date":1367691488,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, texts[0]);\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, texts[0]);\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aeebe27bce18b879b80f68494c52cda1021b5705","date":1417792137,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the\n    // tokenstream and call end() before incrementToken() returns false.\n    // But we first need to clear the re-used tokenstream components that have enableChecks.\n    analyzer.getReuseStrategy().setReusableComponents(analyzer, FIELD_NAME, null);\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, texts[0]);\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, texts[0]);\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["eafa8c5eabc3dacd34680054e6a33bda024080ac","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["aeebe27bce18b879b80f68494c52cda1021b5705"],"aeebe27bce18b879b80f68494c52cda1021b5705":["c83d6c4335f31cae14f625a222bc842f20073dcd"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","aeebe27bce18b879b80f68494c52cda1021b5705"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"aeebe27bce18b879b80f68494c52cda1021b5705":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}