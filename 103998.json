{"path":"contrib/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","commits":[{"id":"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a","date":1107704112,"type":1,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4df60738409662c962b4ed3201d830cd3c14530","date":1114991851,"type":5,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a4df60738409662c962b4ed3201d830cd3c14530"],"a4df60738409662c962b4ed3201d830cd3c14530":["a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a"],"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a":["a4df60738409662c962b4ed3201d830cd3c14530"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"a4df60738409662c962b4ed3201d830cd3c14530":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}