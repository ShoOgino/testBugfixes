{"path":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","commits":[{"id":"ea4107f60b9f95623c16025c9c247412ff809092","date":1468333987,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory();\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["1964308aac54eb27f1a144665f19e6bfa27eca60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory();\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1964308aac54eb27f1a144665f19e6bfa27eca60","date":1477578465,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory();\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":["ea4107f60b9f95623c16025c9c247412ff809092"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory();\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e24ef15e7e97c77f0829a62180966fccd2e94b5","date":1477623326,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory();\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9","date":1498572460,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98f11c416d30e556e0004a9a84960702d12d35b1","date":1498628359,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/Analyzer#normalize(String,String).mjava","sourceNew":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exception\", e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Normalize a string down to the representation that it would have in the\n   * index.\n   * <p>\n   * This is typically used by query parsers in order to generate a query on\n   * a given term, without tokenizing or stemming, which are undesirable if\n   * the string to analyze is a partial word (eg. in case of a wildcard or\n   * fuzzy query).\n   * <p>\n   * This method uses {@link #initReaderForNormalization(String, Reader)} in\n   * order to apply necessary character-level normalization and then\n   * {@link #normalize(String, TokenStream)} in order to apply the normalizing\n   * token filters.\n   */\n  public final BytesRef normalize(final String fieldName, final String text) {\n    try {\n      // apply char filters\n      final String filteredText;\n      try (Reader reader = new StringReader(text)) {\n        Reader filterReader = initReaderForNormalization(fieldName, reader);\n        char[] buffer = new char[64];\n        StringBuilder builder = new StringBuilder();\n        for (;;) {\n          final int read = filterReader.read(buffer, 0, buffer.length);\n          if (read == -1) {\n            break;\n          }\n          builder.append(buffer, 0, read);\n        }\n        filteredText = builder.toString();\n      } catch (IOException e) {\n        throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n      }\n\n      final AttributeFactory attributeFactory = attributeFactory(fieldName);\n      try (TokenStream ts = normalize(fieldName,\n          new StringTokenStream(attributeFactory, filteredText, text.length()))) {\n        final TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        if (ts.incrementToken() == false) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 0 for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        final BytesRef term = BytesRef.deepCopyOf(termAtt.getBytesRef());\n        if (ts.incrementToken()) {\n          throw new IllegalStateException(\"The normalization token stream is \"\n              + \"expected to produce exactly 1 token, but got 2+ for analyzer \"\n              + this + \" and input \\\"\" + text + \"\\\"\");\n        }\n        ts.end();\n        return term;\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Normalization threw an unexpected exeption\", e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1964308aac54eb27f1a144665f19e6bfa27eca60":["ea4107f60b9f95623c16025c9c247412ff809092"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","1964308aac54eb27f1a144665f19e6bfa27eca60"],"bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9":["7e24ef15e7e97c77f0829a62180966fccd2e94b5"],"ea4107f60b9f95623c16025c9c247412ff809092":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7e24ef15e7e97c77f0829a62180966fccd2e94b5":["ea4107f60b9f95623c16025c9c247412ff809092","1964308aac54eb27f1a144665f19e6bfa27eca60"],"28288370235ed02234a64753cdbf0c6ec096304a":["7e24ef15e7e97c77f0829a62180966fccd2e94b5","bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9"],"98f11c416d30e556e0004a9a84960702d12d35b1":["7e24ef15e7e97c77f0829a62180966fccd2e94b5","bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ea4107f60b9f95623c16025c9c247412ff809092"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"1964308aac54eb27f1a144665f19e6bfa27eca60":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","7e24ef15e7e97c77f0829a62180966fccd2e94b5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ea4107f60b9f95623c16025c9c247412ff809092","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9":["28288370235ed02234a64753cdbf0c6ec096304a","98f11c416d30e556e0004a9a84960702d12d35b1"],"ea4107f60b9f95623c16025c9c247412ff809092":["1964308aac54eb27f1a144665f19e6bfa27eca60","7e24ef15e7e97c77f0829a62180966fccd2e94b5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"7e24ef15e7e97c77f0829a62180966fccd2e94b5":["bd6174b2f3408e04d4778eb0a4a1fdb1f9f8bdc9","28288370235ed02234a64753cdbf0c6ec096304a","98f11c416d30e556e0004a9a84960702d12d35b1"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"98f11c416d30e556e0004a9a84960702d12d35b1":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","98f11c416d30e556e0004a9a84960702d12d35b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}