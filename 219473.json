{"path":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","commits":[{"id":"0beaed456aa3358e5e4a99ea2aea994ef6c81de3","date":1365434191,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"/dev/null","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collection = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collection, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collection)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collection);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collection, slice);\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    // todo: handle randomizing subshard name in case a shard with the same name already exists.\n    List<DocRouter.Range> subRanges = new PlainIdRouter().partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collection + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collection, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collection + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collection);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collection + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collection + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collection, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collection + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collection + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collection);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collection);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collection + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["5aa6dcd736e5c400d1c763ae8fa2fe5aedb75132","2dbed1dd58810b079506c1e4cd13ce80e646faed","e90f002d571348d57377e0d63fd954b0fd1da637","0220ee39df0e359431efa5115aeb0729982e3c96","4ddb837927b3de29503b68a05fec256665edab50","8b81868c96ab084d96d4589dd330753c529bc450","103857ec20f79f31c7a00310a91ed001b9a6ef17","a234e8fc54f31f81256b691a1a134f969ef3d01c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1de8c3198c585d95ab4c1fa224a5b0bf72840b15","date":1365523020,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collection = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collection, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collection)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collection);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collection, slice);\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    // todo: handle randomizing subshard name in case a shard with the same name already exists.\n    List<DocRouter.Range> subRanges = new PlainIdRouter().partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collection + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collection, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collection + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collection);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collection + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collection + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collection, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collection + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collection + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collection);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collection);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collection + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collection = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collection, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collection)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collection);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collection, slice);\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    // todo: handle randomizing subshard name in case a shard with the same name already exists.\n    List<DocRouter.Range> subRanges = new PlainIdRouter().partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collection + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collection, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collection + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collection);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collection + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collection + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collection, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collection + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collection + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collection);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collection);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collection + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["0220ee39df0e359431efa5115aeb0729982e3c96"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dbed1dd58810b079506c1e4cd13ce80e646faed","date":1368050251,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collection = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collection, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collection)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collection);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collection, slice);\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    // todo: handle randomizing subshard name in case a shard with the same name already exists.\n    List<DocRouter.Range> subRanges = new PlainIdRouter().partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collection + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collection, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collection + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collection);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collection + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collection + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collection, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collection + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collection + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collection);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collection);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collection + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50","204509ef590c8dfddd322f12e0f9fb6a0060be69"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e90f002d571348d57377e0d63fd954b0fd1da637","date":1368212480,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders or timed out waiting for them to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"bugIntro":["0220ee39df0e359431efa5115aeb0729982e3c96"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders or timed out waiting for them to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders or timed out waiting for them to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["8b81868c96ab084d96d4589dd330753c529bc450"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0220ee39df0e359431efa5115aeb0729982e3c96","date":1372771370,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders or timed out waiting for them to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["e90f002d571348d57377e0d63fd954b0fd1da637","0beaed456aa3358e5e4a99ea2aea994ef6c81de3","1de8c3198c585d95ab4c1fa224a5b0bf72840b15"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96adbab674ae121f8b6b3e10474070b4bd97a219","date":1373614333,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["457c0871b070288069b6f4422a913198f83e24ee"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              String core = replica.getStr(\"core\");\n              log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n              ModifiableSolrParams params = new ModifiableSolrParams();\n              params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n              params.set(CoreAdminParams.CORE, core);\n              params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n              sendShardRequest(replica.getNodeName(), params);\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders or timed out waiting for them to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(subShardNodeName + \"_\" + shardName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1816753738ff1f27f11b38030e83c0ded050b7a4","date":1380106089,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices); todo: is it necessary, we're not creating collections?\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e73db80cda3387e197641256d964f8c1c3992c7","date":1380978036,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    // todo: fixed to two partitions?\n    // todo: accept the range as a param to api?\n    List<DocRouter.Range> subRanges = router.partitionRange(2, range);\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"457c0871b070288069b6f4422a913198f83e24ee","date":1381238514,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = commit(coreUrl, true);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib commit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["96adbab674ae121f8b6b3e10474070b4bd97a219"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bc841231667f1f315bae6799c068f9aad6543967","date":1381415189,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()))  {\n            for (Replica replica : oSlice.getReplicas()) {\n              if (clusterState.liveNodesContain(replica.getNodeName())) {\n                String core = replica.getStr(\"core\");\n                log.info(\"Unloading core: \" + core + \" from node: \" + replica.getNodeName());\n                ModifiableSolrParams params = new ModifiableSolrParams();\n                params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n                params.set(CoreAdminParams.CORE, core);\n                params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n                sendShardRequest(replica.getNodeName(), params);\n              } else  {\n                log.warn(\"Replica {} exists in shard {} but is not live and cannot be unloaded\", replica, oSlice);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      log.info(\"Successfully created all replica shards for all sub-slices \"\n          + subSlices);\n\n      log.info(\"Requesting update shard state\");\n      DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n      Map<String, Object> propMap = new HashMap<String, Object>();\n      propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n      propMap.put(slice, Slice.INACTIVE);\n      for (String subSlice : subSlices) {\n        propMap.put(subSlice, Slice.ACTIVE);\n      }\n      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n      ZkNodeProps m = new ZkNodeProps(propMap);\n      inQueue.offer(ZkStateReader.toJSON(m));\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["a234e8fc54f31f81256b691a1a134f969ef3d01c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9d5abf772262a05c74afddcadc95c4bdab07f1f","date":1381747682,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice parentSlice = clusterState.getSlice(collectionName, slice);\n    \n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b509abc8bfbaa6d2583fdc203d0f830aeffaadb7","date":1381950977,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cee263b0163fa111cfda384934079baae77fccac","date":1382523980,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da888af1ab894358122a22229051215f58cf4d54","date":1384408702,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      log.info(\"Calling soft commit to make sub shard updates visible\");\n      String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n      // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n      // and we force open a searcher so that we have documents to show upon switching states\n      UpdateResponse updateResponse = null;\n      try {\n        updateResponse = softCommit(coreUrl);\n        processResponse(results, null, coreUrl, updateResponse, slice);\n      } catch (Exception e) {\n        processResponse(results, e, coreUrl, updateResponse, slice);\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n      }\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b711ca2887ae3fda68611cbb78c1b389fa9833be","date":1384985717,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b753f4bd40fd8ec5276ceae15e867d2dceeb5552","date":1387799061,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.getCollections().contains(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"204509ef590c8dfddd322f12e0f9fb6a0060be69","date":1391756602,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72","date":1391985588,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6bdd460d2fe72f80e52acb2774d58a4ae2e67b9b","date":1392108367,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"afd709fe548f0ddbaa0a678015e7c3d41ff1c87a","date":1392108948,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87d54fb06e5d5ca00e6b0db75b52de2013d09ce4","date":1392661838,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a234e8fc54f31f81256b691a1a134f969ef3d01c","date":1392683993,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3","bc841231667f1f315bae6799c068f9aad6543967"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8497bb4f9de61b5520423bd9af88ea11a6e109e7","date":1393245090,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e6225820ca5f401e2fe460f36e4a919a384cef6c","date":1393272577,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"058f5a3debcfa0ea477da3eabb4cbe2ec0fac211","date":1394784078,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50","8b81868c96ab084d96d4589dd330753c529bc450","ee31282189f924712ed9ad83e3073e0c207a1a53"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d","date":1395095209,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72"],"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6f26f74e4969851a019d28f10315cb1c77786f22","date":1400539241,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params, shardHandler);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["4ddb837927b3de29503b68a05fec256665edab50","ee31282189f924712ed9ad83e3073e0c207a1a53"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params, shardHandler);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ddb837927b3de29503b68a05fec256665edab50","date":1400760638,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(m));\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          if(asyncId != null) {\n            String requestId = asyncId + Math.abs(System.nanoTime());\n            params.set(ASYNC, requestId);\n            requestMap.put(subShardNodeName, requestId);\n          }\n\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params, shardHandler);\n\n          String coreNodeName = waitForCoreNodeName(collection, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["6f26f74e4969851a019d28f10315cb1c77786f22","b711ca2887ae3fda68611cbb78c1b389fa9833be","058f5a3debcfa0ea477da3eabb4cbe2ec0fac211","2dbed1dd58810b079506c1e4cd13ce80e646faed","cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d","0beaed456aa3358e5e4a99ea2aea994ef6c81de3","87d54fb06e5d5ca00e6b0db75b52de2013d09ce4"],"bugIntro":["8b81868c96ab084d96d4589dd330753c529bc450"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30c688f7052130cef7bd419c85e3c5be214f7b9e","date":1411018984,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"createshard\");\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"103857ec20f79f31c7a00310a91ed001b9a6ef17","date":1412698959,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24a5da2a0d397ff29f3de8f6cf451d3412c2509a","date":1417276391,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, Overseer.OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ee31282189f924712ed9ad83e3073e0c207a1a53","date":1427223880,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["6f26f74e4969851a019d28f10315cb1c77786f22","058f5a3debcfa0ea477da3eabb4cbe2ec0fac211"],"bugIntro":["8b81868c96ab084d96d4589dd330753c529bc450"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dbf528c0e702c5cbd1339b2da1cdc823fd44a925","date":1427230904,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            if (Slice.ACTIVE.equals(oSlice.getState())) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState())) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.INACTIVE);\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.ACTIVE);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.RECOVERY);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            if (Slice.ACTIVE.equals(oSlice.getState())) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState())) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.INACTIVE);\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.ACTIVE);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.RECOVERY);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n\n      String asyncId = message.getStr(ASYNC);\n      HashMap<String, String> requestMap = new HashMap<String, String>();\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if(asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n        sendShardRequest(nodeName, p, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      setupAsyncRequest(asyncId, requestMap, params, parentShardLeader.getNodeName());\n\n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler);\n\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n          shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        setupAsyncRequest(asyncId, requestMap, params, nodeName);\n\n        sendShardRequest(nodeName, params, shardHandler);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      Collections.shuffle(nodeList, RANDOM);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          HashMap<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n          setupAsyncRequest(asyncId, requestMap, p, nodeName);\n\n          sendShardRequest(nodeName, p, shardHandler);\n\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n          shardHandler);\n\n      completeAsyncRequest(asyncId, requestMap, results);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296","date":1427866967,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            final Slice.State state = oSlice.getState();\n            if (state == Slice.State.ACTIVE) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.State.INACTIVE.toString());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.ACTIVE.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.RECOVERY.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            if (Slice.ACTIVE.equals(oSlice.getState())) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState())) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.INACTIVE);\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.ACTIVE);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.RECOVERY);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c924d4069ef5a5bc479a493befe0121aada6896","date":1427901860,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            final Slice.State state = oSlice.getState();\n            if (state == Slice.State.ACTIVE) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.State.INACTIVE.toString());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.ACTIVE.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.RECOVERY.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            if (Slice.ACTIVE.equals(oSlice.getState())) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState())) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.CONSTRUCTION);\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.INACTIVE);\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.ACTIVE);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.RECOVERY);\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a219f1dcad1700e84807666bdbd2b573e8de7021","date":1428130940,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            final Slice.State state = oSlice.getState();\n            if (state == Slice.State.ACTIVE) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(Replica.State.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.State.INACTIVE.toString());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.ACTIVE.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.RECOVERY.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            final Slice.State state = oSlice.getState();\n            if (state == Slice.State.ACTIVE) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(ZkStateReader.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.State.INACTIVE.toString());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.ACTIVE.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.RECOVERY.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"439c63ae5d22132fca810a0029a854e97d2c1a3e","date":1432733612,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<String,String>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList, RANDOM);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n      \n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n          \n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n              \n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n          \n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i - 1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          \n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n          \n        }\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Map previousMDCContext = MDC.getCopyOfContextMap();\n    MDCUtils.setMDC(collectionName, slice, null, null);\n    try {\n      log.info(\"Split shard invoked\");\n      String splitKey = message.getStr(\"split.key\");\n      ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n\n      DocCollection collection = clusterState.getCollection(collectionName);\n      DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n      Slice parentSlice = null;\n\n      if (slice == null) {\n        if (router instanceof CompositeIdRouter) {\n          Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n          if (searchSlices.isEmpty()) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n          }\n          if (searchSlices.size() > 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n          }\n          parentSlice = searchSlices.iterator().next();\n          slice = parentSlice.getName();\n          log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n        }\n      } else {\n        parentSlice = clusterState.getSlice(collectionName, slice);\n      }\n\n      if (parentSlice == null) {\n        if (clusterState.hasCollection(collectionName)) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n        } else {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n        }\n      }\n\n      // find the leader for the shard\n      Replica parentShardLeader = null;\n      try {\n        parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n\n      DocRouter.Range range = parentSlice.getRange();\n      if (range == null) {\n        range = new PlainIdRouter().fullRange();\n      }\n\n      List<DocRouter.Range> subRanges = null;\n      String rangesStr = message.getStr(CoreAdminParams.RANGES);\n      if (rangesStr != null) {\n        String[] ranges = rangesStr.split(\",\");\n        if (ranges.length == 0 || ranges.length == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n        } else {\n          subRanges = new ArrayList<>(ranges.length);\n          for (int i = 0; i < ranges.length; i++) {\n            String r = ranges[i];\n            try {\n              subRanges.add(DocRouter.DEFAULT.fromString(r));\n            } catch (Exception e) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n            }\n            if (!subRanges.get(i).isSubsetOf(range)) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n            }\n          }\n          List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n          Collections.sort(temp);\n          if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n          }\n          for (int i = 1; i < temp.size(); i++) {\n            if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n              throw new SolrException(ErrorCode.BAD_REQUEST,\n                  \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                      \"do not cover the entire range of parent shard: \" + range);\n            }\n          }\n        }\n      } else if (splitKey != null) {\n        if (router instanceof CompositeIdRouter) {\n          CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n          subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n          if (subRanges.size() == 1) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n          }\n          for (DocRouter.Range subRange : subRanges) {\n            if (subRange.min == subRange.max) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n            }\n          }\n          log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n          rangesStr = \"\";\n          for (int i = 0; i < subRanges.size(); i++) {\n            DocRouter.Range subRange = subRanges.get(i);\n            rangesStr += subRange.toString();\n            if (i < subRanges.size() - 1)\n              rangesStr += ',';\n          }\n        }\n      } else {\n        // todo: fixed to two partitions?\n        subRanges = router.partitionRange(2, range);\n      }\n\n      try {\n        List<String> subSlices = new ArrayList<>(subRanges.size());\n        List<String> subShardNames = new ArrayList<>(subRanges.size());\n        String nodeName = parentShardLeader.getNodeName();\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = slice + \"_\" + i;\n          subSlices.add(subSlice);\n          String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n          subShardNames.add(subShardName);\n\n          Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n          if (oSlice != null) {\n            final Slice.State state = oSlice.getState();\n            if (state == Slice.State.ACTIVE) {\n              throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n            } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n              // delete the shards\n              for (String sub : subSlices) {\n                log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n                Map<String, Object> propMap = new HashMap<>();\n                propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n                propMap.put(COLLECTION_PROP, collectionName);\n                propMap.put(SHARD_ID_PROP, sub);\n                ZkNodeProps m = new ZkNodeProps(propMap);\n                try {\n                  deleteShard(clusterState, m, new NamedList());\n                } catch (Exception e) {\n                  throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n                }\n              }\n            }\n          }\n        }\n\n        // do not abort splitshard if the unloading fails\n        // this can happen because the replicas created previously may be down\n        // the only side effect of this is that the sub shard may end up having more replicas than we want\n        collectShardResponses(results, false, null, shardHandler);\n\n        String asyncId = message.getStr(ASYNC);\n        HashMap<String, String> requestMap = new HashMap<String, String>();\n\n        for (int i = 0; i < subRanges.size(); i++) {\n          String subSlice = subSlices.get(i);\n          String subShardName = subShardNames.get(i);\n          DocRouter.Range subRange = subRanges.get(i);\n\n          log.info(\"Creating slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n          propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n          propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n          propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n\n          // wait until we are able to see the new shard in cluster state\n          waitForNewShard(collectionName, subSlice);\n\n          // refresh cluster state\n          clusterState = zkStateReader.getClusterState();\n\n          log.info(\"Adding replica \" + subShardName + \" as part of slice \"\n              + subSlice + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n          propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, subSlice);\n          propMap.put(\"node\", nodeName);\n          propMap.put(CoreAdminParams.NAME, subShardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if(asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        for (String subShardName : subShardNames) {\n          // wait for parent leader to acknowledge the sub-shard core\n          log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n          String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardName);\n          cmd.setNodeName(nodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.ACTIVE);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD timed out waiting for subshard leaders to come up\", shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all sub-shards for collection \"\n            + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n        log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n            + slice + \" of collection \" + collectionName + \" on \"\n            + parentShardLeader);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n        params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n          params.add(CoreAdminParams.TARGET_CORE, subShardName);\n        }\n        params.set(CoreAdminParams.RANGES, rangesStr);\n\n        sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n        collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\",\n            shardHandler);\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n        // apply buffered updates on sub-shards\n        for (int i = 0; i < subShardNames.size(); i++) {\n          String subShardName = subShardNames.get(i);\n\n          log.info(\"Applying buffered updates on : \" + subShardName);\n\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n          params.set(CoreAdminParams.NAME, subShardName);\n\n          sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n        // Replica creation for the new Slices\n\n        // look at the replication factor and see if it matches reality\n        // if it does not, find best nodes to create more cores\n\n        // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n        int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n        // we need to look at every node and see how many cores it serves\n        // add our new cores to existing nodes serving the least number of cores\n        // but (for now) require that each core goes on a distinct node.\n\n        // TODO: add smarter options that look at the current number of cores per\n        // node?\n        // for now we just go random\n        Set<String> nodes = clusterState.getLiveNodes();\n        List<String> nodeList = new ArrayList<>(nodes.size());\n        nodeList.addAll(nodes);\n\n        Collections.shuffle(nodeList, RANDOM);\n\n        // TODO: Have maxShardsPerNode param for this operation?\n\n        // Remove the node that hosts the parent shard for replica creation.\n        nodeList.remove(nodeName);\n\n        // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n        for (int i = 1; i <= subSlices.size(); i++) {\n          Collections.shuffle(nodeList, RANDOM);\n          String sliceName = subSlices.get(i - 1);\n          for (int j = 2; j <= repFactor; j++) {\n            String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n            String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n            log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n                + sliceName + \" of collection \" + collectionName + \" on \"\n                + subShardNodeName);\n\n            HashMap<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, sliceName);\n            propMap.put(\"node\", subShardNodeName);\n            propMap.put(CoreAdminParams.NAME, shardName);\n            // copy over property params:\n            for (String key : message.keySet()) {\n              if (key.startsWith(COLL_PROP_PREFIX)) {\n                propMap.put(key, message.getStr(key));\n              }\n            }\n            // add async param\n            if (asyncId != null) {\n              propMap.put(ASYNC, asyncId);\n            }\n            addReplica(clusterState, new ZkNodeProps(propMap), results);\n\n            String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n            // wait for the replicas to be seen as active on sub shard leader\n            log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n            CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n            cmd.setCoreName(subShardNames.get(i - 1));\n            cmd.setNodeName(subShardNodeName);\n            cmd.setCoreNodeName(coreNodeName);\n            cmd.setState(Replica.State.RECOVERING);\n            cmd.setCheckLive(true);\n            cmd.setOnlyIfLeader(true);\n            ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n\n            sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n\n          }\n        }\n\n        collectShardResponses(results, true,\n            \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\",\n            shardHandler);\n\n        completeAsyncRequest(asyncId, requestMap, results);\n\n        log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n        commit(results, slice, parentShardLeader);\n\n        if (repFactor == 1) {\n          // switch sub shard states to 'active'\n          log.info(\"Replication factor is 1 so switching shard states\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          propMap.put(slice, Slice.State.INACTIVE.toString());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.ACTIVE.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        } else {\n          log.info(\"Requesting shard state be set to 'recovery'\");\n          DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n          Map<String, Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n          for (String subSlice : subSlices) {\n            propMap.put(subSlice, Slice.State.RECOVERY.toString());\n          }\n          propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n          ZkNodeProps m = new ZkNodeProps(propMap);\n          inQueue.offer(ZkStateReader.toJSON(m));\n        }\n\n        return true;\n      } catch (SolrException e) {\n        throw e;\n      } catch (Exception e) {\n        log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n        throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n      }\n    } finally {\n      MDCUtils.cleanupMDC(previousMDCContext);\n    }\n  }\n\n","bugFix":null,"bugIntro":["1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","1498cb4cd65cf2303ccde0e0d6edf5a5e1e8fd28","685af99397b6da31116a2cac747ed255d217d080","685af99397b6da31116a2cac747ed255d217d080","5aa6dcd736e5c400d1c763ae8fa2fe5aedb75132","8b81868c96ab084d96d4589dd330753c529bc450"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f222f4f2bda21ffd6b39b3362b2412e98e4d5e31","date":1435517625,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<String,String>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList, RANDOM);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n      \n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n          \n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n              \n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n          \n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i - 1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          \n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n          \n        }\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<String,String>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList, RANDOM);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n      \n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n          \n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n              \n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n          \n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i - 1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          \n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n          \n        }\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b81868c96ab084d96d4589dd330753c529bc450","date":1435652870,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<String,String>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList, RANDOM);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n      \n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n          \n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n              \n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          addReplica(clusterState, new ZkNodeProps(propMap), results);\n          \n          String coreNodeName = waitForCoreNodeName(collectionName, subShardNodeName, shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i - 1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(Replica.State.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n          \n          sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n          \n        }\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas or timed out waiting for them to come up\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04","058f5a3debcfa0ea477da3eabb4cbe2ec0fac211","ee31282189f924712ed9ad83e3073e0c207a1a53","4ddb837927b3de29503b68a05fec256665edab50","439c63ae5d22132fca810a0029a854e97d2c1a3e","0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"bugIntro":["685af99397b6da31116a2cac747ed255d217d080","685af99397b6da31116a2cac747ed255d217d080"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b693a83132c9e45afcd564fd65a25b60ed80388b","date":1436882146,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getInQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(ZkStateReader.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f","bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e31e6ce5848e5040d4c9ecbb25bc6ccf5d0894ac","date":1438841252,"type":5,"author":"Gregory Chanan","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionMessageHandler#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getInQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice = null;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n    \n    if (parentSlice == null) {\n      if (clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collectionName);\n      }\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null, shardHandler);\n      \n      final String asyncId = message.getStr(ASYNC);\n      HashMap<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to create subshard leaders\", shardHandler);\n      \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", shardHandler);\n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n      \n      collectShardResponses(results, true, \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\",\n          shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList, RANDOM);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getInQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n      \n      collectShardResponses(results, true,\n          \"SPLITSHARD failed to create subshard replicas\", shardHandler);\n          \n      completeAsyncRequest(asyncId, requestMap, results);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f222f4f2bda21ffd6b39b3362b2412e98e4d5e31":["439c63ae5d22132fca810a0029a854e97d2c1a3e"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["e90f002d571348d57377e0d63fd954b0fd1da637","96adbab674ae121f8b6b3e10474070b4bd97a219"],"058f5a3debcfa0ea477da3eabb4cbe2ec0fac211":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"30c688f7052130cef7bd419c85e3c5be214f7b9e":["4ddb837927b3de29503b68a05fec256665edab50"],"0c924d4069ef5a5bc479a493befe0121aada6896":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296"],"a219f1dcad1700e84807666bdbd2b573e8de7021":["d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296"],"24a5da2a0d397ff29f3de8f6cf451d3412c2509a":["103857ec20f79f31c7a00310a91ed001b9a6ef17"],"e90f002d571348d57377e0d63fd954b0fd1da637":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"96adbab674ae121f8b6b3e10474070b4bd97a219":["0220ee39df0e359431efa5115aeb0729982e3c96"],"ee31282189f924712ed9ad83e3073e0c207a1a53":["24a5da2a0d397ff29f3de8f6cf451d3412c2509a"],"d9d5abf772262a05c74afddcadc95c4bdab07f1f":["bc841231667f1f315bae6799c068f9aad6543967"],"6f26f74e4969851a019d28f10315cb1c77786f22":["cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d"],"e6225820ca5f401e2fe460f36e4a919a384cef6c":["8497bb4f9de61b5520423bd9af88ea11a6e109e7"],"a234e8fc54f31f81256b691a1a134f969ef3d01c":["87d54fb06e5d5ca00e6b0db75b52de2013d09ce4"],"cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d":["058f5a3debcfa0ea477da3eabb4cbe2ec0fac211"],"b711ca2887ae3fda68611cbb78c1b389fa9833be":["da888af1ab894358122a22229051215f58cf4d54"],"da888af1ab894358122a22229051215f58cf4d54":["cee263b0163fa111cfda384934079baae77fccac"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["e90f002d571348d57377e0d63fd954b0fd1da637"],"57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72":["204509ef590c8dfddd322f12e0f9fb6a0060be69"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["da888af1ab894358122a22229051215f58cf4d54","b711ca2887ae3fda68611cbb78c1b389fa9833be"],"8497bb4f9de61b5520423bd9af88ea11a6e109e7":["a234e8fc54f31f81256b691a1a134f969ef3d01c"],"2dbed1dd58810b079506c1e4cd13ce80e646faed":["1de8c3198c585d95ab4c1fa224a5b0bf72840b15"],"b753f4bd40fd8ec5276ceae15e867d2dceeb5552":["b711ca2887ae3fda68611cbb78c1b389fa9833be"],"d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296":["dbf528c0e702c5cbd1339b2da1cdc823fd44a925"],"cee263b0163fa111cfda384934079baae77fccac":["b509abc8bfbaa6d2583fdc203d0f830aeffaadb7"],"87d54fb06e5d5ca00e6b0db75b52de2013d09ce4":["afd709fe548f0ddbaa0a678015e7c3d41ff1c87a"],"204509ef590c8dfddd322f12e0f9fb6a0060be69":["b753f4bd40fd8ec5276ceae15e867d2dceeb5552"],"b7605579001505896d48b07160075a5c8b8e128e":["cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d","6f26f74e4969851a019d28f10315cb1c77786f22"],"439c63ae5d22132fca810a0029a854e97d2c1a3e":["a219f1dcad1700e84807666bdbd2b573e8de7021"],"e31e6ce5848e5040d4c9ecbb25bc6ccf5d0894ac":["b693a83132c9e45afcd564fd65a25b60ed80388b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["24a5da2a0d397ff29f3de8f6cf451d3412c2509a","dbf528c0e702c5cbd1339b2da1cdc823fd44a925"],"bc841231667f1f315bae6799c068f9aad6543967":["457c0871b070288069b6f4422a913198f83e24ee"],"2e73db80cda3387e197641256d964f8c1c3992c7":["1816753738ff1f27f11b38030e83c0ded050b7a4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b509abc8bfbaa6d2583fdc203d0f830aeffaadb7":["d9d5abf772262a05c74afddcadc95c4bdab07f1f"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e6225820ca5f401e2fe460f36e4a919a384cef6c"],"1816753738ff1f27f11b38030e83c0ded050b7a4":["96adbab674ae121f8b6b3e10474070b4bd97a219"],"4ddb837927b3de29503b68a05fec256665edab50":["6f26f74e4969851a019d28f10315cb1c77786f22"],"0220ee39df0e359431efa5115aeb0729982e3c96":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"55980207f1977bd1463465de1659b821347e2fa8":["30c688f7052130cef7bd419c85e3c5be214f7b9e","103857ec20f79f31c7a00310a91ed001b9a6ef17"],"8b81868c96ab084d96d4589dd330753c529bc450":["f222f4f2bda21ffd6b39b3362b2412e98e4d5e31"],"b693a83132c9e45afcd564fd65a25b60ed80388b":["8b81868c96ab084d96d4589dd330753c529bc450"],"afd709fe548f0ddbaa0a678015e7c3d41ff1c87a":["6bdd460d2fe72f80e52acb2774d58a4ae2e67b9b"],"6bdd460d2fe72f80e52acb2774d58a4ae2e67b9b":["57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72"],"0beaed456aa3358e5e4a99ea2aea994ef6c81de3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1de8c3198c585d95ab4c1fa224a5b0bf72840b15":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"103857ec20f79f31c7a00310a91ed001b9a6ef17":["30c688f7052130cef7bd419c85e3c5be214f7b9e"],"457c0871b070288069b6f4422a913198f83e24ee":["2e73db80cda3387e197641256d964f8c1c3992c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e31e6ce5848e5040d4c9ecbb25bc6ccf5d0894ac"],"dbf528c0e702c5cbd1339b2da1cdc823fd44a925":["ee31282189f924712ed9ad83e3073e0c207a1a53"]},"commit2Childs":{"f222f4f2bda21ffd6b39b3362b2412e98e4d5e31":["8b81868c96ab084d96d4589dd330753c529bc450"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"058f5a3debcfa0ea477da3eabb4cbe2ec0fac211":["cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d"],"30c688f7052130cef7bd419c85e3c5be214f7b9e":["55980207f1977bd1463465de1659b821347e2fa8","103857ec20f79f31c7a00310a91ed001b9a6ef17"],"0c924d4069ef5a5bc479a493befe0121aada6896":[],"a219f1dcad1700e84807666bdbd2b573e8de7021":["439c63ae5d22132fca810a0029a854e97d2c1a3e"],"e90f002d571348d57377e0d63fd954b0fd1da637":["37a0f60745e53927c4c876cfe5b5a58170f0646c","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"24a5da2a0d397ff29f3de8f6cf451d3412c2509a":["ee31282189f924712ed9ad83e3073e0c207a1a53","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"96adbab674ae121f8b6b3e10474070b4bd97a219":["37a0f60745e53927c4c876cfe5b5a58170f0646c","1816753738ff1f27f11b38030e83c0ded050b7a4"],"ee31282189f924712ed9ad83e3073e0c207a1a53":["dbf528c0e702c5cbd1339b2da1cdc823fd44a925"],"d9d5abf772262a05c74afddcadc95c4bdab07f1f":["b509abc8bfbaa6d2583fdc203d0f830aeffaadb7"],"6f26f74e4969851a019d28f10315cb1c77786f22":["b7605579001505896d48b07160075a5c8b8e128e","4ddb837927b3de29503b68a05fec256665edab50"],"e6225820ca5f401e2fe460f36e4a919a384cef6c":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a234e8fc54f31f81256b691a1a134f969ef3d01c":["8497bb4f9de61b5520423bd9af88ea11a6e109e7"],"cc9dd0d6ff8e58a4ce40ba13d20664a95589de3d":["6f26f74e4969851a019d28f10315cb1c77786f22","b7605579001505896d48b07160075a5c8b8e128e"],"b711ca2887ae3fda68611cbb78c1b389fa9833be":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","b753f4bd40fd8ec5276ceae15e867d2dceeb5552"],"da888af1ab894358122a22229051215f58cf4d54":["b711ca2887ae3fda68611cbb78c1b389fa9833be","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["0220ee39df0e359431efa5115aeb0729982e3c96"],"57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72":["6bdd460d2fe72f80e52acb2774d58a4ae2e67b9b"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"8497bb4f9de61b5520423bd9af88ea11a6e109e7":["e6225820ca5f401e2fe460f36e4a919a384cef6c"],"2dbed1dd58810b079506c1e4cd13ce80e646faed":["e90f002d571348d57377e0d63fd954b0fd1da637"],"b753f4bd40fd8ec5276ceae15e867d2dceeb5552":["204509ef590c8dfddd322f12e0f9fb6a0060be69"],"d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296":["0c924d4069ef5a5bc479a493befe0121aada6896","a219f1dcad1700e84807666bdbd2b573e8de7021"],"cee263b0163fa111cfda384934079baae77fccac":["da888af1ab894358122a22229051215f58cf4d54"],"87d54fb06e5d5ca00e6b0db75b52de2013d09ce4":["a234e8fc54f31f81256b691a1a134f969ef3d01c"],"204509ef590c8dfddd322f12e0f9fb6a0060be69":["57641b2e9dc6155be493cfb6ae9b8a9c8ceffa72"],"b7605579001505896d48b07160075a5c8b8e128e":[],"439c63ae5d22132fca810a0029a854e97d2c1a3e":["f222f4f2bda21ffd6b39b3362b2412e98e4d5e31"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["0c924d4069ef5a5bc479a493befe0121aada6896"],"e31e6ce5848e5040d4c9ecbb25bc6ccf5d0894ac":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bc841231667f1f315bae6799c068f9aad6543967":["d9d5abf772262a05c74afddcadc95c4bdab07f1f"],"2e73db80cda3387e197641256d964f8c1c3992c7":["457c0871b070288069b6f4422a913198f83e24ee"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"b509abc8bfbaa6d2583fdc203d0f830aeffaadb7":["cee263b0163fa111cfda384934079baae77fccac"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["058f5a3debcfa0ea477da3eabb4cbe2ec0fac211"],"1816753738ff1f27f11b38030e83c0ded050b7a4":["2e73db80cda3387e197641256d964f8c1c3992c7"],"4ddb837927b3de29503b68a05fec256665edab50":["30c688f7052130cef7bd419c85e3c5be214f7b9e"],"0220ee39df0e359431efa5115aeb0729982e3c96":["96adbab674ae121f8b6b3e10474070b4bd97a219"],"55980207f1977bd1463465de1659b821347e2fa8":[],"8b81868c96ab084d96d4589dd330753c529bc450":["b693a83132c9e45afcd564fd65a25b60ed80388b"],"afd709fe548f0ddbaa0a678015e7c3d41ff1c87a":["87d54fb06e5d5ca00e6b0db75b52de2013d09ce4"],"b693a83132c9e45afcd564fd65a25b60ed80388b":["e31e6ce5848e5040d4c9ecbb25bc6ccf5d0894ac"],"6bdd460d2fe72f80e52acb2774d58a4ae2e67b9b":["afd709fe548f0ddbaa0a678015e7c3d41ff1c87a"],"0beaed456aa3358e5e4a99ea2aea994ef6c81de3":["1de8c3198c585d95ab4c1fa224a5b0bf72840b15"],"1de8c3198c585d95ab4c1fa224a5b0bf72840b15":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"103857ec20f79f31c7a00310a91ed001b9a6ef17":["24a5da2a0d397ff29f3de8f6cf451d3412c2509a","55980207f1977bd1463465de1659b821347e2fa8"],"457c0871b070288069b6f4422a913198f83e24ee":["bc841231667f1f315bae6799c068f9aad6543967"],"dbf528c0e702c5cbd1339b2da1cdc823fd44a925":["d0dcc63c22f7cfe3d3a83aee576d0fc5b403a296","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","0c924d4069ef5a5bc479a493befe0121aada6896","74f45af4339b0daf7a95c820ab88c1aea74fbce0","b7605579001505896d48b07160075a5c8b8e128e","55980207f1977bd1463465de1659b821347e2fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}