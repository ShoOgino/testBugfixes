{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","commits":[{"id":"e8550173e67bccdaad7c5cbb85fb81886fecfcb7","date":1376943063,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"/dev/null","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":2,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v = nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte((byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add((nv.longValue() - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"/dev/null","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d36ccb9a1c11aeb91962e89bda4a2e643c8629b3","date":1401710950,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE, PackedInts.DEFAULT);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE, PackedInts.DEFAULT);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8106bc60c7452250f84c65cdb43ab6b1d8eb1534","date":1401906364,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE, PackedInts.DEFAULT);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE, PackedInts.DEFAULT);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":["d36ccb9a1c11aeb91962e89bda4a2e643c8629b3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf","date":1402500925,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = blockDelta < 0 ? 64 : PackedInts.bitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n      assert count == maxDoc;\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, maxDoc, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, maxDoc, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, maxDoc, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n      assert count == maxDoc;\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n\n    if (uniqueValues != null) {\n      // small number of unique values\n      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);\n      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);\n      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {\n        meta.writeByte(UNCOMPRESSED); // uncompressed\n        for (Number nv : values) {\n          data.writeByte(nv == null ? 0 : (byte) nv.longValue());\n        }\n      } else {\n        meta.writeByte(TABLE_COMPRESSED); // table-compressed\n        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<>();\n        data.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          data.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n\n        meta.writeVInt(PackedInts.VERSION_CURRENT);\n        data.writeVInt(formatAndBits.format.getId());\n        data.writeVInt(formatAndBits.bitsPerValue);\n\n        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n        for(Number nv : values) {\n          writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        writer.finish();\n      }\n    } else if (gcd != 0 && gcd != 1) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14ffaac9c4a4a2c750bf0cd956506802561e062","date":1402602036,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = blockDelta < 0 ? 64 : PackedInts.bitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = blockDelta < 0 ? 64 : PackedInts.bitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n      assert count == maxDoc;\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, maxDoc, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, maxDoc, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, maxDoc, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dff0cb4eabd8bb5c27d3a284e18c812a89958a66","date":1402928522,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = PackedInts.unsignedBitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = blockDelta < 0 ? 64 : PackedInts.bitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = PackedInts.unsignedBitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long count = 0;\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = blockDelta < 0 ? 64 : PackedInts.bitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n      assert count == maxDoc;\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, maxDoc, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, maxDoc, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, maxDoc, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = PackedInts.unsignedBitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks it's pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = PackedInts.unsignedBitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":4,"author":"Dawid Weiss","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":null,"sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    meta.writeVInt(field.number);\n    meta.writeByte(NUMBER);\n    meta.writeLong(data.getFilePointer());\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long blockSum = 0;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    long count = 0;\n\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      long currentBlockMin = Long.MAX_VALUE;\n      long currentBlockMax = Long.MIN_VALUE;\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        currentBlockMin = Math.min(minValue, v);\n        currentBlockMax = Math.max(maxValue, v);\n        \n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n        if (count % BLOCK_SIZE == 0) {\n          final long blockDelta = currentBlockMax - currentBlockMin;\n          final int blockDeltaRequired = PackedInts.unsignedBitsRequired(blockDelta);\n          final int blockBPV = PackedInts.fastestFormatAndBits(BLOCK_SIZE, blockDeltaRequired, acceptableOverheadRatio).bitsPerValue;\n          blockSum += blockBPV;\n          currentBlockMax = Long.MIN_VALUE;\n          currentBlockMin = Long.MAX_VALUE;\n        }\n      }\n    } else {\n      for (Number nv : values) {\n        long v = nv.longValue();\n        maxValue = Math.max(v, maxValue);\n        minValue = Math.min(v, minValue);\n        count++;\n      }\n    }\n    \n    if (missing) {\n      long start = data.getFilePointer();\n      writeMissingBitset(values);\n      meta.writeLong(start);\n      meta.writeLong(data.getFilePointer() - start);\n    } else {\n      meta.writeLong(-1L);\n    }\n    \n    final long delta = maxValue - minValue;\n    final int deltaRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);\n    final FormatAndBits deltaBPV = PackedInts.fastestFormatAndBits(maxDoc, deltaRequired, acceptableOverheadRatio);\n        \n    final FormatAndBits tableBPV;\n    if (count < Integer.MAX_VALUE && uniqueValues != null) {\n      tableBPV = PackedInts.fastestFormatAndBits(maxDoc, PackedInts.bitsRequired(uniqueValues.size()-1), acceptableOverheadRatio);\n    } else {\n      tableBPV = null;\n    }\n    \n    final FormatAndBits gcdBPV;\n    if (count < Integer.MAX_VALUE && gcd != 0 && gcd != 1) {\n      final long gcdDelta = (maxValue - minValue) / gcd;\n      final int gcdRequired = gcdDelta < 0 ? 64 : PackedInts.bitsRequired(gcdDelta);\n      gcdBPV = PackedInts.fastestFormatAndBits(maxDoc, gcdRequired, acceptableOverheadRatio);\n    } else {\n      gcdBPV = null;\n    }\n    \n    boolean doBlock = false;\n    if (blockSum != 0) {\n      int numBlocks = maxDoc / BLOCK_SIZE;\n      float avgBPV = blockSum / (float)numBlocks;\n      // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final \"incomplete\" block.\n      // with at least 4 blocks it's pretty accurate. The difference must also be significant (according to acceptable overhead).\n      if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n        doBlock = true;\n      }\n    }\n    // blockpackedreader allows us to read in huge streams of ints\n    if (count >= Integer.MAX_VALUE) {\n      doBlock = true;\n    }\n    \n    if (tableBPV != null && (tableBPV.bitsPerValue+tableBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      // small number of unique values\n      meta.writeByte(TABLE_COMPRESSED); // table-compressed\n      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n      final HashMap<Long,Integer> encode = new HashMap<>();\n      int length = 1 << tableBPV.bitsPerValue;\n      data.writeVInt(length);\n      for (int i = 0; i < decode.length; i++) {\n        data.writeLong(decode[i]);\n        encode.put(decode[i], i);\n      }\n      for (int i = decode.length; i < length; i++) {\n        data.writeLong(0);\n      }\n      \n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(tableBPV.format.getId());\n      data.writeVInt(tableBPV.bitsPerValue);\n      \n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, tableBPV.format, (int)count, tableBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for(Number nv : values) {\n        writer.add(encode.get(nv == null ? 0 : nv.longValue()));\n      }\n      writer.finish();\n    } else if (gcdBPV != null && (gcdBPV.bitsPerValue+gcdBPV.bitsPerValue*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {\n      meta.writeByte(GCD_COMPRESSED);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeLong(minValue);\n      data.writeLong(gcd);\n      data.writeVInt(gcdBPV.format.getId());\n      data.writeVInt(gcdBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, gcdBPV.format, (int)count, gcdBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long value = nv == null ? 0 : nv.longValue();\n        writer.add((value - minValue) / gcd);\n      }\n      writer.finish();\n    } else if (doBlock) {\n      meta.writeByte(BLOCK_COMPRESSED); // block delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      data.writeVInt(BLOCK_SIZE);\n      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n      for (Number nv : values) {\n        writer.add(nv == null ? 0 : nv.longValue());\n      }\n      writer.finish();\n    } else {\n      meta.writeByte(DELTA_COMPRESSED); // delta-compressed\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeLong(count);\n      final long minDelta = deltaBPV.bitsPerValue == 64 ? 0 : minValue;\n      data.writeLong(minDelta);\n      data.writeVInt(deltaBPV.format.getId());\n      data.writeVInt(deltaBPV.bitsPerValue);\n\n      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, deltaBPV.format, (int)count, deltaBPV.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);\n      for (Number nv : values) {\n        long v = nv == null ? 0 : nv.longValue();\n        writer.add(v - minDelta);\n      }\n      writer.finish();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"dff0cb4eabd8bb5c27d3a284e18c812a89958a66":["d14ffaac9c4a4a2c750bf0cd956506802561e062"],"b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e8550173e67bccdaad7c5cbb85fb81886fecfcb7"],"e8550173e67bccdaad7c5cbb85fb81886fecfcb7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf","dff0cb4eabd8bb5c27d3a284e18c812a89958a66"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["dff0cb4eabd8bb5c27d3a284e18c812a89958a66"],"24f89e8a6aac05753cde4c83d62a74356098200d":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"d36ccb9a1c11aeb91962e89bda4a2e643c8629b3":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["d36ccb9a1c11aeb91962e89bda4a2e643c8629b3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["24f89e8a6aac05753cde4c83d62a74356098200d"],"d14ffaac9c4a4a2c750bf0cd956506802561e062":["b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["d36ccb9a1c11aeb91962e89bda4a2e643c8629b3"],"dff0cb4eabd8bb5c27d3a284e18c812a89958a66":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","d14ffaac9c4a4a2c750bf0cd956506802561e062"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"e8550173e67bccdaad7c5cbb85fb81886fecfcb7":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["24f89e8a6aac05753cde4c83d62a74356098200d"],"24f89e8a6aac05753cde4c83d62a74356098200d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d36ccb9a1c11aeb91962e89bda4a2e643c8629b3":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","e8550173e67bccdaad7c5cbb85fb81886fecfcb7"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["b3f1f6da1318f50d5d7c35654b84bf25cddc7ecf"],"d14ffaac9c4a4a2c750bf0cd956506802561e062":["dff0cb4eabd8bb5c27d3a284e18c812a89958a66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","c6f080a2ab37c464dd98db173f6cbf10dc74f211","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}