{"path":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","commits":[{"id":"c7e79e31f55cbb444e3023d430a340658755aa31","date":1357666399,"type":1,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser[ExtendedDismaxQParserPlugin].ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","sourceNew":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","sourceOld":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n\n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n\n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n\n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n\n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n\n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n\n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e6354dd7c71fe122926fc53d7d29f715b1283db","date":1357915185,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser[ExtendedDismaxQParserPlugin].ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","sourceNew":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","sourceOld":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n\n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n\n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n\n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n\n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n\n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n\n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"123698fbe83b595f9e084f0019cd35ab4a01d7f7","date":1399070065,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","sourceNew":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getIndexAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","sourceOld":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"829fd6c12636f3099b6a266518d961b41e7c5b45","date":1522211839,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","sourceNew":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getIndexAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getCharFilterFactories(), tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","sourceOld":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getIndexAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","bugFix":["97a1d065fbe56cbf9eaf80cd3f8d4477203dda70"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43564cbb30b064675027cfb569564e8531096e97","date":1522334265,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.ExtendedSolrQueryParser#noStopwordFilterAnalyzer(String).mjava","sourceNew":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getIndexAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getCharFilterFactories(), tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","sourceOld":"    private Analyzer noStopwordFilterAnalyzer(String fieldName) {\n      FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n      Analyzer qa = ft.getQueryAnalyzer();\n      if (!(qa instanceof TokenizerChain)) {\n        return qa;\n      }\n      \n      TokenizerChain tcq = (TokenizerChain) qa;\n      Analyzer ia = ft.getIndexAnalyzer();\n      if (ia == qa || !(ia instanceof TokenizerChain)) {\n        return qa;\n      }\n      TokenizerChain tci = (TokenizerChain) ia;\n      \n      // make sure that there isn't a stop filter in the indexer\n      for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n        if (tf instanceof StopFilterFactory) {\n          return qa;\n        }\n      }\n      \n      // now if there is a stop filter in the query analyzer, remove it\n      int stopIdx = -1;\n      TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n      \n      for (int i = 0; i < facs.length; i++) {\n        TokenFilterFactory tf = facs[i];\n        if (tf instanceof StopFilterFactory) {\n          stopIdx = i;\n          break;\n        }\n      }\n      \n      if (stopIdx == -1) {\n        // no stop filter exists\n        return qa;\n      }\n      \n      TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length - 1];\n      for (int i = 0, j = 0; i < facs.length; i++) {\n        if (i == stopIdx) continue;\n        newtf[j++] = facs[i];\n      }\n      \n      TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n      newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n      return newa;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"43564cbb30b064675027cfb569564e8531096e97":["123698fbe83b595f9e084f0019cd35ab4a01d7f7","829fd6c12636f3099b6a266518d961b41e7c5b45"],"123698fbe83b595f9e084f0019cd35ab4a01d7f7":["c7e79e31f55cbb444e3023d430a340658755aa31"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"829fd6c12636f3099b6a266518d961b41e7c5b45":["123698fbe83b595f9e084f0019cd35ab4a01d7f7"],"c7e79e31f55cbb444e3023d430a340658755aa31":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c7e79e31f55cbb444e3023d430a340658755aa31"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["43564cbb30b064675027cfb569564e8531096e97"]},"commit2Childs":{"43564cbb30b064675027cfb569564e8531096e97":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"123698fbe83b595f9e084f0019cd35ab4a01d7f7":["43564cbb30b064675027cfb569564e8531096e97","829fd6c12636f3099b6a266518d961b41e7c5b45"],"829fd6c12636f3099b6a266518d961b41e7c5b45":["43564cbb30b064675027cfb569564e8531096e97"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c7e79e31f55cbb444e3023d430a340658755aa31","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"c7e79e31f55cbb444e3023d430a340658755aa31":["123698fbe83b595f9e084f0019cd35ab4a01d7f7","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4e6354dd7c71fe122926fc53d7d29f715b1283db","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}