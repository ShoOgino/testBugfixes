{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOnIOException().mjava","commits":[{"id":"8da3c22a3b1a00ae6e2664f3ac0d82cfa3a8f666","date":1381263930,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOnIOException().mjava","pathOld":"/dev/null","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes:\n  public void testNoLostDeletesOnIOException() throws Exception {\n\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n\n          @Override\n          public void eval(MockDirectoryWrapper dir) throws IOException {\n            StackTraceElement[] trace = new Exception().getStackTrace();\n            if (shouldFail.get() == false) {\n              return;\n            }\n\n            boolean sawSeal = false;\n            boolean sawWrite = false;\n            for (int i = 0; i < trace.length; i++) {\n              if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n                sawSeal = true;\n                break;\n              }\n              if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n                sawWrite = true;\n              }\n            }\n\n            // Don't throw exc if we are \"flushing\", else\n            // the segment is aborted and docs are lost:\n            if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n              // Only sometimes throw the exc, so we get\n              // it sometimes on creating the file, on\n              // flushing buffer, on closing the file:\n              if (VERBOSE) {\n                System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n                new Throwable().printStackTrace(System.out);\n              }\n              shouldFail.set(false);\n              throw new FakeIOException();\n            }\n          }\n      });\n\n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n\n      try {\n\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            if (VERBOSE) {\n              System.out.println(\"  delete id=\" + (docBase+i));\n            }\n            deleteCount++;\n            w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n\n        // TODO: also call w.close() in here, sometimes,\n        // so we sometimes get a fail via dropAll\n\n      } catch (FakeIOException ioe) {\n        // expected\n        if (VERBOSE) {\n          System.out.println(\"TEST: w.close() hit expected IOE\");\n        }\n        // No exception should happen here (we only fail once):\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        w.commit();\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e40c7b0baa9f015b690f60bacb592b64264ba3d","date":1381610202,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOnIOException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOnIOException().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes:\n  public void testNoLostDeletesOnIOException() throws Exception {\n\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n\n          @Override\n          public void eval(MockDirectoryWrapper dir) throws IOException {\n            StackTraceElement[] trace = new Exception().getStackTrace();\n            if (shouldFail.get() == false) {\n              return;\n            }\n\n            boolean sawSeal = false;\n            boolean sawWrite = false;\n            for (int i = 0; i < trace.length; i++) {\n              if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n                sawSeal = true;\n                break;\n              }\n              if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n                sawWrite = true;\n              }\n            }\n\n            // Don't throw exc if we are \"flushing\", else\n            // the segment is aborted and docs are lost:\n            if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n              // Only sometimes throw the exc, so we get\n              // it sometimes on creating the file, on\n              // flushing buffer, on closing the file:\n              if (VERBOSE) {\n                System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n                new Throwable().printStackTrace(System.out);\n              }\n              shouldFail.set(false);\n              throw new FakeIOException();\n            }\n          }\n      });\n\n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            if (VERBOSE) {\n              System.out.println(\"  delete id=\" + (docBase+i));\n            }\n            deleteCount++;\n            w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (FakeIOException ioe) {\n        // expected\n        if (VERBOSE) {\n          System.out.println(\"TEST: w.close() hit expected IOE\");\n        }\n        // No exception should happen here (we only fail once):\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes:\n  public void testNoLostDeletesOnIOException() throws Exception {\n\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n\n          @Override\n          public void eval(MockDirectoryWrapper dir) throws IOException {\n            StackTraceElement[] trace = new Exception().getStackTrace();\n            if (shouldFail.get() == false) {\n              return;\n            }\n\n            boolean sawSeal = false;\n            boolean sawWrite = false;\n            for (int i = 0; i < trace.length; i++) {\n              if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n                sawSeal = true;\n                break;\n              }\n              if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n                sawWrite = true;\n              }\n            }\n\n            // Don't throw exc if we are \"flushing\", else\n            // the segment is aborted and docs are lost:\n            if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n              // Only sometimes throw the exc, so we get\n              // it sometimes on creating the file, on\n              // flushing buffer, on closing the file:\n              if (VERBOSE) {\n                System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n                new Throwable().printStackTrace(System.out);\n              }\n              shouldFail.set(false);\n              throw new FakeIOException();\n            }\n          }\n      });\n\n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n\n      try {\n\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            if (VERBOSE) {\n              System.out.println(\"  delete id=\" + (docBase+i));\n            }\n            deleteCount++;\n            w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n\n        // TODO: also call w.close() in here, sometimes,\n        // so we sometimes get a fail via dropAll\n\n      } catch (FakeIOException ioe) {\n        // expected\n        if (VERBOSE) {\n          System.out.println(\"TEST: w.close() hit expected IOE\");\n        }\n        // No exception should happen here (we only fail once):\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        w.commit();\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba","date":1381646249,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOrUpdatesOnIOException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOnIOException().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdatesOnIOException() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      for (AtomicReaderContext context : r.leaves()) {\n        Bits liveDocs = context.reader().getLiveDocs();\n        NumericDocValues f = context.reader().getNumericDocValues(\"f\");\n        NumericDocValues cf = context.reader().getNumericDocValues(\"cf\");\n        for (int i = 0; i < context.reader().maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes:\n  public void testNoLostDeletesOnIOException() throws Exception {\n\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n\n          @Override\n          public void eval(MockDirectoryWrapper dir) throws IOException {\n            StackTraceElement[] trace = new Exception().getStackTrace();\n            if (shouldFail.get() == false) {\n              return;\n            }\n\n            boolean sawSeal = false;\n            boolean sawWrite = false;\n            for (int i = 0; i < trace.length; i++) {\n              if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n                sawSeal = true;\n                break;\n              }\n              if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n                sawWrite = true;\n              }\n            }\n\n            // Don't throw exc if we are \"flushing\", else\n            // the segment is aborted and docs are lost:\n            if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n              // Only sometimes throw the exc, so we get\n              // it sometimes on creating the file, on\n              // flushing buffer, on closing the file:\n              if (VERBOSE) {\n                System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n                new Throwable().printStackTrace(System.out);\n              }\n              shouldFail.set(false);\n              throw new FakeIOException();\n            }\n          }\n      });\n\n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            if (VERBOSE) {\n              System.out.println(\"  delete id=\" + (docBase+i));\n            }\n            deleteCount++;\n            w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (FakeIOException ioe) {\n        // expected\n        if (VERBOSE) {\n          System.out.println(\"TEST: w.close() hit expected IOE\");\n        }\n        // No exception should happen here (we only fail once):\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["865877758b369e6bdf9a485e27627ed6fbf391b0","f83d8858d121721403246759848f7edf175cc704","402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5e40c7b0baa9f015b690f60bacb592b64264ba3d":["8da3c22a3b1a00ae6e2664f3ac0d82cfa3a8f666"],"8da3c22a3b1a00ae6e2664f3ac0d82cfa3a8f666":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba":["5e40c7b0baa9f015b690f60bacb592b64264ba3d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba"]},"commit2Childs":{"5e40c7b0baa9f015b690f60bacb592b64264ba3d":["d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba"],"8da3c22a3b1a00ae6e2664f3ac0d82cfa3a8f666":["5e40c7b0baa9f015b690f60bacb592b64264ba3d"],"d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8da3c22a3b1a00ae6e2664f3ac0d82cfa3a8f666"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}