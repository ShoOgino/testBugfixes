{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/MultiTermHighlighting#uninvertAndFilterTerms(Terms,int,CharacterRunAutomaton[],int).mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/MultiTermHighlighting#uninvertAndFilterTerms(Terms,int,CharacterRunAutomaton[],int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Return a TokenStream un-inverted from the provided Terms, but filtered based on the automata. The\n   * Terms must have exactly one doc count (e.g. term vector or MemoryIndex).\n   */\n  //TODO: Alternatively, produce a list of OffsetsEnums from the Terms that match the automata.\n  public static TokenStream uninvertAndFilterTerms(Terms termsIndex,\n                                                      int doc,\n                                                      final CharacterRunAutomaton[] automata,\n                                                      int offsetLength)\n      throws IOException {\n    assert automata.length > 0;\n    //Note: if automata were plain Automaton (not CharacterRunAutomaton), we might instead use\n    // TermsEnum.intersect(compiledAutomaton).  But probably won't help due to O(N) TV impl so whatever.\n    FilterLeafReader.FilterTerms filteredTermsIndex = new FilterLeafReader.FilterTerms(termsIndex) {\n      @Override\n      public TermsEnum iterator() throws IOException {\n        return new FilteredTermsEnum(super.iterator(), false) {//false == no seek\n          CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//reuse only for UTF8->UTF16 call\n\n          @Override\n          protected AcceptStatus accept(BytesRef termBytesRef) throws IOException {\n            //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n            tempCharsRefBuilder.grow(termBytesRef.length);\n            final int charLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n            for (CharacterRunAutomaton runAutomaton : automata) {\n              if (runAutomaton.run(tempCharsRefBuilder.chars(), 0, charLen)) {\n                return AcceptStatus.YES;\n              }\n            }\n            return AcceptStatus.NO;\n          }\n        };\n      }\n\n      @Override\n      public long size() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumTotalTermFreq() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumDocFreq() throws IOException {\n        return -1; // unknown\n      }\n    };\n    float loadFactor = 1f / 64f;\n    return new TokenStreamFromTermVector(filteredTermsIndex, doc, offsetLength, loadFactor);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/MultiTermHighlighting#uninvertAndFilterTerms(Terms,int,CharacterRunAutomaton[],int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Return a TokenStream un-inverted from the provided Terms, but filtered based on the automata. The\n   * Terms must have exactly one doc count (e.g. term vector or MemoryIndex).\n   */\n  //TODO: Alternatively, produce a list of OffsetsEnums from the Terms that match the automata.\n  public static TokenStream uninvertAndFilterTerms(Terms termsIndex,\n                                                      int doc,\n                                                      final CharacterRunAutomaton[] automata,\n                                                      int offsetLength)\n      throws IOException {\n    assert automata.length > 0;\n    //Note: if automata were plain Automaton (not CharacterRunAutomaton), we might instead use\n    // TermsEnum.intersect(compiledAutomaton).  But probably won't help due to O(N) TV impl so whatever.\n    FilterLeafReader.FilterTerms filteredTermsIndex = new FilterLeafReader.FilterTerms(termsIndex) {\n      @Override\n      public TermsEnum iterator() throws IOException {\n        return new FilteredTermsEnum(super.iterator(), false) {//false == no seek\n          CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//reuse only for UTF8->UTF16 call\n\n          @Override\n          protected AcceptStatus accept(BytesRef termBytesRef) throws IOException {\n            //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n            tempCharsRefBuilder.grow(termBytesRef.length);\n            final int charLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n            for (CharacterRunAutomaton runAutomaton : automata) {\n              if (runAutomaton.run(tempCharsRefBuilder.chars(), 0, charLen)) {\n                return AcceptStatus.YES;\n              }\n            }\n            return AcceptStatus.NO;\n          }\n        };\n      }\n\n      @Override\n      public long size() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumTotalTermFreq() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumDocFreq() throws IOException {\n        return -1; // unknown\n      }\n    };\n    float loadFactor = 1f / 64f;\n    return new TokenStreamFromTermVector(filteredTermsIndex, doc, offsetLength, loadFactor);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2e9861e4a2b724d9fc51b618714c579491b78d7","date":1479244606,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/MultiTermHighlighting#uninvertAndFilterTerms(Terms,int,CharacterRunAutomaton[],int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Return a TokenStream un-inverted from the provided Terms, but filtered based on the automata. The\n   * Terms must have exactly one doc count (e.g. term vector or MemoryIndex).\n   */\n  //TODO: Alternatively, produce a list of OffsetsEnums from the Terms that match the automata.\n  public static TokenStream uninvertAndFilterTerms(Terms termsIndex,\n                                                      int doc,\n                                                      final CharacterRunAutomaton[] automata,\n                                                      int offsetLength)\n      throws IOException {\n    assert automata.length > 0;\n    //Note: if automata were plain Automaton (not CharacterRunAutomaton), we might instead use\n    // TermsEnum.intersect(compiledAutomaton).  But probably won't help due to O(N) TV impl so whatever.\n    FilterLeafReader.FilterTerms filteredTermsIndex = new FilterLeafReader.FilterTerms(termsIndex) {\n      @Override\n      public TermsEnum iterator() throws IOException {\n        return new FilteredTermsEnum(super.iterator(), false) {//false == no seek\n          CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//reuse only for UTF8->UTF16 call\n\n          @Override\n          protected AcceptStatus accept(BytesRef termBytesRef) throws IOException {\n            //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n            tempCharsRefBuilder.grow(termBytesRef.length);\n            final int charLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n            for (CharacterRunAutomaton runAutomaton : automata) {\n              if (runAutomaton.run(tempCharsRefBuilder.chars(), 0, charLen)) {\n                return AcceptStatus.YES;\n              }\n            }\n            return AcceptStatus.NO;\n          }\n        };\n      }\n\n      @Override\n      public long size() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumTotalTermFreq() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumDocFreq() throws IOException {\n        return -1; // unknown\n      }\n    };\n    float loadFactor = 1f / 64f;\n    return new TokenStreamFromTermVector(filteredTermsIndex, doc, offsetLength, loadFactor);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/MultiTermHighlighting#uninvertAndFilterTerms(Terms,int,CharacterRunAutomaton[],int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Return a TokenStream un-inverted from the provided Terms, but filtered based on the automata. The\n   * Terms must have exactly one doc count (e.g. term vector or MemoryIndex).\n   */\n  //TODO: Alternatively, produce a list of OffsetsEnums from the Terms that match the automata.\n  public static TokenStream uninvertAndFilterTerms(Terms termsIndex,\n                                                      int doc,\n                                                      final CharacterRunAutomaton[] automata,\n                                                      int offsetLength)\n      throws IOException {\n    assert automata.length > 0;\n    //Note: if automata were plain Automaton (not CharacterRunAutomaton), we might instead use\n    // TermsEnum.intersect(compiledAutomaton).  But probably won't help due to O(N) TV impl so whatever.\n    FilterLeafReader.FilterTerms filteredTermsIndex = new FilterLeafReader.FilterTerms(termsIndex) {\n      @Override\n      public TermsEnum iterator() throws IOException {\n        return new FilteredTermsEnum(super.iterator(), false) {//false == no seek\n          CharsRefBuilder tempCharsRefBuilder = new CharsRefBuilder();//reuse only for UTF8->UTF16 call\n\n          @Override\n          protected AcceptStatus accept(BytesRef termBytesRef) throws IOException {\n            //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n            tempCharsRefBuilder.grow(termBytesRef.length);\n            final int charLen = UnicodeUtil.UTF8toUTF16(termBytesRef, tempCharsRefBuilder.chars());\n            for (CharacterRunAutomaton runAutomaton : automata) {\n              if (runAutomaton.run(tempCharsRefBuilder.chars(), 0, charLen)) {\n                return AcceptStatus.YES;\n              }\n            }\n            return AcceptStatus.NO;\n          }\n        };\n      }\n\n      @Override\n      public long size() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumTotalTermFreq() throws IOException {\n        return -1; // unknown\n      }\n\n      @Override\n      public long getSumDocFreq() throws IOException {\n        return -1; // unknown\n      }\n    };\n    float loadFactor = 1f / 64f;\n    return new TokenStreamFromTermVector(filteredTermsIndex, doc, offsetLength, loadFactor);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f2e9861e4a2b724d9fc51b618714c579491b78d7"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f2e9861e4a2b724d9fc51b618714c579491b78d7"]},"commit2Childs":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["f2e9861e4a2b724d9fc51b618714c579491b78d7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}