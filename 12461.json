{"path":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","commits":[{"id":"d15a470e5afe8cbbc2876f3caa51dec0cd2d192d","date":1361993648,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","pathOld":"/dev/null","sourceNew":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new CategoryPath(\"a\", '/'), 10));\n\n    // Aggregatses the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["62320c82ded4563b45ef7583d7f51e0f7f7e7625"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"62320c82ded4563b45ef7583d7f51e0f7f7e7625","date":1365337102,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","sourceNew":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new CategoryPath(\"a\", '/'), 10));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    // LUCENE-4913:\n    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {\n      assertTrue(childNode.ordinal != 0);\n    }\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","sourceOld":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new CategoryPath(\"a\", '/'), 10));\n\n    // Aggregatses the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","bugFix":["d15a470e5afe8cbbc2876f3caa51dec0cd2d192d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c190847801a50f4dd20fd639bdc29b54ea3b288b","date":1384461522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","sourceNew":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new FacetLabel(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new FacetLabel(\"a\", '/'), 10));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    // LUCENE-4913:\n    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {\n      assertTrue(childNode.ordinal != 0);\n    }\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","sourceOld":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new CategoryPath(\"a\", '/'), 10));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    // LUCENE-4913:\n    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {\n      assertTrue(childNode.ordinal != 0);\n    }\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19f5022544a8fc895776356d1b35a4b46d05945c","date":1385063323,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","sourceNew":null,"sourceOld":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new FacetLabel(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new FacetLabel(\"a\", '/'), 10));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    // LUCENE-4913:\n    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {\n      assertTrue(childNode.ordinal != 0);\n    }\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testAllParents().mjava","sourceNew":null,"sourceOld":"  public void testAllParents() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    CategoryListParams clp = new CategoryListParams(\"$facets\") {\n        @Override\n        public OrdinalPolicy getOrdinalPolicy(String fieldName) {\n          return OrdinalPolicy.ALL_PARENTS;\n        }\n      };\n    FacetIndexingParams fip = new FacetIndexingParams(clp);\n\n    FacetFields facetFields = new FacetFields(taxoWriter, fip);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"a/path\", '/')));\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(fip,\n                                                  new CountFacetRequest(new CategoryPath(\"a\", '/'), 10));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    assertEquals(1, (int) results.get(0).getFacetResultNode().value);\n\n    // LUCENE-4913:\n    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {\n      assertTrue(childNode.ordinal != 0);\n    }\n\n    searcher.getIndexReader().close();\n    taxoReader.close();\n    dir.close();\n    taxoDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"62320c82ded4563b45ef7583d7f51e0f7f7e7625":["d15a470e5afe8cbbc2876f3caa51dec0cd2d192d"],"19f5022544a8fc895776356d1b35a4b46d05945c":["c190847801a50f4dd20fd639bdc29b54ea3b288b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["62320c82ded4563b45ef7583d7f51e0f7f7e7625","19f5022544a8fc895776356d1b35a4b46d05945c"],"d15a470e5afe8cbbc2876f3caa51dec0cd2d192d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["62320c82ded4563b45ef7583d7f51e0f7f7e7625"]},"commit2Childs":{"62320c82ded4563b45ef7583d7f51e0f7f7e7625":["3cc728b07df73b197e6d940d27f9b08b63918f13","c190847801a50f4dd20fd639bdc29b54ea3b288b"],"19f5022544a8fc895776356d1b35a4b46d05945c":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d15a470e5afe8cbbc2876f3caa51dec0cd2d192d"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d15a470e5afe8cbbc2876f3caa51dec0cd2d192d":["62320c82ded4563b45ef7583d7f51e0f7f7e7625"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["19f5022544a8fc895776356d1b35a4b46d05945c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}