{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {\n    Tokenizer tokenizer = new KeywordTokenizer(new StringReader(\"\"));\n    TokenFilter filter = new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, tokenizer));\n    // read test cases from external file:\n    InputStreamReader isr = new InputStreamReader(getClass().getResourceAsStream(\"data.txt\"), \"iso-8859-1\");\n    BufferedReader breader = new BufferedReader(isr);\n    while(true) {\n      String line = breader.readLine();\n      if (line == null)\n        break;\n      line = line.trim();\n      if (line.startsWith(\"#\") || line.equals(\"\"))\n        continue;    // ignore comments and empty lines\n      String[] parts = line.split(\";\");\n      //System.out.println(parts[0] + \" -- \" + parts[1]);\n      tokenizer.reset(new StringReader(parts[0]));\n      filter.reset();\n      assertTokenStreamContents(filter, new String[] { parts[1] });\n    }\n    breader.close();\n    isr.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Tokenizer tokenizer = new KeywordTokenizer(new StringReader(\"\"));\n    TokenFilter filter = new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, tokenizer));\n    // read test cases from external file:\n    InputStreamReader isr = new InputStreamReader(getClass().getResourceAsStream(\"data.txt\"), \"iso-8859-1\");\n    BufferedReader breader = new BufferedReader(isr);\n    while(true) {\n      String line = breader.readLine();\n      if (line == null)\n        break;\n      line = line.trim();\n      if (line.startsWith(\"#\") || line.equals(\"\"))\n        continue;    // ignore comments and empty lines\n      String[] parts = line.split(\";\");\n      //System.out.println(parts[0] + \" -- \" + parts[1]);\n      tokenizer.reset(new StringReader(parts[0]));\n      filter.reset();\n      assertTokenStreamContents(filter, new String[] { parts[1] });\n    }\n    breader.close();\n    isr.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8864af8de677008c534d14fc77568b83a6c1361","date":1279109434,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Tokenizer tokenizer = new KeywordTokenizer(new StringReader(\"\"));\n    TokenFilter filter = new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, tokenizer));\n    // read test cases from external file:\n    InputStreamReader isr = new InputStreamReader(getClass().getResourceAsStream(\"data.txt\"), \"iso-8859-1\");\n    BufferedReader breader = new BufferedReader(isr);\n    while(true) {\n      String line = breader.readLine();\n      if (line == null)\n        break;\n      line = line.trim();\n      if (line.startsWith(\"#\") || line.equals(\"\"))\n        continue;    // ignore comments and empty lines\n      String[] parts = line.split(\";\");\n      //System.out.println(parts[0] + \" -- \" + parts[1]);\n      tokenizer.reset(new StringReader(parts[0]));\n      filter.reset();\n      assertTokenStreamContents(filter, new String[] { parts[1] });\n    }\n    breader.close();\n    isr.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Tokenizer tokenizer = new KeywordTokenizer(new StringReader(\"\"));\n    TokenFilter filter = new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, tokenizer));\n    // read test cases from external file:\n    InputStreamReader isr = new InputStreamReader(getClass().getResourceAsStream(\"data.txt\"), \"iso-8859-1\");\n    BufferedReader breader = new BufferedReader(isr);\n    while(true) {\n      String line = breader.readLine();\n      if (line == null)\n        break;\n      line = line.trim();\n      if (line.startsWith(\"#\") || line.equals(\"\"))\n        continue;    // ignore comments and empty lines\n      String[] parts = line.split(\";\");\n      //System.out.println(parts[0] + \" -- \" + parts[1]);\n      tokenizer.reset(new StringReader(parts[0]));\n      filter.reset();\n      assertTokenStreamContents(filter, new String[] { parts[1] });\n    }\n    breader.close();\n    isr.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"581320e68d1383a2350b6a7e52adb01c63ab8407","date":1303577715,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f1664166601a0f7376d051dda5dd63c068c313","date":1303641250,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName,\n          Reader reader) {\n        Tokenizer t = new KeywordTokenizer(reader);\n        return new TokenStreamComponents(t,\n            new GermanStemFilter(new LowerCaseFilter(TEST_VERSION_CURRENT, t)));\n      }\n    };\n    \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter#testStemming().mjava","sourceNew":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","sourceOld":"  public void testStemming() throws Exception {  \n    InputStream vocOut = getClass().getResourceAsStream(\"data.txt\");\n    assertVocabulary(analyzer, vocOut);\n    vocOut.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["581320e68d1383a2350b6a7e52adb01c63ab8407"],"581320e68d1383a2350b6a7e52adb01c63ab8407":["d8864af8de677008c534d14fc77568b83a6c1361"],"24f1664166601a0f7376d051dda5dd63c068c313":["5f4e87790277826a2aea119328600dfb07761f32","581320e68d1383a2350b6a7e52adb01c63ab8407"],"a3776dccca01c11e7046323cfad46a3b4a471233":["d8864af8de677008c534d14fc77568b83a6c1361","581320e68d1383a2350b6a7e52adb01c63ab8407"],"d8864af8de677008c534d14fc77568b83a6c1361":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d8864af8de677008c534d14fc77568b83a6c1361","581320e68d1383a2350b6a7e52adb01c63ab8407"],"5f4e87790277826a2aea119328600dfb07761f32":["0f080986da691a3bba7b757f43ab72cdc82b57ce","d8864af8de677008c534d14fc77568b83a6c1361"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"581320e68d1383a2350b6a7e52adb01c63ab8407":["b89678825b68eccaf09e6ab71675fc0b0af1e099","24f1664166601a0f7376d051dda5dd63c068c313","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"24f1664166601a0f7376d051dda5dd63c068c313":[],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"d8864af8de677008c534d14fc77568b83a6c1361":["581320e68d1383a2350b6a7e52adb01c63ab8407","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","5f4e87790277826a2aea119328600dfb07761f32"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"5f4e87790277826a2aea119328600dfb07761f32":["24f1664166601a0f7376d051dda5dd63c068c313"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["d8864af8de677008c534d14fc77568b83a6c1361","5f4e87790277826a2aea119328600dfb07761f32"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["24f1664166601a0f7376d051dda5dd63c068c313","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}