{"path":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","pathOld":"/dev/null","sourceNew":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    for (Token t = stream.next(); t!=null; t = stream.next()) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + t.termText()\n\t\t\t   + \" start=\" + t.startOffset()\n\t\t\t   + \" end=\" + t.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0eaa92bed9a03eab90a8ecd63ab0d80042296159","date":1206434151,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","pathOld":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","sourceNew":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    for (Token t = stream.next(); t!=null; t = stream.next()) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + new String(t.termBuffer(), 0, t.termLength())\n\t\t\t   + \" start=\" + t.startOffset()\n\t\t\t   + \" end=\" + t.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","sourceOld":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    for (Token t = stream.next(); t!=null; t = stream.next()) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + t.termText()\n\t\t\t   + \" start=\" + t.startOffset()\n\t\t\t   + \" end=\" + t.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","pathOld":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","sourceNew":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    final Token reusableToken = new Token();\n    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + nextToken.term()\n\t\t\t   + \" start=\" + nextToken.startOffset()\n\t\t\t   + \" end=\" + nextToken.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","sourceOld":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    for (Token t = stream.next(); t!=null; t = stream.next()) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + new String(t.termBuffer(), 0, t.termLength())\n\t\t\t   + \" start=\" + t.startOffset()\n\t\t\t   + \" end=\" + t.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","pathOld":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","sourceNew":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    \n    stream.reset();\n    TermAttribute termAtt = (TermAttribute) stream.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) stream.getAttribute(OffsetAttribute.class);\n    while (stream.incrementToken()) {      \n      if (verbose) {\n        System.out.println(\"Text=\" + termAtt.term()\n\t\t\t   + \" start=\" + offsetAtt.startOffset()\n\t\t\t   + \" end=\" + offsetAtt.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","sourceOld":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    final Token reusableToken = new Token();\n    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n      if (verbose) {\n\tSystem.out.println(\"Text=\" + nextToken.term()\n\t\t\t   + \" start=\" + nextToken.startOffset()\n\t\t\t   + \" end=\" + nextToken.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"520d06488b13c8ef837dd0815b77a80fff8779e1","date":1240950232,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/test/org/apache/lucene/AnalysisTest#test(Reader,boolean,long).mjava","sourceNew":null,"sourceOld":"  static void test(Reader reader, boolean verbose, long bytes)\n       throws Exception {\n    Analyzer analyzer = new SimpleAnalyzer();\n    TokenStream stream = analyzer.tokenStream(null, reader);\n\n    Date start = new Date();\n\n    int count = 0;\n    \n    stream.reset();\n    TermAttribute termAtt = (TermAttribute) stream.getAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) stream.getAttribute(OffsetAttribute.class);\n    while (stream.incrementToken()) {      \n      if (verbose) {\n        System.out.println(\"Text=\" + termAtt.term()\n\t\t\t   + \" start=\" + offsetAtt.startOffset()\n\t\t\t   + \" end=\" + offsetAtt.endOffset());\n      }\n      count++;\n    }\n\n    Date end = new Date();\n\n    long time = end.getTime() - start.getTime();\n    System.out.println(time + \" milliseconds to extract \" + count + \" tokens\");\n    System.out.println((time*1000.0)/count + \" microseconds/token\");\n    System.out.println((bytes * 1000.0 * 60.0 * 60.0)/(time * 1000000.0)\n\t\t       + \" megabytes/hour\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["0eaa92bed9a03eab90a8ecd63ab0d80042296159"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"520d06488b13c8ef837dd0815b77a80fff8779e1":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0eaa92bed9a03eab90a8ecd63ab0d80042296159":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["520d06488b13c8ef837dd0815b77a80fff8779e1"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["0eaa92bed9a03eab90a8ecd63ab0d80042296159"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["520d06488b13c8ef837dd0815b77a80fff8779e1"],"520d06488b13c8ef837dd0815b77a80fff8779e1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"0eaa92bed9a03eab90a8ecd63ab0d80042296159":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}