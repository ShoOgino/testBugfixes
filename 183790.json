{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","commits":[{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment) throws IOException {\n    assert flushedSegment != null;\n\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        codec.liveDocsFormat().writeLiveDocs(flushedSegment.liveDocs, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef","date":1512420564,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":["c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"]},"commit2Childs":{"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}