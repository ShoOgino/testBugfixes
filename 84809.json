{"path":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","commits":[{"id":"4b8414cdacb05e1277df96a30710f570f4251d9a","date":1323040348,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,boolean,int).mjava","sourceNew":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(true /** nocommit: remove readOnly */, sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, boolean readOnly, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.readOnly = true; // nocommit: remove readOnly at all\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(readOnly, sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f","date":1323210518,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","sourceNew":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(true /** nocommit: remove readOnly */, sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,IndexDeletionPolicy,boolean,int).mjava","sourceNew":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, IndexDeletionPolicy deletionPolicy, boolean readOnly, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.readOnly = readOnly;\n    this.segmentInfos = sis;\n    this.deletionPolicy = deletionPolicy;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(readOnly, sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,IndexDeletionPolicy,boolean,int).mjava","sourceNew":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, IndexDeletionPolicy deletionPolicy, boolean readOnly, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.readOnly = readOnly;\n    this.segmentInfos = sis;\n    this.deletionPolicy = deletionPolicy;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(readOnly, sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4","date":1323543613,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","sourceNew":null,"sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DirectoryReader#DirectoryReader(Directory,SegmentInfos,int).mjava","sourceNew":null,"sourceOld":"  /** Construct reading the named set of readers. */\n  DirectoryReader(Directory directory, SegmentInfos sis, int termInfosIndexDivisor) throws IOException {\n    this.directory = directory;\n    this.segmentInfos = sis;\n    this.termInfosIndexDivisor = termInfosIndexDivisor;\n    readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());\n    applyAllDeletes = false;\n\n    // To reduce the chance of hitting FileNotFound\n    // (and having to retry), we open segments in\n    // reverse because IndexWriter merges & deletes\n    // the newest segments first.\n\n    SegmentReader[] readers = new SegmentReader[sis.size()];\n    for (int i = sis.size()-1; i >= 0; i--) {\n      boolean success = false;\n      try {\n        readers[i] = SegmentReader.get(sis.info(i), termInfosIndexDivisor, IOContext.READ);\n        readers[i].readerFinishedListeners = readerFinishedListeners;\n        success = true;\n      } finally {\n        if (!success) {\n          // Close all readers we had opened:\n          for(i++;i<sis.size();i++) {\n            try {\n              readers[i].close();\n            } catch (Throwable ignore) {\n              // keep going - we want to clean up as much as possible\n            }\n          }\n        }\n      }\n    }\n\n    initialize(readers);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4b8414cdacb05e1277df96a30710f570f4251d9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["4b8414cdacb05e1277df96a30710f570f4251d9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4"]},"commit2Childs":{"4b8414cdacb05e1277df96a30710f570f4251d9a":["cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f"],"6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4b8414cdacb05e1277df96a30710f570f4251d9a","ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["6e2df1cc3e8621670ff46d098e43b7dd3f66eaf4","ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}