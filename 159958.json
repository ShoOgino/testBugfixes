{"path":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","commits":[{"id":"18d06c030cb0a920c116ec6e16933a4590a70bb9","date":1144087994,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","pathOld":"/dev/null","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc());\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1940b60224897131cf61bb615e02af1b26558c8","date":1169501002,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc());\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6d6338c87060be5f66757a94945975f3bbd377a9","date":1189278234,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30c238dad8c4234f556cd28cd22ff426247e70c4","date":1195490330,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    int last = offset+len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    final int lastDocRequested = offset+len;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","bugFix":["18d06c030cb0a920c116ec6e16933a4590a70bb9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db25c1f61b5ae826f10777da6551a832703967d5","date":1215306972,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            hq.insert(new FieldDoc(doc, score));\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            if (numHits[0]++ < lastDocRequested || score >= minScore) {\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              hq.insert(new ScoreDoc(doc, score));\n              minScore = ((ScoreDoc)hq.top()).score;\n            }\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    int last = offset+len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6d6338c87060be5f66757a94945975f3bbd377a9":["b1940b60224897131cf61bb615e02af1b26558c8"],"30c238dad8c4234f556cd28cd22ff426247e70c4":["6d6338c87060be5f66757a94945975f3bbd377a9"],"18d06c030cb0a920c116ec6e16933a4590a70bb9":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b1940b60224897131cf61bb615e02af1b26558c8":["18d06c030cb0a920c116ec6e16933a4590a70bb9"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"db25c1f61b5ae826f10777da6551a832703967d5":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"6d6338c87060be5f66757a94945975f3bbd377a9":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"30c238dad8c4234f556cd28cd22ff426247e70c4":["db25c1f61b5ae826f10777da6551a832703967d5"],"18d06c030cb0a920c116ec6e16933a4590a70bb9":["b1940b60224897131cf61bb615e02af1b26558c8"],"b1940b60224897131cf61bb615e02af1b26558c8":["6d6338c87060be5f66757a94945975f3bbd377a9"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["18d06c030cb0a920c116ec6e16933a4590a70bb9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"db25c1f61b5ae826f10777da6551a832703967d5":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["db25c1f61b5ae826f10777da6551a832703967d5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}