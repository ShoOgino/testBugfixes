{"path":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","commits":[{"id":"84c1ba52905cc7eaf624aac5e10414eccc0af92d","date":1464805673,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b8ee93140fd0efef7e101786e3ed5160a700b5f","date":1464820111,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77","date":1464821470,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f69e96b07e265f3e18957be540909b01fae36f8","date":1464859090,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    //System.out.println(\"TEST: numOperations=\" + numOperations + \" ADD_CUTOFF=\" + ADD_CUTOFF + \" UPD_CUTOFF=\" + UPD_CUTOFF);\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        //System.out.println(\"TEST i=\" + i + \": addDocument id=\" + id + \" val=\" + val);\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDocument id=\" + id + \" val=\" + val);\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          //System.out.println(\"TEST i=\" + i + \": updateDV id=\" + id + \" val=\" + val);\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testBiasedMixOfRandomUpdates().mjava","sourceNew":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits.value);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  public void testBiasedMixOfRandomUpdates() throws Exception {\n    // 3 types of operations: add, updated, updateDV.\n    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,\n    // in terms of some ops happen more often then others\n    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);\n    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);\n\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    final int numOperations = atLeast(1000);\n    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);\n\n    // start with at least one doc before any chance of updates\n    final int numSeedDocs = atLeast(1); \n    for (int i = 0; i < numSeedDocs; i++) {\n      final long val = random().nextLong();\n      expected.put(i, val);\n      writer.addDocument(doc(i, val));\n    }\n\n    int numDocUpdates = 0;\n    int numValueUpdates = 0;\n\n    for (int i = 0; i < numOperations; i++) {\n      final int op = TestUtil.nextInt(random(), 1, 100);\n      final long val = random().nextLong();\n      if (op <= ADD_CUTOFF) {\n        final int id = expected.size();\n        expected.put(id, val);\n        writer.addDocument(doc(id, val));\n      } else {\n        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);\n        expected.put(id, val);\n        if (op <= UPD_CUTOFF) {\n          numDocUpdates++;\n          writer.updateDocument(new Term(\"id\",\"doc-\" + id), doc(id, val));\n        } else {\n          numValueUpdates++;\n          writer.updateNumericDocValue(new Term(\"id\",\"doc-\" + id), \"val\", val);\n        }\n      }\n    }\n\n    writer.commit();\n    \n    final DirectoryReader reader = DirectoryReader.open(dir);\n    final IndexSearcher searcher = new IndexSearcher(reader);\n\n    // TODO: make more efficient if max numOperations is going to be increased much\n    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {\n      String id = \"doc-\" + expect.getKey();\n      TopFieldDocs td = searcher.search(new TermQuery(new Term(\"id\", id)), 1, \n                                        new Sort(new SortField(\"val\", SortField.Type.LONG)));\n      assertEquals(id + \" missing?\", 1, td.totalHits);\n      assertEquals(id + \" value\", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);\n    }\n    \n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["1f69e96b07e265f3e18957be540909b01fae36f8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f69e96b07e265f3e18957be540909b01fae36f8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","84c1ba52905cc7eaf624aac5e10414eccc0af92d"],"84c1ba52905cc7eaf624aac5e10414eccc0af92d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["28288370235ed02234a64753cdbf0c6ec096304a"],"5b8ee93140fd0efef7e101786e3ed5160a700b5f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","84c1ba52905cc7eaf624aac5e10414eccc0af92d"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["1f69e96b07e265f3e18957be540909b01fae36f8","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["1f69e96b07e265f3e18957be540909b01fae36f8","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f69e96b07e265f3e18957be540909b01fae36f8"],"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5b8ee93140fd0efef7e101786e3ed5160a700b5f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"1f69e96b07e265f3e18957be540909b01fae36f8":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f69e96b07e265f3e18957be540909b01fae36f8","84c1ba52905cc7eaf624aac5e10414eccc0af92d","5b8ee93140fd0efef7e101786e3ed5160a700b5f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77"],"84c1ba52905cc7eaf624aac5e10414eccc0af92d":["1f69e96b07e265f3e18957be540909b01fae36f8","5b8ee93140fd0efef7e101786e3ed5160a700b5f"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5b8ee93140fd0efef7e101786e3ed5160a700b5f":["b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}