{"path":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","commits":[{"id":"6864413dbc0c12104c978c05456f3da1d45adb03","date":1186770873,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next().mjava","sourceNew":"  public final Token next(Token token) throws IOException {\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = token.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = token.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    token.termLength = length;\n    token.startOffset = start;\n    token.endOffset = start+length;\n    return token;\n  }\n\n","sourceOld":"  /** Returns the next token in the stream, or null at EOS. */\n  public final Token next() throws IOException {\n    int length = 0;\n    int start = offset;\n    while (true) {\n      final char c;\n\n      offset++;\n      if (bufferIndex >= dataLen) {\n        dataLen = input.read(ioBuffer);\n        bufferIndex = 0;\n      }\n      ;\n      if (dataLen == -1) {\n        if (length > 0)\n          break;\n        else\n          return null;\n      } else\n        c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset - 1;\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n\n    }\n\n    return new Token(new String(buffer, 0, length), start, start + length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa","date":1199000070,"type":3,"author":"Doron Cohen","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":"  public final Token next(Token token) throws IOException {\n    token.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = token.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = token.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    token.termLength = length;\n    token.startOffset = start;\n    token.endOffset = start+length;\n    return token;\n  }\n\n","sourceOld":"  public final Token next(Token token) throws IOException {\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = token.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = token.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    token.termLength = length;\n    token.startOffset = start;\n    token.endOffset = start+length;\n    return token;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":"  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(start);\n    reusableToken.setEndOffset(start+length);\n    return reusableToken;\n  }\n\n","sourceOld":"  public final Token next(Token token) throws IOException {\n    token.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = token.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = token.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    token.termLength = length;\n    token.startOffset = start;\n    token.endOffset = start+length;\n    return token;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":"  /** @deprecated */\n  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(start);\n    reusableToken.setEndOffset(start+length);\n    return reusableToken;\n  }\n\n","sourceOld":"  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(start);\n    reusableToken.setEndOffset(start+length);\n    return reusableToken;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2","date":1245784531,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":"  /** @deprecated */\n  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(input.correctOffset(start));\n    reusableToken.setEndOffset(input.correctOffset(start+length));\n    return reusableToken;\n  }\n\n","sourceOld":"  /** @deprecated */\n  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(start);\n    reusableToken.setEndOffset(start+length);\n    return reusableToken;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec8b5a20a12931b8d7e616c79c5248ae06cc5568","date":1248471948,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":"  /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n   * not be overridden. Delegates to the backwards compatibility layer. */\n  public final Token next(final Token reusableToken) throws IOException {\n    return super.next(reusableToken);\n  }\n\n","sourceOld":"  /** @deprecated */\n  public final Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    reusableToken.clear();\n    int length = 0;\n    int start = bufferIndex;\n    char[] buffer = reusableToken.termBuffer();\n    while (true) {\n\n      if (bufferIndex >= dataLen) {\n        offset += dataLen;\n        dataLen = input.read(ioBuffer);\n        if (dataLen == -1) {\n          if (length > 0)\n            break;\n          else\n            return null;\n        }\n        bufferIndex = 0;\n      }\n\n      final char c = ioBuffer[bufferIndex++];\n\n      if (isTokenChar(c)) {               // if it's a token char\n\n        if (length == 0)\t\t\t           // start of token\n          start = offset + bufferIndex - 1;\n        else if (length == buffer.length)\n          buffer = reusableToken.resizeTermBuffer(1+length);\n\n        buffer[length++] = normalize(c); // buffer it, normalized\n\n        if (length == MAX_WORD_LEN)\t\t   // buffer overflow!\n          break;\n\n      } else if (length > 0)             // at non-Letter w/ chars\n        break;                           // return 'em\n    }\n\n    reusableToken.setTermLength(length);\n    reusableToken.setStartOffset(input.correctOffset(start));\n    reusableToken.setEndOffset(input.correctOffset(start+length));\n    return reusableToken;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"439b0fe2f799d1c722151e88e32bdefad8d34ebe","date":1255282509,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/analysis/CharTokenizer#next(Token).mjava","sourceNew":null,"sourceOld":"  /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n   * not be overridden. Delegates to the backwards compatibility layer. */\n  public final Token next(final Token reusableToken) throws IOException {\n    return super.next(reusableToken);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["fee44d0bd0b9443ff6068d0ba8458fd103dff4aa"],"6864413dbc0c12104c978c05456f3da1d45adb03":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa":["6864413dbc0c12104c978c05456f3da1d45adb03"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["ec8b5a20a12931b8d7e616c79c5248ae06cc5568"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"ec8b5a20a12931b8d7e616c79c5248ae06cc5568":["cd27af5c226d98a7c6378c388a67a3bff7c0b3a2"],"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"6864413dbc0c12104c978c05456f3da1d45adb03":["fee44d0bd0b9443ff6068d0ba8458fd103dff4aa"],"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["cd27af5c226d98a7c6378c388a67a3bff7c0b3a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6864413dbc0c12104c978c05456f3da1d45adb03"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ec8b5a20a12931b8d7e616c79c5248ae06cc5568":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"cd27af5c226d98a7c6378c388a67a3bff7c0b3a2":["ec8b5a20a12931b8d7e616c79c5248ae06cc5568"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}