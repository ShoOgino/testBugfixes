{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    assert !enableChecks || (streamState == State.RESET || streamState == State.INCREMENT) \n                            : \"incrementToken() called while in wrong state: \" + streamState;\n    clearAttributes();\n    for (;;) {\n      int startOffset = off;\n      int cp = readCodePoint();\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        assert correctedStartOffset >= 0;\n        assert correctedEndOffset >= 0;\n        assert correctedStartOffset >= lastOffset;\n        lastOffset = correctedStartOffset;\n        assert correctedEndOffset >= correctedStartOffset;\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        streamState = State.INCREMENT;\n        return true;\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    assert !enableChecks || (streamState == State.RESET || streamState == State.INCREMENT) \n                            : \"incrementToken() called while in wrong state: \" + streamState;\n    clearAttributes();\n    for (;;) {\n      int startOffset = off;\n      int cp = readCodePoint();\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        assert correctedStartOffset >= 0;\n        assert correctedEndOffset >= 0;\n        assert correctedStartOffset >= lastOffset;\n        lastOffset = correctedStartOffset;\n        assert correctedEndOffset >= correctedStartOffset;\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        streamState = State.INCREMENT;\n        return true;\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"089745617a0f9c49f3719652025f61c07e5ce4ae","date":1381543020,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    assert !enableChecks || (streamState == State.RESET || streamState == State.INCREMENT) \n                            : \"incrementToken() called while in wrong state: \" + streamState;\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, its because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        assert correctedStartOffset >= 0;\n        assert correctedEndOffset >= 0;\n        assert correctedStartOffset >= lastOffset;\n        lastOffset = correctedStartOffset;\n        assert correctedEndOffset >= correctedStartOffset;\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    assert !enableChecks || (streamState == State.RESET || streamState == State.INCREMENT) \n                            : \"incrementToken() called while in wrong state: \" + streamState;\n    clearAttributes();\n    for (;;) {\n      int startOffset = off;\n      int cp = readCodePoint();\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        assert correctedStartOffset >= 0;\n        assert correctedEndOffset >= 0;\n        assert correctedStartOffset >= lastOffset;\n        lastOffset = correctedStartOffset;\n        assert correctedEndOffset >= correctedStartOffset;\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        streamState = State.INCREMENT;\n        return true;\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","bugFix":["1d6179f9c4237a7e5d423f4e4b439a94e967efc9","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529","date":1419024596,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    if (streamState != State.RESET && streamState != State.INCREMENT) {\n      fail(\"incrementToken() called while in wrong state: \" + streamState);\n    }\n\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, its because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        if (correctedStartOffset < 0) {\n          failAlways(\"invalid start offset: \" + correctedStartOffset + \", before correction: \" + startOffset);\n        }\n        if (correctedEndOffset < 0) {\n          failAlways(\"invalid end offset: \" + correctedEndOffset + \", before correction: \" + endOffset);\n        }\n        if (correctedStartOffset < lastOffset) {\n          failAlways(\"start offset went backwards: \" + correctedStartOffset + \", before correction: \" + startOffset + \", lastOffset: \" + lastOffset);\n        }\n        lastOffset = correctedStartOffset;\n        if (correctedEndOffset < correctedStartOffset) {\n          failAlways(\"end offset: \" + correctedEndOffset + \" is before start offset: \" + correctedStartOffset);\n        }\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    assert !enableChecks || (streamState == State.RESET || streamState == State.INCREMENT) \n                            : \"incrementToken() called while in wrong state: \" + streamState;\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, its because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        assert correctedStartOffset >= 0;\n        assert correctedEndOffset >= 0;\n        assert correctedStartOffset >= lastOffset;\n        lastOffset = correctedStartOffset;\n        assert correctedEndOffset >= correctedStartOffset;\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    if (streamState != State.RESET && streamState != State.INCREMENT) {\n      fail(\"incrementToken() called while in wrong state: \" + streamState);\n    }\n\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, it's because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        if (correctedStartOffset < 0) {\n          failAlways(\"invalid start offset: \" + correctedStartOffset + \", before correction: \" + startOffset);\n        }\n        if (correctedEndOffset < 0) {\n          failAlways(\"invalid end offset: \" + correctedEndOffset + \", before correction: \" + endOffset);\n        }\n        if (correctedStartOffset < lastOffset) {\n          failAlways(\"start offset went backwards: \" + correctedStartOffset + \", before correction: \" + startOffset + \", lastOffset: \" + lastOffset);\n        }\n        lastOffset = correctedStartOffset;\n        if (correctedEndOffset < correctedStartOffset) {\n          failAlways(\"end offset: \" + correctedEndOffset + \" is before start offset: \" + correctedStartOffset);\n        }\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    if (streamState != State.RESET && streamState != State.INCREMENT) {\n      fail(\"incrementToken() called while in wrong state: \" + streamState);\n    }\n\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, its because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        if (correctedStartOffset < 0) {\n          failAlways(\"invalid start offset: \" + correctedStartOffset + \", before correction: \" + startOffset);\n        }\n        if (correctedEndOffset < 0) {\n          failAlways(\"invalid end offset: \" + correctedEndOffset + \", before correction: \" + endOffset);\n        }\n        if (correctedStartOffset < lastOffset) {\n          failAlways(\"start offset went backwards: \" + correctedStartOffset + \", before correction: \" + startOffset + \", lastOffset: \" + lastOffset);\n        }\n        lastOffset = correctedStartOffset;\n        if (correctedEndOffset < correctedStartOffset) {\n          failAlways(\"end offset: \" + correctedEndOffset + \" is before start offset: \" + correctedStartOffset);\n        }\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"617d6d4150e0fb2acef8980ce51e3b8e628fb200","date":1580326292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    if (streamState != State.RESET && streamState != State.INCREMENT) {\n      fail(\"incrementToken() called while in wrong state: \" + streamState);\n    }\n\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        char chars[] = new char[2];\n        int endOffset;\n        do {\n          int len = Character.toChars(normalize(cp), chars, 0);\n          for (int i = 0; i < len; i++) {\n            termAtt.append(chars[i]);\n          }\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, it's because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        if (correctedStartOffset < 0) {\n          failAlways(\"invalid start offset: \" + correctedStartOffset + \", before correction: \" + startOffset);\n        }\n        if (correctedEndOffset < 0) {\n          failAlways(\"invalid end offset: \" + correctedEndOffset + \", before correction: \" + endOffset);\n        }\n        if (correctedStartOffset < lastOffset) {\n          failAlways(\"start offset went backwards: \" + correctedStartOffset + \", before correction: \" + startOffset + \", lastOffset: \" + lastOffset);\n        }\n        lastOffset = correctedStartOffset;\n        if (correctedEndOffset < correctedStartOffset) {\n          failAlways(\"end offset: \" + correctedEndOffset + \" is before start offset: \" + correctedStartOffset);\n        }\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    if (streamState != State.RESET && streamState != State.INCREMENT) {\n      fail(\"incrementToken() called while in wrong state: \" + streamState);\n    }\n\n    clearAttributes();\n    for (;;) {\n      int startOffset;\n      int cp;\n      if (bufferedCodePoint >= 0) {\n        cp = bufferedCodePoint;\n        startOffset = bufferedOff;\n        bufferedCodePoint = -1;\n      } else {\n        startOffset = off;\n        cp = readCodePoint();\n      }\n      if (cp < 0) {\n        break;\n      } else if (isTokenChar(cp)) {\n        int endOffset;\n        do {\n          char chars[] = Character.toChars(normalize(cp));\n          for (int i = 0; i < chars.length; i++)\n            termAtt.append(chars[i]);\n          endOffset = off;\n          if (termAtt.length() >= maxTokenLength) {\n            break;\n          }\n          cp = readCodePoint();\n        } while (cp >= 0 && isTokenChar(cp));\n        \n        if (termAtt.length() < maxTokenLength) {\n          // buffer up, in case the \"rejected\" char can start a new word of its own\n          bufferedCodePoint = cp;\n          bufferedOff = endOffset;\n        } else {\n          // otherwise, it's because we hit term limit.\n          bufferedCodePoint = -1;\n        }\n        int correctedStartOffset = correctOffset(startOffset);\n        int correctedEndOffset = correctOffset(endOffset);\n        if (correctedStartOffset < 0) {\n          failAlways(\"invalid start offset: \" + correctedStartOffset + \", before correction: \" + startOffset);\n        }\n        if (correctedEndOffset < 0) {\n          failAlways(\"invalid end offset: \" + correctedEndOffset + \", before correction: \" + endOffset);\n        }\n        if (correctedStartOffset < lastOffset) {\n          failAlways(\"start offset went backwards: \" + correctedStartOffset + \", before correction: \" + startOffset + \", lastOffset: \" + lastOffset);\n        }\n        lastOffset = correctedStartOffset;\n        if (correctedEndOffset < correctedStartOffset) {\n          failAlways(\"end offset: \" + correctedEndOffset + \" is before start offset: \" + correctedStartOffset);\n        }\n        offsetAtt.setOffset(correctedStartOffset, correctedEndOffset);\n        if (state == -1 || runAutomaton.isAccept(state)) {\n          // either we hit a reject state (longest match), or end-of-text, but in an accept state\n          streamState = State.INCREMENT;\n          return true;\n        }\n      }\n    }\n    streamState = State.INCREMENT_FALSE;\n    return false;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["50d5b588b112eeb3d6b2a3fcc43a40ef0615a529"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"617d6d4150e0fb2acef8980ce51e3b8e628fb200":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529":["089745617a0f9c49f3719652025f61c07e5ce4ae"],"089745617a0f9c49f3719652025f61c07e5ce4ae":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["617d6d4150e0fb2acef8980ce51e3b8e628fb200"]},"commit2Childs":{"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["617d6d4150e0fb2acef8980ce51e3b8e628fb200"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["089745617a0f9c49f3719652025f61c07e5ce4ae"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"617d6d4150e0fb2acef8980ce51e3b8e628fb200":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"089745617a0f9c49f3719652025f61c07e5ce4ae":["50d5b588b112eeb3d6b2a3fcc43a40ef0615a529"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}