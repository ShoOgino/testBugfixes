{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef","date":1512420564,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15e716649e2bd79a98b5e68c464154ea4c44677a","date":1523975212,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          readerPool.release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"15e716649e2bd79a98b5e68c464154ea4c44677a":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["15e716649e2bd79a98b5e68c464154ea4c44677a"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"15e716649e2bd79a98b5e68c464154ea4c44677a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}