{"path":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#init(Document,int).mjava","commits":[{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#init(Document,int).mjava","pathOld":"/dev/null","sourceNew":"  /** Initializes shared state for this new document */\n  void init(Document doc, int docID) throws IOException, AbortException {\n\n    assert !isIdle;\n    assert docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    this.docID = docID;\n    docBoost = doc.getBoost();\n    numStoredFields = 0;\n    numFieldData = 0;\n    numVectorFields = 0;\n    maxTermPrefix = null;\n\n    assert 0 == fdtLocal.length();\n    assert 0 == fdtLocal.getFilePointer();\n    assert 0 == tvfLocal.length();\n    assert 0 == tvfLocal.getFilePointer();\n    final int thisFieldGen = fieldGen++;\n\n    List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n    boolean docHasVectors = false;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n\n      FieldInfo fi = docWriter.fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                              field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                              field.getOmitNorms(), false);\n      if (fi.isIndexed && !fi.omitNorms) {\n        // Maybe grow our buffered norms\n        if (docWriter.norms.length <= fi.number) {\n          int newSize = (int) ((1+fi.number)*1.25);\n          BufferedNorms[] newNorms = new BufferedNorms[newSize];\n          System.arraycopy(docWriter.norms, 0, newNorms, 0, docWriter.norms.length);\n          docWriter.norms = newNorms;\n        }\n          \n        if (docWriter.norms[fi.number] == null)\n          docWriter.norms[fi.number] = new BufferedNorms();\n\n        docWriter.hasNorms = true;\n      }\n\n      // Make sure we have a FieldData allocated\n      int hashPos = fi.name.hashCode() & fieldDataHashMask;\n      DocumentsWriterFieldData fp = fieldDataHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        fp = new DocumentsWriterFieldData(this, fi);\n        fp.next = fieldDataHash[hashPos];\n        fieldDataHash[hashPos] = fp;\n\n        if (numAllFieldData == allFieldDataArray.length) {\n          int newSize = (int) (allFieldDataArray.length*1.5);\n          int newHashSize = fieldDataHash.length*2;\n\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          DocumentsWriterFieldData newHashArray[] = new DocumentsWriterFieldData[newHashSize];\n          System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n          // Rehash\n          fieldDataHashMask = newSize-1;\n          for(int j=0;j<fieldDataHash.length;j++) {\n            DocumentsWriterFieldData fp0 = fieldDataHash[j];\n            while(fp0 != null) {\n              hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n              DocumentsWriterFieldData nextFP0 = fp0.next;\n              fp0.next = newHashArray[hashPos];\n              newHashArray[hashPos] = fp0;\n              fp0 = nextFP0;\n            }\n          }\n\n          allFieldDataArray = newArray;\n          fieldDataHash = newHashArray;\n        }\n        allFieldDataArray[numAllFieldData++] = fp;\n      } else {\n        assert fp.fieldInfo == fi;\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.lastGen = thisFieldGen;\n        fp.fieldCount = 0;\n        fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n        fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n        if (numFieldData == fieldDataArray.length) {\n          int newSize = fieldDataArray.length*2;\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n          fieldDataArray = newArray;\n\n        }\n        fieldDataArray[numFieldData++] = fp;\n      }\n\n      if (field.isTermVectorStored()) {\n        if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n          final int newSize = (int) (numVectorFields*1.5);\n          vectorFieldPointers = new long[newSize];\n          vectorFieldNumbers = new int[newSize];\n        }\n        fp.doVectors = true;\n        docHasVectors = true;\n\n        fp.doVectorPositions |= field.isStorePositionWithTermVector();\n        fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n      }\n\n      if (fp.fieldCount == fp.docFields.length) {\n        Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n        System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n        fp.docFields = newArray;\n      }\n\n      // Lazily allocate arrays for postings:\n      if (field.isIndexed() && fp.postingsHash == null)\n        fp.initPostingArrays();\n\n      fp.docFields[fp.fieldCount++] = field;\n    }\n\n    // Maybe init the local & global fieldsWriter\n    if (localFieldsWriter == null) {\n      if (docWriter.fieldsWriter == null) {\n        assert docWriter.docStoreSegment == null;\n        assert docWriter.segment != null;\n        docWriter.docStoreSegment = docWriter.segment;\n        // If we hit an exception while init'ing the\n        // fieldsWriter, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.fieldsWriter = new FieldsWriter(docWriter.directory, docWriter.docStoreSegment, docWriter.fieldInfos);\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n        docWriter.files = null;\n      }\n      localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);\n    }\n\n    // First time we see a doc that has field(s) with\n    // stored vectors, we init our tvx writer\n    if (docHasVectors) {\n      if (docWriter.tvx == null) {\n        assert docWriter.docStoreSegment != null;\n        // If we hit an exception while init'ing the term\n        // vector output files, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n          docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n          docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n          // We must \"catch up\" for all docs before us\n          // that had no vectors:\n          for(int i=0;i<docWriter.numDocsInStore;i++) {\n            docWriter.tvx.writeLong(docWriter.tvd.getFilePointer());\n            docWriter.tvd.writeVInt(0);\n            docWriter.tvx.writeLong(0);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n        docWriter.files = null;\n      }\n      numVectorFields = 0;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4","date":1206538765,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#init(Document,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#init(Document,int).mjava","sourceNew":"  /** Initializes shared state for this new document */\n  void init(Document doc, int docID) throws IOException, AbortException {\n\n    assert !isIdle;\n    assert docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    this.docID = docID;\n    docBoost = doc.getBoost();\n    numStoredFields = 0;\n    numFieldData = 0;\n    numVectorFields = 0;\n    maxTermPrefix = null;\n\n    assert 0 == fdtLocal.length();\n    assert 0 == fdtLocal.getFilePointer();\n    assert 0 == tvfLocal.length();\n    assert 0 == tvfLocal.getFilePointer();\n    final int thisFieldGen = fieldGen++;\n\n    List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n    boolean docHasVectors = false;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n\n      FieldInfo fi = docWriter.fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                              field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                              field.getOmitNorms(), false);\n      if (fi.isIndexed && !fi.omitNorms) {\n        // Maybe grow our buffered norms\n        if (docWriter.norms.length <= fi.number) {\n          int newSize = (int) ((1+fi.number)*1.25);\n          BufferedNorms[] newNorms = new BufferedNorms[newSize];\n          System.arraycopy(docWriter.norms, 0, newNorms, 0, docWriter.norms.length);\n          docWriter.norms = newNorms;\n        }\n          \n        if (docWriter.norms[fi.number] == null)\n          docWriter.norms[fi.number] = new BufferedNorms();\n\n        docWriter.hasNorms = true;\n      }\n\n      // Make sure we have a FieldData allocated\n      int hashPos = fi.name.hashCode() & fieldDataHashMask;\n      DocumentsWriterFieldData fp = fieldDataHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        fp = new DocumentsWriterFieldData(this, fi);\n        fp.next = fieldDataHash[hashPos];\n        fieldDataHash[hashPos] = fp;\n\n        if (numAllFieldData == allFieldDataArray.length) {\n          int newSize = (int) (allFieldDataArray.length*1.5);\n          int newHashSize = fieldDataHash.length*2;\n\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          DocumentsWriterFieldData newHashArray[] = new DocumentsWriterFieldData[newHashSize];\n          System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n          // Rehash\n          fieldDataHashMask = newSize-1;\n          for(int j=0;j<fieldDataHash.length;j++) {\n            DocumentsWriterFieldData fp0 = fieldDataHash[j];\n            while(fp0 != null) {\n              hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n              DocumentsWriterFieldData nextFP0 = fp0.next;\n              fp0.next = newHashArray[hashPos];\n              newHashArray[hashPos] = fp0;\n              fp0 = nextFP0;\n            }\n          }\n\n          allFieldDataArray = newArray;\n          fieldDataHash = newHashArray;\n        }\n        allFieldDataArray[numAllFieldData++] = fp;\n      } else {\n        assert fp.fieldInfo == fi;\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.lastGen = thisFieldGen;\n        fp.fieldCount = 0;\n        fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n        fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n        if (numFieldData == fieldDataArray.length) {\n          int newSize = fieldDataArray.length*2;\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n          fieldDataArray = newArray;\n\n        }\n        fieldDataArray[numFieldData++] = fp;\n      }\n\n      if (field.isTermVectorStored()) {\n        if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n          final int newSize = (int) (numVectorFields*1.5);\n          vectorFieldPointers = new long[newSize];\n          vectorFieldNumbers = new int[newSize];\n        }\n        fp.doVectors = true;\n        docHasVectors = true;\n\n        fp.doVectorPositions |= field.isStorePositionWithTermVector();\n        fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n      }\n\n      if (fp.fieldCount == fp.docFields.length) {\n        Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n        System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n        fp.docFields = newArray;\n      }\n\n      // Lazily allocate arrays for postings:\n      if (field.isIndexed() && fp.postingsHash == null)\n        fp.initPostingArrays();\n\n      fp.docFields[fp.fieldCount++] = field;\n    }\n\n    // Maybe init the local & global fieldsWriter\n    if (localFieldsWriter == null) {\n      if (docWriter.fieldsWriter == null) {\n        assert docWriter.docStoreSegment == null;\n        assert docWriter.segment != null;\n        docWriter.files = null;\n        docWriter.docStoreSegment = docWriter.segment;\n        // If we hit an exception while init'ing the\n        // fieldsWriter, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.fieldsWriter = new FieldsWriter(docWriter.directory, docWriter.docStoreSegment, docWriter.fieldInfos);\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n      }\n      localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);\n    }\n\n    // First time we see a doc that has field(s) with\n    // stored vectors, we init our tvx writer\n    if (docHasVectors) {\n      if (docWriter.tvx == null) {\n        assert docWriter.docStoreSegment != null;\n        docWriter.files = null;\n        // If we hit an exception while init'ing the term\n        // vector output files, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);\n          docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);\n          docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);\n\n          // We must \"catch up\" for all docs before us\n          // that had no vectors:\n          for(int i=0;i<docWriter.numDocsInStore;i++) {\n            docWriter.tvx.writeLong(docWriter.tvd.getFilePointer());\n            docWriter.tvd.writeVInt(0);\n            docWriter.tvx.writeLong(0);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n      }\n      numVectorFields = 0;\n    }\n  }\n\n","sourceOld":"  /** Initializes shared state for this new document */\n  void init(Document doc, int docID) throws IOException, AbortException {\n\n    assert !isIdle;\n    assert docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    this.docID = docID;\n    docBoost = doc.getBoost();\n    numStoredFields = 0;\n    numFieldData = 0;\n    numVectorFields = 0;\n    maxTermPrefix = null;\n\n    assert 0 == fdtLocal.length();\n    assert 0 == fdtLocal.getFilePointer();\n    assert 0 == tvfLocal.length();\n    assert 0 == tvfLocal.getFilePointer();\n    final int thisFieldGen = fieldGen++;\n\n    List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n    boolean docHasVectors = false;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n\n      FieldInfo fi = docWriter.fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                              field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                              field.getOmitNorms(), false);\n      if (fi.isIndexed && !fi.omitNorms) {\n        // Maybe grow our buffered norms\n        if (docWriter.norms.length <= fi.number) {\n          int newSize = (int) ((1+fi.number)*1.25);\n          BufferedNorms[] newNorms = new BufferedNorms[newSize];\n          System.arraycopy(docWriter.norms, 0, newNorms, 0, docWriter.norms.length);\n          docWriter.norms = newNorms;\n        }\n          \n        if (docWriter.norms[fi.number] == null)\n          docWriter.norms[fi.number] = new BufferedNorms();\n\n        docWriter.hasNorms = true;\n      }\n\n      // Make sure we have a FieldData allocated\n      int hashPos = fi.name.hashCode() & fieldDataHashMask;\n      DocumentsWriterFieldData fp = fieldDataHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        fp = new DocumentsWriterFieldData(this, fi);\n        fp.next = fieldDataHash[hashPos];\n        fieldDataHash[hashPos] = fp;\n\n        if (numAllFieldData == allFieldDataArray.length) {\n          int newSize = (int) (allFieldDataArray.length*1.5);\n          int newHashSize = fieldDataHash.length*2;\n\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          DocumentsWriterFieldData newHashArray[] = new DocumentsWriterFieldData[newHashSize];\n          System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n          // Rehash\n          fieldDataHashMask = newSize-1;\n          for(int j=0;j<fieldDataHash.length;j++) {\n            DocumentsWriterFieldData fp0 = fieldDataHash[j];\n            while(fp0 != null) {\n              hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n              DocumentsWriterFieldData nextFP0 = fp0.next;\n              fp0.next = newHashArray[hashPos];\n              newHashArray[hashPos] = fp0;\n              fp0 = nextFP0;\n            }\n          }\n\n          allFieldDataArray = newArray;\n          fieldDataHash = newHashArray;\n        }\n        allFieldDataArray[numAllFieldData++] = fp;\n      } else {\n        assert fp.fieldInfo == fi;\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.lastGen = thisFieldGen;\n        fp.fieldCount = 0;\n        fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n        fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n        if (numFieldData == fieldDataArray.length) {\n          int newSize = fieldDataArray.length*2;\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n          fieldDataArray = newArray;\n\n        }\n        fieldDataArray[numFieldData++] = fp;\n      }\n\n      if (field.isTermVectorStored()) {\n        if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n          final int newSize = (int) (numVectorFields*1.5);\n          vectorFieldPointers = new long[newSize];\n          vectorFieldNumbers = new int[newSize];\n        }\n        fp.doVectors = true;\n        docHasVectors = true;\n\n        fp.doVectorPositions |= field.isStorePositionWithTermVector();\n        fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n      }\n\n      if (fp.fieldCount == fp.docFields.length) {\n        Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n        System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n        fp.docFields = newArray;\n      }\n\n      // Lazily allocate arrays for postings:\n      if (field.isIndexed() && fp.postingsHash == null)\n        fp.initPostingArrays();\n\n      fp.docFields[fp.fieldCount++] = field;\n    }\n\n    // Maybe init the local & global fieldsWriter\n    if (localFieldsWriter == null) {\n      if (docWriter.fieldsWriter == null) {\n        assert docWriter.docStoreSegment == null;\n        assert docWriter.segment != null;\n        docWriter.docStoreSegment = docWriter.segment;\n        // If we hit an exception while init'ing the\n        // fieldsWriter, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.fieldsWriter = new FieldsWriter(docWriter.directory, docWriter.docStoreSegment, docWriter.fieldInfos);\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n        docWriter.files = null;\n      }\n      localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);\n    }\n\n    // First time we see a doc that has field(s) with\n    // stored vectors, we init our tvx writer\n    if (docHasVectors) {\n      if (docWriter.tvx == null) {\n        assert docWriter.docStoreSegment != null;\n        // If we hit an exception while init'ing the term\n        // vector output files, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);\n          docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);\n          docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);\n\n          // We must \"catch up\" for all docs before us\n          // that had no vectors:\n          for(int i=0;i<docWriter.numDocsInStore;i++) {\n            docWriter.tvx.writeLong(docWriter.tvd.getFilePointer());\n            docWriter.tvd.writeVInt(0);\n            docWriter.tvx.writeLong(0);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n        docWriter.files = null;\n      }\n      numVectorFields = 0;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#init(Document,int).mjava","sourceNew":null,"sourceOld":"  /** Initializes shared state for this new document */\n  void init(Document doc, int docID) throws IOException, AbortException {\n\n    assert !isIdle;\n    assert docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    this.docID = docID;\n    docBoost = doc.getBoost();\n    numStoredFields = 0;\n    numFieldData = 0;\n    numVectorFields = 0;\n    maxTermPrefix = null;\n\n    assert 0 == fdtLocal.length();\n    assert 0 == fdtLocal.getFilePointer();\n    assert 0 == tvfLocal.length();\n    assert 0 == tvfLocal.getFilePointer();\n    final int thisFieldGen = fieldGen++;\n\n    List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n    boolean docHasVectors = false;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n\n      FieldInfo fi = docWriter.fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),\n                                              field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                              field.getOmitNorms(), false);\n      if (fi.isIndexed && !fi.omitNorms) {\n        // Maybe grow our buffered norms\n        if (docWriter.norms.length <= fi.number) {\n          int newSize = (int) ((1+fi.number)*1.25);\n          BufferedNorms[] newNorms = new BufferedNorms[newSize];\n          System.arraycopy(docWriter.norms, 0, newNorms, 0, docWriter.norms.length);\n          docWriter.norms = newNorms;\n        }\n          \n        if (docWriter.norms[fi.number] == null)\n          docWriter.norms[fi.number] = new BufferedNorms();\n\n        docWriter.hasNorms = true;\n      }\n\n      // Make sure we have a FieldData allocated\n      int hashPos = fi.name.hashCode() & fieldDataHashMask;\n      DocumentsWriterFieldData fp = fieldDataHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fi.name))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        fp = new DocumentsWriterFieldData(this, fi);\n        fp.next = fieldDataHash[hashPos];\n        fieldDataHash[hashPos] = fp;\n\n        if (numAllFieldData == allFieldDataArray.length) {\n          int newSize = (int) (allFieldDataArray.length*1.5);\n          int newHashSize = fieldDataHash.length*2;\n\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          DocumentsWriterFieldData newHashArray[] = new DocumentsWriterFieldData[newHashSize];\n          System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);\n\n          // Rehash\n          fieldDataHashMask = newSize-1;\n          for(int j=0;j<fieldDataHash.length;j++) {\n            DocumentsWriterFieldData fp0 = fieldDataHash[j];\n            while(fp0 != null) {\n              hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;\n              DocumentsWriterFieldData nextFP0 = fp0.next;\n              fp0.next = newHashArray[hashPos];\n              newHashArray[hashPos] = fp0;\n              fp0 = nextFP0;\n            }\n          }\n\n          allFieldDataArray = newArray;\n          fieldDataHash = newHashArray;\n        }\n        allFieldDataArray[numAllFieldData++] = fp;\n      } else {\n        assert fp.fieldInfo == fi;\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.lastGen = thisFieldGen;\n        fp.fieldCount = 0;\n        fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;\n        fp.doNorms = fi.isIndexed && !fi.omitNorms;\n\n        if (numFieldData == fieldDataArray.length) {\n          int newSize = fieldDataArray.length*2;\n          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];\n          System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);\n          fieldDataArray = newArray;\n\n        }\n        fieldDataArray[numFieldData++] = fp;\n      }\n\n      if (field.isTermVectorStored()) {\n        if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {\n          final int newSize = (int) (numVectorFields*1.5);\n          vectorFieldPointers = new long[newSize];\n          vectorFieldNumbers = new int[newSize];\n        }\n        fp.doVectors = true;\n        docHasVectors = true;\n\n        fp.doVectorPositions |= field.isStorePositionWithTermVector();\n        fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();\n      }\n\n      if (fp.fieldCount == fp.docFields.length) {\n        Fieldable[] newArray = new Fieldable[fp.docFields.length*2];\n        System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);\n        fp.docFields = newArray;\n      }\n\n      // Lazily allocate arrays for postings:\n      if (field.isIndexed() && fp.postingsHash == null)\n        fp.initPostingArrays();\n\n      fp.docFields[fp.fieldCount++] = field;\n    }\n\n    // Maybe init the local & global fieldsWriter\n    if (localFieldsWriter == null) {\n      if (docWriter.fieldsWriter == null) {\n        assert docWriter.docStoreSegment == null;\n        assert docWriter.segment != null;\n        docWriter.files = null;\n        docWriter.docStoreSegment = docWriter.segment;\n        // If we hit an exception while init'ing the\n        // fieldsWriter, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.fieldsWriter = new FieldsWriter(docWriter.directory, docWriter.docStoreSegment, docWriter.fieldInfos);\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n      }\n      localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);\n    }\n\n    // First time we see a doc that has field(s) with\n    // stored vectors, we init our tvx writer\n    if (docHasVectors) {\n      if (docWriter.tvx == null) {\n        assert docWriter.docStoreSegment != null;\n        docWriter.files = null;\n        // If we hit an exception while init'ing the term\n        // vector output files, we must abort this segment\n        // because those files will be in an unknown\n        // state:\n        try {\n          docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);\n          docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);\n          docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  \".\" + IndexFileNames.VECTORS_FIELDS_EXTENSION);\n          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);\n\n          // We must \"catch up\" for all docs before us\n          // that had no vectors:\n          for(int i=0;i<docWriter.numDocsInStore;i++) {\n            docWriter.tvx.writeLong(docWriter.tvd.getFilePointer());\n            docWriter.tvd.writeVInt(0);\n            docWriter.tvx.writeLong(0);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, docWriter);\n        }\n      }\n      numVectorFields = 0;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5350389bf83287111f7760b9e3db3af8e3648474":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a0af3a442be522899177e5e11384a45a6784a3f"],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["5350389bf83287111f7760b9e3db3af8e3648474"],"5a0af3a442be522899177e5e11384a45a6784a3f":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}