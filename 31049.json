{"path":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","commits":[{"id":"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce","date":1297021734,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","sourceNew":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","sourceNew":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","sourceNew":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","sourceNew":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesToReuse(Analyzer,String,String[],int[],int[],String[],int[]).mjava","sourceNew":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"69e043c521d4e8db770cc140c63f5ef51f03426a":["f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7b91922b55d15444d554721b352861d028eb8278"],"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["7b91922b55d15444d554721b352861d028eb8278"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce":["29ef99d61cda9641b6250bf9567329a6e65f901d","69e043c521d4e8db770cc140c63f5ef51f03426a","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}