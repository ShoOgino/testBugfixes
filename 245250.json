{"path":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77d924c3b8deab5881ed0d996d597a4ea5bbc40a","date":1316977817,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"08970e5b8411182a29412c177eff67ec1110095b","date":1366640815,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c85fa43e6918808743daa7847ba0264373af687f","date":1395166336,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"345612\", null, \"word\", 4, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":["02dba75457528db0b73837ff68f971ecb715ab78"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7043e7411cdd4dbbc872bf9fedd21231168cd5b8","date":1426087141,"type":4,"author":"Ryan McKinley","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ae131ec9cda430231fcefc081a4c4f5d29238ee","date":1426094638,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","date":1465936684,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"128099577578723971decd2fd3c3b0a043ff9855","date":1473703890,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9354b2a0148657f4393d2a4acb438fde7f1d1dad","date":1490673235,"type":5,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79073f40991d761d73bb67bb490d1f562da07e53","date":1490873944,"type":5,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9354b2a0148657f4393d2a4acb438fde7f1d1dad":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"9ae131ec9cda430231fcefc081a4c4f5d29238ee":["7043e7411cdd4dbbc872bf9fedd21231168cd5b8"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"77d924c3b8deab5881ed0d996d597a4ea5bbc40a":["c26f00b574427b55127e869b935845554afde1fa"],"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345":["9ae131ec9cda430231fcefc081a4c4f5d29238ee"],"08970e5b8411182a29412c177eff67ec1110095b":["77d924c3b8deab5881ed0d996d597a4ea5bbc40a"],"89424def13674ea17829b41c5883c54ecc31a132":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","128099577578723971decd2fd3c3b0a043ff9855"],"79073f40991d761d73bb67bb490d1f562da07e53":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7043e7411cdd4dbbc872bf9fedd21231168cd5b8":["c85fa43e6918808743daa7847ba0264373af687f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","89424def13674ea17829b41c5883c54ecc31a132"],"128099577578723971decd2fd3c3b0a043ff9855":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c85fa43e6918808743daa7847ba0264373af687f":["08970e5b8411182a29412c177eff67ec1110095b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9354b2a0148657f4393d2a4acb438fde7f1d1dad"]},"commit2Childs":{"9354b2a0148657f4393d2a4acb438fde7f1d1dad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9ae131ec9cda430231fcefc081a4c4f5d29238ee":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345"],"c26f00b574427b55127e869b935845554afde1fa":["77d924c3b8deab5881ed0d996d597a4ea5bbc40a"],"77d924c3b8deab5881ed0d996d597a4ea5bbc40a":["08970e5b8411182a29412c177eff67ec1110095b"],"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345":["89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","128099577578723971decd2fd3c3b0a043ff9855"],"08970e5b8411182a29412c177eff67ec1110095b":["c85fa43e6918808743daa7847ba0264373af687f"],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"79073f40991d761d73bb67bb490d1f562da07e53":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"7043e7411cdd4dbbc872bf9fedd21231168cd5b8":["9ae131ec9cda430231fcefc081a4c4f5d29238ee"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["9354b2a0148657f4393d2a4acb438fde7f1d1dad","79073f40991d761d73bb67bb490d1f562da07e53"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"128099577578723971decd2fd3c3b0a043ff9855":["89424def13674ea17829b41c5883c54ecc31a132"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"c85fa43e6918808743daa7847ba0264373af687f":["7043e7411cdd4dbbc872bf9fedd21231168cd5b8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["79073f40991d761d73bb67bb490d1f562da07e53","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}