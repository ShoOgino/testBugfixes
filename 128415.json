{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","commits":[{"id":"17c1f75cbc80446a0380e76b64210cdf8e3858d7","date":1359394908,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected, actual);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b","date":1359664357,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected, actual);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"003dc2cb64b06ce8caea28156505feb3c5059ba2","date":1359688932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9faa42f41b6adb98daf009cf99a4ee239189e469","date":1376648738,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField(\"field\")) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d12774649e77a6efccc3502c735c2893b52af6d6","date":1376964312,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField(\"field\")) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9af1110b168971c380ce207b143c211b8301d74","date":1377458956,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    BytesRef bytesRef = new BytesRef();\n    hash.add(bytesRef); // add empty value for the gaps\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"487204058e79506a6ddba0065cfff810bc15b06a","date":1378216488,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    final boolean defaultCodecSupportsDocsWithField = defaultCodecSupportsDocsWithField(\"field\");\n    if (!defaultCodecSupportsDocsWithField) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"820c994610cafbde5892c00924bdf69fffaaf38f","date":1378227060,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    final boolean defaultCodecSupportsDocsWithField = defaultCodecSupportsDocsWithField(\"field\");\n    if (!defaultCodecSupportsDocsWithField) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = _TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<String, String>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    BytesRef actual = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      docValues.lookupOrd(i, actual);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues.get(docId, actual);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad2a673349939e48652bf304cccf673c3412198f","date":1409585169,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!defaultCodecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["820c994610cafbde5892c00924bdf69fffaaf38f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termDocsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.termDocsEnum(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78cf5dd35bc9be11f14b0c5b5806b05d3f529f91","date":1456586780,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2638813028b254a88b418ebeafb541afb49653","date":1456804822,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = slowR.postings(new Term(\"id\", entry.getKey()));\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87","e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues = MultiDocValues.getSortedValues(reader, \"field\");\n      assertEquals(docId, docValues.advance(docId));\n      final BytesRef actual = docValues.binaryValue();\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues = MultiDocValues.getSortedValues(reader, \"field\");\n      assertEquals(docId, docValues.advance(docId));\n      final BytesRef actual = docValues.binaryValue();\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testRandomSortedBytes().mjava","sourceNew":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      docValues = MultiDocValues.getSortedValues(reader, \"field\");\n      assertEquals(docId, docValues.advance(docId));\n      final BytesRef actual = docValues.binaryValue();\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomSortedBytes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      cfg.setMergePolicy(newLogMergePolicy());\n    }\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);\n    int numDocs = atLeast(100);\n    BytesRefHash hash = new BytesRefHash();\n    Map<String, String> docToString = new HashMap<>();\n    int maxLength = TestUtil.nextInt(random(), 1, 50);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"\" + i, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      hash.add(br);\n      docToString.put(\"\" + i, string);\n      w.addDocument(doc);\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    int numDocsNoValue = atLeast(10);\n    for (int i = 0; i < numDocsNoValue; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"id\", \"noValue\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    if (!codecSupportsDocsWithField()) {\n      BytesRef bytesRef = new BytesRef();\n      hash.add(bytesRef); // add empty value for the gaps\n    }\n    if (rarely()) {\n      w.commit();\n    }\n    if (!codecSupportsDocsWithField()) {\n      // if the codec doesnt support missing, we expect missing to be mapped to byte[]\n      // by the impersonator, but we have to give it a chance to merge them to this\n      w.forceMerge(1);\n    }\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      String id = \"\" + i + numDocs;\n      doc.add(newTextField(\"id\", id, Field.Store.YES));\n      String string = TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);\n      BytesRef br = new BytesRef(string);\n      hash.add(br);\n      docToString.put(id, string);\n      doc.add(new SortedDocValuesField(\"field\", br));\n      w.addDocument(doc);\n    }\n    w.commit();\n    IndexReader reader = w.getReader();\n    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, \"field\");\n    int[] sort = hash.sort();\n    BytesRef expected = new BytesRef();\n    assertEquals(hash.size(), docValues.getValueCount());\n    for (int i = 0; i < hash.size(); i++) {\n      hash.get(sort[i], expected);\n      final BytesRef actual = docValues.lookupOrd(i);\n      assertEquals(expected.utf8ToString(), actual.utf8ToString());\n      int ord = docValues.lookupTerm(expected);\n      assertEquals(i, ord);\n    }\n    Set<Entry<String, String>> entrySet = docToString.entrySet();\n\n    for (Entry<String, String> entry : entrySet) {\n      // pk lookup\n      PostingsEnum termPostingsEnum = TestUtil.docs(random(), reader, \"id\", new BytesRef(entry.getKey()), null, 0);\n      int docId = termPostingsEnum.nextDoc();\n      expected = new BytesRef(entry.getValue());\n      final BytesRef actual = docValues.get(docId);\n      assertEquals(expected, actual);\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"af2638813028b254a88b418ebeafb541afb49653":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4","78cf5dd35bc9be11f14b0c5b5806b05d3f529f91"],"17c1f75cbc80446a0380e76b64210cdf8e3858d7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["17c1f75cbc80446a0380e76b64210cdf8e3858d7"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["af2638813028b254a88b418ebeafb541afb49653"],"487204058e79506a6ddba0065cfff810bc15b06a":["c9af1110b168971c380ce207b143c211b8301d74"],"c9af1110b168971c380ce207b143c211b8301d74":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"78cf5dd35bc9be11f14b0c5b5806b05d3f529f91":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"820c994610cafbde5892c00924bdf69fffaaf38f":["487204058e79506a6ddba0065cfff810bc15b06a"],"003dc2cb64b06ce8caea28156505feb3c5059ba2":["3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"9faa42f41b6adb98daf009cf99a4ee239189e469":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["820c994610cafbde5892c00924bdf69fffaaf38f"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["d4d69c535930b5cce125cff868d40f6373dc27d4","c9af1110b168971c380ce207b143c211b8301d74"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["d4d69c535930b5cce125cff868d40f6373dc27d4","d12774649e77a6efccc3502c735c2893b52af6d6"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["d0ef034a4f10871667ae75181537775ddcf8ade4","ad2a673349939e48652bf304cccf673c3412198f"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","003dc2cb64b06ce8caea28156505feb3c5059ba2"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d12774649e77a6efccc3502c735c2893b52af6d6":["9faa42f41b6adb98daf009cf99a4ee239189e469"],"ad2a673349939e48652bf304cccf673c3412198f":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"af2638813028b254a88b418ebeafb541afb49653":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"17c1f75cbc80446a0380e76b64210cdf8e3858d7":["3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["003dc2cb64b06ce8caea28156505feb3c5059ba2"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"487204058e79506a6ddba0065cfff810bc15b06a":["820c994610cafbde5892c00924bdf69fffaaf38f"],"78cf5dd35bc9be11f14b0c5b5806b05d3f529f91":["af2638813028b254a88b418ebeafb541afb49653"],"c9af1110b168971c380ce207b143c211b8301d74":["487204058e79506a6ddba0065cfff810bc15b06a","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["17c1f75cbc80446a0380e76b64210cdf8e3858d7","d4d69c535930b5cce125cff868d40f6373dc27d4"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"820c994610cafbde5892c00924bdf69fffaaf38f":["6613659748fe4411a7dcf85266e55db1f95f7315"],"003dc2cb64b06ce8caea28156505feb3c5059ba2":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"9faa42f41b6adb98daf009cf99a4ee239189e469":["d12774649e77a6efccc3502c735c2893b52af6d6"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["c9af1110b168971c380ce207b143c211b8301d74"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["af2638813028b254a88b418ebeafb541afb49653","78cf5dd35bc9be11f14b0c5b5806b05d3f529f91"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["9faa42f41b6adb98daf009cf99a4ee239189e469","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["402ad3ddc9da7b70da1b167667a60ece6a1381fb","ad2a673349939e48652bf304cccf673c3412198f"],"d12774649e77a6efccc3502c735c2893b52af6d6":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"ad2a673349939e48652bf304cccf673c3412198f":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}