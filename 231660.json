{"path":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","commits":[{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":1,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cc45c615dbb82bf79d5f9550286098367874fbf","date":1409571423,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":["c024a3e8fec0a081cbf9539845db12f0dc84d029","6613659748fe4411a7dcf85266e55db1f95f7315"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7249bf663236743660155abfc0941aebf4245391","date":1433953380,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1862266772deb28cdcb7d996b64d2177022687","date":1453077824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","sourceNew":null,"sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = LARGE_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = LARGE_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w);\n    w.close();\n\n    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(ar\n                         );\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      Document doc = ar.document(docID);\n      BytesRef bytes = s.get(docID);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["2a1862266772deb28cdcb7d996b64d2177022687"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["2a1862266772deb28cdcb7d996b64d2177022687","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"83870855d82aba6819217abeff5a40779dbb28b4":["2a1862266772deb28cdcb7d996b64d2177022687","0e121d43b5a10f2df530f406f935102656e9c4e8"],"56572ec06f1407c066d6b7399413178b33176cd8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","93dd449115a9247533e44bab47e8429e5dccbc6d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["2a1862266772deb28cdcb7d996b64d2177022687","0e121d43b5a10f2df530f406f935102656e9c4e8"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["d0ef034a4f10871667ae75181537775ddcf8ade4","4cc45c615dbb82bf79d5f9550286098367874fbf"],"2a1862266772deb28cdcb7d996b64d2177022687":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"7249bf663236743660155abfc0941aebf4245391":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4cc45c615dbb82bf79d5f9550286098367874fbf":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["7249bf663236743660155abfc0941aebf4245391"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"56572ec06f1407c066d6b7399413178b33176cd8":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7249bf663236743660155abfc0941aebf4245391"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"2a1862266772deb28cdcb7d996b64d2177022687":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","56572ec06f1407c066d6b7399413178b33176cd8"],"7249bf663236743660155abfc0941aebf4245391":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"4cc45c615dbb82bf79d5f9550286098367874fbf":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["402ad3ddc9da7b70da1b167667a60ece6a1381fb","4cc45c615dbb82bf79d5f9550286098367874fbf"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["2a1862266772deb28cdcb7d996b64d2177022687"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","56572ec06f1407c066d6b7399413178b33176cd8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}