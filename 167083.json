{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","commits":[{"id":"e8176b5c0894f97addb4b77198ec5684476b1b32","date":1365103218,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int).mjava","sourceNew":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new SorterTemplate() {\n      String pivot;\n      \n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n\n      @Override\n      protected void setPivot(int i) {\n        pivot = fields[i];\n      }\n\n      @Override\n      protected int comparePivot(int j) {\n        return pivot.compareTo(fields[j]);\n      }\n      \n    }.mergeSort(0, fields.length-1);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();;\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fields field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassages The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copy because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    Arrays.sort(fields);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, maxPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"116a468ed771d87fd94eb1350dd2d42bbf0b262f","date":1365791134,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new SorterTemplate() {\n      String pivot;\n      \n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n\n      @Override\n      protected void setPivot(int i) {\n        pivot = fields[i];\n      }\n\n      @Override\n      protected int comparePivot(int j) {\n        return pivot.compareTo(fields[j]);\n      }\n      \n    }.mergeSort(0, fields.length-1);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new SorterTemplate() {\n      String pivot;\n      \n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n\n      @Override\n      protected void setPivot(int i) {\n        pivot = fields[i];\n      }\n\n      @Override\n      protected int comparePivot(int j) {\n        return pivot.compareTo(fields[j]);\n      }\n      \n    }.mergeSort(0, fields.length-1);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();;\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dcc555744b1a581a4beccd0b75f8d3fe49735a2f","date":1367588265,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new InPlaceMergeSorter() {\n\n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n      \n    }.sort(0, fields.length);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new SorterTemplate() {\n      String pivot;\n      \n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n\n      @Override\n      protected void setPivot(int i) {\n        pivot = fields[i];\n      }\n\n      @Override\n      protected int comparePivot(int j) {\n        return pivot.compareTo(fields[j]);\n      }\n      \n    }.mergeSort(0, fields.length-1);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b93d3a9b49519a58eab214c690c3df946003b9b5","date":1379000095,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>docidsIn</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    Map<String,String[]> snippets = new HashMap<String,String[]>();\n    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {\n      Object[] snippetObjects = ent.getValue();\n      String[] snippetStrings = new String[snippetObjects.length];\n      snippets.put(ent.getKey(), snippetStrings);\n      for(int i=0;i<snippetObjects.length;i++) {\n        Object snippet = snippetObjects[i];\n        if (snippet != null) {\n          snippetStrings[i] = snippet.toString();\n        }\n      }\n    }\n\n    return snippets;\n  }\n\n","sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>topDocs</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    final IndexReader reader = searcher.getIndexReader();\n    query = rewrite(query);\n    SortedSet<Term> queryTerms = new TreeSet<Term>();\n    query.extractTerms(queryTerms);\n\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n\n    // Make our own copies because we sort in-place:\n    int[] docids = new int[docidsIn.length];\n    System.arraycopy(docidsIn, 0, docids, 0, docidsIn.length);\n    final String fields[] = new String[fieldsIn.length];\n    System.arraycopy(fieldsIn, 0, fields, 0, fieldsIn.length);\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    System.arraycopy(maxPassagesIn, 0, maxPassages, 0, maxPassagesIn.length);\n\n    // sort for sequential io\n    Arrays.sort(docids);\n    new InPlaceMergeSorter() {\n\n      @Override\n      protected void swap(int i, int j) {\n        String tmp = fields[i];\n        fields[i] = fields[j];\n        fields[j] = tmp;\n        int tmp2 = maxPassages[i];\n        maxPassages[i] = maxPassages[j];\n        maxPassages[j] = tmp2;\n      }\n\n      @Override\n      protected int compare(int i, int j) {\n        return fields[i].compareTo(fields[j]);\n      }\n      \n    }.sort(0, fields.length);\n    \n    // pull stored data:\n    String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);\n    \n    Map<String,String[]> highlights = new HashMap<String,String[]>();\n    for (int i = 0; i < fields.length; i++) {\n      String field = fields[i];\n      int numPassages = maxPassages[i];\n      Term floor = new Term(field, \"\");\n      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);\n      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);\n      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n      // Strip off the redundant field:\n      BytesRef terms[] = new BytesRef[fieldTerms.size()];\n      int termUpto = 0;\n      for(Term term : fieldTerms) {\n        terms[termUpto++] = term.bytes();\n      }\n      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages);\n        \n      String[] result = new String[docids.length];\n      for (int j = 0; j < docidsIn.length; j++) {\n        result[j] = fieldHighlights.get(docidsIn[j]);\n      }\n      highlights.put(field, result);\n    }\n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>docidsIn</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    Map<String,String[]> snippets = new HashMap<>();\n    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {\n      Object[] snippetObjects = ent.getValue();\n      String[] snippetStrings = new String[snippetObjects.length];\n      snippets.put(ent.getKey(), snippetStrings);\n      for(int i=0;i<snippetObjects.length;i++) {\n        Object snippet = snippetObjects[i];\n        if (snippet != null) {\n          snippetStrings[i] = snippet.toString();\n        }\n      }\n    }\n\n    return snippets;\n  }\n\n","sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>docidsIn</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    Map<String,String[]> snippets = new HashMap<String,String[]>();\n    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {\n      Object[] snippetObjects = ent.getValue();\n      String[] snippetStrings = new String[snippetObjects.length];\n      snippets.put(ent.getKey(), snippetStrings);\n      for(int i=0;i<snippetObjects.length;i++) {\n        Object snippet = snippetObjects[i];\n        if (snippet != null) {\n          snippetStrings[i] = snippet.toString();\n        }\n      }\n    }\n\n    return snippets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"381618eac2691bb34ab9a3fca76ad55c6274517e","date":1495564791,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>docidsIn</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    Map<String,String[]> snippets = new HashMap<>();\n    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {\n      Object[] snippetObjects = ent.getValue();\n      String[] snippetStrings = new String[snippetObjects.length];\n      snippets.put(ent.getKey(), snippetStrings);\n      for(int i=0;i<snippetObjects.length;i++) {\n        Object snippet = snippetObjects[i];\n        if (snippet != null) {\n          snippetStrings[i] = snippet.toString();\n        }\n      }\n    }\n\n    return snippets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightFields(String[],Query,IndexSearcher,int[],int[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Highlights the top-N passages from multiple fields,\n   * for the provided int[] docids.\n   * \n   * @param fieldsIn field names to highlight. \n   *        Must have a stored string value and also be indexed with offsets.\n   * @param query query to highlight.\n   * @param searcher searcher that was previously used to execute the query.\n   * @param docidsIn containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to \n   *        form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets \n   *         corresponding to the documents in <code>docidsIn</code>. \n   *         If no highlights were found for a document, the\n   *         first {@code maxPassages} from the field will\n   *         be returned.\n   * @throws IOException if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without \n   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {\n    Map<String,String[]> snippets = new HashMap<>();\n    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {\n      Object[] snippetObjects = ent.getValue();\n      String[] snippetStrings = new String[snippetObjects.length];\n      snippets.put(ent.getKey(), snippetStrings);\n      for(int i=0;i<snippetObjects.length;i++) {\n        Object snippet = snippetObjects[i];\n        if (snippet != null) {\n          snippetStrings[i] = snippet.toString();\n        }\n      }\n    }\n\n    return snippets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["b93d3a9b49519a58eab214c690c3df946003b9b5"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","381618eac2691bb34ab9a3fca76ad55c6274517e"],"e8176b5c0894f97addb4b77198ec5684476b1b32":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b93d3a9b49519a58eab214c690c3df946003b9b5":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"116a468ed771d87fd94eb1350dd2d42bbf0b262f":["e8176b5c0894f97addb4b77198ec5684476b1b32"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["116a468ed771d87fd94eb1350dd2d42bbf0b262f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["381618eac2691bb34ab9a3fca76ad55c6274517e"]},"commit2Childs":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["381618eac2691bb34ab9a3fca76ad55c6274517e","e9017cf144952056066919f1ebc7897ff9bd71b1"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"e8176b5c0894f97addb4b77198ec5684476b1b32":["116a468ed771d87fd94eb1350dd2d42bbf0b262f"],"b93d3a9b49519a58eab214c690c3df946003b9b5":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e8176b5c0894f97addb4b77198ec5684476b1b32"],"116a468ed771d87fd94eb1350dd2d42bbf0b262f":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["b93d3a9b49519a58eab214c690c3df946003b9b5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}