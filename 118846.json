{"path":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","commits":[{"id":"e7a005111928c661ab5d236ed6a3a079b438d2cf","date":1205411670,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().isIndexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document infomation\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be chached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i belive this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7","date":1214673815,"type":3,"author":"Karl-Johan Wettin","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document infomation\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be chached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i belive this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().isIndexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document infomation\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be chached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i belive this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add7d922e63099fbce8f0a1b31216df7ef5067f1","date":1252002701,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document infomation\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be chached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i belive this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0eb5b0b5b98c777dad412afbfb347d2c0889327","date":1259094367,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, DefaultSimilarity.encodeNorm(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = Similarity.encodeNorm(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c41356c8a19fd7493940c7a1d798ede2fe03ddf8","date":1260481087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c41356c8a19fd7493940c7a1d798ede2fe03ddf8":["b0eb5b0b5b98c777dad412afbfb347d2c0889327"],"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e7a005111928c661ab5d236ed6a3a079b438d2cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b0eb5b0b5b98c777dad412afbfb347d2c0889327":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["c41356c8a19fd7493940c7a1d798ede2fe03ddf8"]},"commit2Childs":{"c41356c8a19fd7493940c7a1d798ede2fe03ddf8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["b0eb5b0b5b98c777dad412afbfb347d2c0889327"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"e7a005111928c661ab5d236ed6a3a079b438d2cf":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"b0eb5b0b5b98c777dad412afbfb347d2c0889327":["c41356c8a19fd7493940c7a1d798ede2fe03ddf8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}