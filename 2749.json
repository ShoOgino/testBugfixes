{"path":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"937923083e4d137932336fc80f3d78758ff698a6","date":1454691519,"type":1,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"62cc423073d23f01208c6cf85844dedd80011121","date":1454731549,"type":3,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":["0f01ec580f0443437a320e1e34902e33f38c5720"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f01ec580f0443437a320e1e34902e33f38c5720","date":1455049146,"type":3,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":["7a02003eb48495b52b8483703e1b6b48c099ec7d","62cc423073d23f01208c6cf85844dedd80011121","937923083e4d137932336fc80f3d78758ff698a6","404bbb5c1692276fefc358d0d4a9ccb74ed2518e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c64189697927b548f74ba66dfa5051548662938","date":1455059336,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a093d23e938d132b81b5f2de3d6b168afe3608e","date":1455076308,"type":5,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int docId;\n            while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              builder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0f01ec580f0443437a320e1e34902e33f38c5720":["5a207d19eac354d649c3f0e2cce070017c78125e"],"5a207d19eac354d649c3f0e2cce070017c78125e":["937923083e4d137932336fc80f3d78758ff698a6","62cc423073d23f01208c6cf85844dedd80011121"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","62cc423073d23f01208c6cf85844dedd80011121"],"62cc423073d23f01208c6cf85844dedd80011121":["937923083e4d137932336fc80f3d78758ff698a6"],"8a093d23e938d132b81b5f2de3d6b168afe3608e":["3c64189697927b548f74ba66dfa5051548662938"],"3c64189697927b548f74ba66dfa5051548662938":["5a207d19eac354d649c3f0e2cce070017c78125e","0f01ec580f0443437a320e1e34902e33f38c5720"],"937923083e4d137932336fc80f3d78758ff698a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a093d23e938d132b81b5f2de3d6b168afe3608e"]},"commit2Childs":{"0f01ec580f0443437a320e1e34902e33f38c5720":["3c64189697927b548f74ba66dfa5051548662938"],"5a207d19eac354d649c3f0e2cce070017c78125e":["0f01ec580f0443437a320e1e34902e33f38c5720","3c64189697927b548f74ba66dfa5051548662938"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","937923083e4d137932336fc80f3d78758ff698a6"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"62cc423073d23f01208c6cf85844dedd80011121":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"8a093d23e938d132b81b5f2de3d6b168afe3608e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3c64189697927b548f74ba66dfa5051548662938":["8a093d23e938d132b81b5f2de3d6b168afe3608e"],"937923083e4d137932336fc80f3d78758ff698a6":["5a207d19eac354d649c3f0e2cce070017c78125e","62cc423073d23f01208c6cf85844dedd80011121"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}