{"path":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (fieldInfos.hasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b9dc373b96fc96e6300e2f5af947f6998e6aa6a6","date":1295759448,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        // nocommit: this can be simplified to always be si.getDocStoreSegment()\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58b67e54678c8a78d52529a01063552ec8bd0f66","date":1295781102,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        // nocommit: this can be simplified to always be si.getDocStoreSegment()\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment;\n        if (si.getDocStoreOffset() != -1) {\n          storesSegment = si.getDocStoreSegment();\n        } else {\n          storesSegment = segment;\n        }\n\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe19cbe25754c715a0232f453039383119fc122c","date":1306110991,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":null,"sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae132b768aece5bf21cda14e2f17fba66eb6f7d6","date":1306128032,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":null,"sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentReader.CoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":null,"sourceOld":"    synchronized void openDocStores(SegmentInfo si) throws IOException {\n\n      assert si.name.equals(segment);\n\n      if (fieldsReaderOrig == null) {\n        final Directory storeDir;\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            assert storeCFSReader == null;\n            storeCFSReader = new CompoundFileReader(dir,\n                IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n                                                    readBufferSize);\n            storeDir = storeCFSReader;\n            assert storeDir != null;\n          } else {\n            storeDir = dir;\n            assert storeDir != null;\n          }\n        } else if (si.getUseCompoundFile()) {\n          // In some cases, we were originally opened when CFS\n          // was not used, but then we are asked to open doc\n          // stores after the segment has switched to CFS\n          if (cfsReader == null) {\n            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n          }\n          storeDir = cfsReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n\n        final String storesSegment = si.getDocStoreSegment();\n        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n\n        if (si.getHasVectors()) { // open term vector files only as needed\n          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"58b67e54678c8a78d52529a01063552ec8bd0f66":["b9dc373b96fc96e6300e2f5af947f6998e6aa6a6"],"b9dc373b96fc96e6300e2f5af947f6998e6aa6a6":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":["58b67e54678c8a78d52529a01063552ec8bd0f66","fe19cbe25754c715a0232f453039383119fc122c"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["29ef99d61cda9641b6250bf9567329a6e65f901d","fe19cbe25754c715a0232f453039383119fc122c"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","58b67e54678c8a78d52529a01063552ec8bd0f66"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","58b67e54678c8a78d52529a01063552ec8bd0f66"],"fe19cbe25754c715a0232f453039383119fc122c":["58b67e54678c8a78d52529a01063552ec8bd0f66"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fe19cbe25754c715a0232f453039383119fc122c"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"58b67e54678c8a78d52529a01063552ec8bd0f66":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","bb9b72f7c3d7827c64dd4ec580ded81778da361d","29ef99d61cda9641b6250bf9567329a6e65f901d","fe19cbe25754c715a0232f453039383119fc122c"],"b9dc373b96fc96e6300e2f5af947f6998e6aa6a6":["58b67e54678c8a78d52529a01063552ec8bd0f66"],"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":[],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"fe19cbe25754c715a0232f453039383119fc122c":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["b9dc373b96fc96e6300e2f5af947f6998e6aa6a6","ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","bb9b72f7c3d7827c64dd4ec580ded81778da361d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}