{"path":"lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"689f35bd9818b47b8d9fe96cf06518228e949ab6","date":1272894884,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"689f35bd9818b47b8d9fe96cf06518228e949ab6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["689f35bd9818b47b8d9fe96cf06518228e949ab6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"689f35bd9818b47b8d9fe96cf06518228e949ab6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["689f35bd9818b47b8d9fe96cf06518228e949ab6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}