{"path":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","commits":[{"id":"0f41068c0ab05fda1c7fe6e4bb799e00d1414668","date":1400030748,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"/dev/null","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return res;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08548ff681bfed3d72bbf3d17c2b122c5d6d7c77","date":1400032658,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.EMPTY_SORTED_SET;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","sourceOld":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    final SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      return res;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"/dev/null","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.EMPTY_SORTED_SET;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"/dev/null","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.EMPTY_SORTED_SET;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.emptySortedSet();\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySortedSet();\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySorted();\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        final BytesRef value = si.lookupOrd(ord);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","sourceOld":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.EMPTY_SORTED_SET;\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED_SET;\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.EMPTY_SORTED;\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    BytesRef value = new BytesRef();\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        si.lookupOrd(ord, value);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"10eb287e62f3e48c07b2a817c1750c859bb5e7e7","date":1410374187,"type":3,"author":"Tomas Eduardo Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n       \n    for (String facetField : facet) {\n      SchemaField fsf = searcher.getSchema().getField(facetField);\n      if ( fsf.multiValued()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Stats can only facet on single-valued fields, not: \" + facetField );\n      }\n      \n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      \n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.emptySortedSet();\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n    \n    int missingDocCountTotal = 0;\n    final int nTerms = (int) si.getValueCount();    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    \n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      \n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        \n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySortedSet();\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            missingDocCountTotal += accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            missingDocCountTotal += accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySorted();\n          }\n          missingDocCountTotal += accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    // add results in index order\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n\n      if (count > 0) {\n        final BytesRef value = si.lookupOrd(ord);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n    res.addMissing(missingDocCountTotal);\n    \n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String,StatsValues> facetStatsValues = f.facetStatsValues;\n        f.accumulateMissing();\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n    for (String facetField : facet) {\n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    \n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.emptySortedSet();\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));\n\n    final int nTerms = (int) si.getValueCount();   \n    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySortedSet();\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySorted();\n          }\n          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    \n    // add results in index order\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n      if (count > 0) {\n        final BytesRef value = si.lookupOrd(ord);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n\n    res.addMissing(missing.size());\n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"283ff02f401ec3e7a2fad73643970f052383fb0c","date":1411407953,"type":5,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,StatsField,DocSet,String[]).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/DocValuesStats#getCounts(SolrIndexSearcher,String,DocSet,boolean,String[]).mjava","sourceNew":"  public static StatsValues getCounts(SolrIndexSearcher searcher, StatsField statsField, DocSet docs, String[] facet) throws IOException {\n\n    final SchemaField schemaField = statsField.getSchemaField(); \n    final boolean calcDistinct = statsField.getCalcDistinct();\n\n    assert null != statsField.getSchemaField()\n      : \"DocValuesStats requires a StatsField using a SchemaField\";\n\n    final String fieldName = schemaField.getName();\n    final FieldType ft = schemaField.getType();\n    final StatsValues res = StatsValuesFactory.createStatsValues(statsField);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n       \n    for (String facetField : facet) {\n      SchemaField fsf = searcher.getSchema().getField(facetField);\n      if ( fsf.multiValued()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Stats can only facet on single-valued fields, not: \" + facetField );\n      }\n      \n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetSchemaField, statsField);\n    }\n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      \n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.emptySortedSet();\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n    \n    int missingDocCountTotal = 0;\n    final int nTerms = (int) si.getValueCount();    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    \n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      \n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        \n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySortedSet();\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            missingDocCountTotal += accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            missingDocCountTotal += accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySorted();\n          }\n          missingDocCountTotal += accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    // add results in index order\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n\n      if (count > 0) {\n        final BytesRef value = si.lookupOrd(ord);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n    res.addMissing(missingDocCountTotal);\n    \n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String,StatsValues> facetStatsValues = f.facetStatsValues;\n        f.accumulateMissing();\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {\n    SchemaField schemaField = searcher.getSchema().getField(fieldName);\n    FieldType ft = schemaField.getType();\n    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);\n    \n    //Initialize facetstats, if facets have been passed in\n    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];\n    int upto = 0;\n       \n    for (String facetField : facet) {\n      SchemaField fsf = searcher.getSchema().getField(facetField);\n      if ( fsf.multiValued()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Stats can only facet on single-valued fields, not: \" + facetField );\n      }\n      \n      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);\n      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);\n    }\n    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?\n    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();\n\n    SortedSetDocValues si; // for term lookups only\n    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones\n    if (multiValued) {\n      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);\n      \n      if (si instanceof MultiSortedSetDocValues) {\n        ordinalMap = ((MultiSortedSetDocValues)si).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);\n      si = single == null ? null : DocValues.singleton(single);\n      if (single instanceof MultiSortedDocValues) {\n        ordinalMap = ((MultiSortedDocValues)single).mapping;\n      }\n    }\n    if (si == null) {\n      si = DocValues.emptySortedSet();\n    }\n    if (si.getValueCount() >= Integer.MAX_VALUE) {\n      throw new UnsupportedOperationException(\"Currently this stats method is limited to \" + Integer.MAX_VALUE + \" unique terms\");\n    }\n    \n    int missingDocCountTotal = 0;\n    final int nTerms = (int) si.getValueCount();    \n    // count collection array only needs to be as big as the number of terms we are\n    // going to collect counts for.\n    final int[] counts = new int[nTerms];\n    \n    Filter filter = docs.getTopFilter();\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    \n    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {\n      AtomicReaderContext leaf = leaves.get(subIndex);\n      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs\n      DocIdSetIterator disi = null;\n      \n      if (dis != null) {\n        disi = dis.iterator();\n      }\n      if (disi != null) {\n        int docBase = leaf.docBase;\n        \n        if (multiValued) {\n          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySortedSet();\n          }\n          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);\n          if (singleton != null) {\n            // some codecs may optimize SORTED_SET storage for single-valued fields\n            missingDocCountTotal += accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);\n          } else {\n            missingDocCountTotal += accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n          }\n        } else {\n          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);\n          if (sub == null) {\n            sub = DocValues.emptySorted();\n          }\n          missingDocCountTotal += accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);\n        }\n      }\n    }\n    // add results in index order\n    for (int ord = 0; ord < counts.length; ord++) {\n      int count = counts[ord];\n\n      if (count > 0) {\n        final BytesRef value = si.lookupOrd(ord);\n        res.accumulate(value, count);\n        for (FieldFacetStats f : facetStats) {\n          f.accumulateTermNum(ord, value);\n        }\n      }\n    }\n    res.addMissing(missingDocCountTotal);\n    \n    if (facetStats.length > 0) {\n      for (FieldFacetStats f : facetStats) {\n        Map<String,StatsValues> facetStatsValues = f.facetStatsValues;\n        f.accumulateMissing();\n        res.addFacet(f.name, facetStatsValues);\n      }\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"283ff02f401ec3e7a2fad73643970f052383fb0c":["10eb287e62f3e48c07b2a817c1750c859bb5e7e7"],"10eb287e62f3e48c07b2a817c1750c859bb5e7e7":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"08548ff681bfed3d72bbf3d17c2b122c5d6d7c77":["0f41068c0ab05fda1c7fe6e4bb799e00d1414668"],"0f41068c0ab05fda1c7fe6e4bb799e00d1414668":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","08548ff681bfed3d72bbf3d17c2b122c5d6d7c77"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"56572ec06f1407c066d6b7399413178b33176cd8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","93dd449115a9247533e44bab47e8429e5dccbc6d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["283ff02f401ec3e7a2fad73643970f052383fb0c"]},"commit2Childs":{"283ff02f401ec3e7a2fad73643970f052383fb0c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"10eb287e62f3e48c07b2a817c1750c859bb5e7e7":["283ff02f401ec3e7a2fad73643970f052383fb0c"],"08548ff681bfed3d72bbf3d17c2b122c5d6d7c77":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"0f41068c0ab05fda1c7fe6e4bb799e00d1414668":["08548ff681bfed3d72bbf3d17c2b122c5d6d7c77"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","56572ec06f1407c066d6b7399413178b33176cd8"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["10eb287e62f3e48c07b2a817c1750c859bb5e7e7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f41068c0ab05fda1c7fe6e4bb799e00d1414668","93dd449115a9247533e44bab47e8429e5dccbc6d","56572ec06f1407c066d6b7399413178b33176cd8"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}