{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","commits":[{"id":"95323da8eca89d45766013f5b300a865a5ac7dfb","date":1348933777,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, Codec.getDefault(), null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = Codec.getDefault().postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return Codec.getDefault().postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afe38d4771eae27550f75f664e7094cfb7d1e2f2","date":1348936511,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29baaefef1b62d76a3370ff72a0fe5f9bd84e365","date":1348949582,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["afe38d4771eae27550f75f664e7094cfb7d1e2f2"],"afe38d4771eae27550f75f664e7094cfb7d1e2f2":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"]},"commit2Childs":{"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"afe38d4771eae27550f75f664e7094cfb7d1e2f2":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["afe38d4771eae27550f75f664e7094cfb7d1e2f2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}