{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","commits":[{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":1,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":1,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06555ac714e8cc3a237853c80f762cfabc07a509","date":1398998920,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0efcf27cd5ca23def8376b4c321970c14dd71623","date":1400662679,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n\n    // TODO: we could set liveDocs earlier and then fix DVs to also not write deleted docs:\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca044bd4a0bea8391cbebb44a34aba53a9b50541","date":1400784655,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n\n    // TODO: we could set liveDocs earlier and then fix DVs to also not write deleted docs:\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","date":1411653326,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f6bd27530a2846413fe2d00030493c0e2d3a072","date":1411811855,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":["52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2fe60a17a7a0cfd101b1169acf089221bc6c166","date":1412767493,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9299079153fd7895bf3cf6835cf7019af2ba89b3","date":1417813477,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // its possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    writeDimensionalValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cab7a79353f33d1a94cd307bf33aa5148601ebe6","date":1453391888,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    writePoints(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    writeDimensionalValues(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"62ad8b8adb87da9e297f35a6ef432258c8076c96","date":1454429256,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    writePoints(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    writeNorms(state);\n    writeDocValues(state);\n    writePoints(state);\n    \n    // it's possible all docs hit non-aborting exceptions...\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n  }\n\n","bugFix":["b0267c69e2456a3477a1ad785723f2135da3117e","06555ac714e8cc3a237853c80f762cfabc07a509","9299079153fd7895bf3cf6835cf7019af2ba89b3","cab7a79353f33d1a94cd307bf33aa5148601ebe6","52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    initStoredFieldsWriter();\n    fillStoredFields(maxDoc);\n    storedFieldsWriter.finish(state.fieldInfos, maxDoc);\n    storedFieldsWriter.close();\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"622a708571e534680618b3c5e0c28ac539a47776","date":1517406892,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException, AbortingException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix, Collections.emptyMap());\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix, Collections.emptyMap());\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, true, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":5,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["622a708571e534680618b3c5e0c28ac539a47776"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["49f1924bd448393fbdfef8b5ebed799f938169d3"],"0efcf27cd5ca23def8376b4c321970c14dd71623":["06555ac714e8cc3a237853c80f762cfabc07a509"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"0dcf8f79417865e5028d753e669fae06457e8369":["f97270426d92300e08ac1bd1a4ef499ae02e88b7","49f1924bd448393fbdfef8b5ebed799f938169d3"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["06555ac714e8cc3a237853c80f762cfabc07a509","c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","b0267c69e2456a3477a1ad785723f2135da3117e"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"680b6449f09827f58fe987aff279e014c311d966":["0dcf8f79417865e5028d753e669fae06457e8369","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["62ad8b8adb87da9e297f35a6ef432258c8076c96","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"b06445ae1731e049327712db0454e5643ca9b7fe":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","b0267c69e2456a3477a1ad785723f2135da3117e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"622a708571e534680618b3c5e0c28ac539a47776":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"55980207f1977bd1463465de1659b821347e2fa8":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"ca044bd4a0bea8391cbebb44a34aba53a9b50541":["0efcf27cd5ca23def8376b4c321970c14dd71623"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3394716f52b34ab259ad5247e7595d9f9db6e935"],"b0267c69e2456a3477a1ad785723f2135da3117e":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["cab7a79353f33d1a94cd307bf33aa5148601ebe6","62ad8b8adb87da9e297f35a6ef432258c8076c96"],"06555ac714e8cc3a237853c80f762cfabc07a509":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","52c7e49be259508735752fba88085255014a6ecf"],"62ad8b8adb87da9e297f35a6ef432258c8076c96":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["b0267c69e2456a3477a1ad785723f2135da3117e"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["bec68e7c41fed133827595747d853cad504e481e"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["62ad8b8adb87da9e297f35a6ef432258c8076c96"],"52c7e49be259508735752fba88085255014a6ecf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["06555ac714e8cc3a237853c80f762cfabc07a509"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["7a6f8af01d9b3067b143bbdc0a492720e2af97cf","0dcf8f79417865e5028d753e669fae06457e8369"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"0efcf27cd5ca23def8376b4c321970c14dd71623":["ca044bd4a0bea8391cbebb44a34aba53a9b50541"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"0dcf8f79417865e5028d753e669fae06457e8369":["680b6449f09827f58fe987aff279e014c311d966"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","b0267c69e2456a3477a1ad785723f2135da3117e"],"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"622a708571e534680618b3c5e0c28ac539a47776":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["55980207f1977bd1463465de1659b821347e2fa8","9299079153fd7895bf3cf6835cf7019af2ba89b3"],"55980207f1977bd1463465de1659b821347e2fa8":[],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","62ad8b8adb87da9e297f35a6ef432258c8076c96"],"ca044bd4a0bea8391cbebb44a34aba53a9b50541":[],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"b0267c69e2456a3477a1ad785723f2135da3117e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"06555ac714e8cc3a237853c80f762cfabc07a509":["0efcf27cd5ca23def8376b4c321970c14dd71623","5f6bd27530a2846413fe2d00030493c0e2d3a072","c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"bec68e7c41fed133827595747d853cad504e481e":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","06555ac714e8cc3a237853c80f762cfabc07a509"],"62ad8b8adb87da9e297f35a6ef432258c8076c96":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","1e6acbaae7af722f17204ceccf0f7db5753eccf3","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","622a708571e534680618b3c5e0c28ac539a47776"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["5f6bd27530a2846413fe2d00030493c0e2d3a072","55980207f1977bd1463465de1659b821347e2fa8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","b06445ae1731e049327712db0454e5643ca9b7fe","55980207f1977bd1463465de1659b821347e2fa8","ca044bd4a0bea8391cbebb44a34aba53a9b50541","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}