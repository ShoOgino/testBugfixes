{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","commits":[{"id":"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d","date":1426480823,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find its start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n       // Alternative 2: get the docSet of the prefix (could take a while) and\n       // then do the intersection with the baseDocSet first.\n       if (prefix != null && prefix.length() > 0) {\n       docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n       // The issue with this method are problems of returning 0 counts for terms w/o\n       // the prefix.  We can't just filter out those terms later because it may\n       // mean that we didn't collect enough terms in the queue (in the sorted case).\n       }\n       ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n          && startTerm==0 && endTerm==numTermsInField\n          && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a7c13535572b8e97cc477fc3388a57321a7751a","date":1427500960,"type":4,"author":"Yonik Seeley","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find its start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n       // Alternative 2: get the docSet of the prefix (could take a while) and\n       // then do the intersection with the baseDocSet first.\n       if (prefix != null && prefix.length() > 0) {\n       docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n       // The issue with this method are problems of returning 0 counts for terms w/o\n       // the prefix.  We can't just filter out those terms later because it may\n       // mean that we didn't collect enough terms in the queue (in the sorted case).\n       }\n       ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n          && startTerm==0 && endTerm==numTermsInField\n          && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a7c13535572b8e97cc477fc3388a57321a7751a"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["4a7c13535572b8e97cc477fc3388a57321a7751a"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}