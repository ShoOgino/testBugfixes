{"path":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","commits":[{"id":"66e0b82bd39567aa2bf534e5282d05fb4a4a2c76","date":1471585465,"type":1,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionMessageHandler#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n    \n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      final String asyncId = message.getStr(ASYNC);\n      Map<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = parentSlice.getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<Position, String> nodeMap = identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n        for (Map.Entry<Position, String> entry : nodeMap.entrySet()) {\n          String sliceName = entry.getKey().shard;\n          String subShardNodeName = entry.getValue();\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerCollectionMessageHandler#splitShard(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    \n    log.info(\"Split shard invoked\");\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();\n    \n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    \n    Slice parentSlice;\n    \n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n    \n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    \n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n    \n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n    \n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n        \n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String,Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n      \n      final String asyncId = message.getStr(ASYNC);\n      Map<String,String> requestMap = new HashMap<>();\n      \n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n        \n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n        \n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n        \n        // wait until we are able to see the new shard in cluster state\n        waitForNewShard(collectionName, subSlice);\n        \n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n        \n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        addReplica(clusterState, new ZkNodeProps(propMap), results);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n      \n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        \n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n      \n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n          \n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n          \n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n      \n      sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n      \n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n      \n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        \n        log.info(\"Applying buffered updates on : \" + subShardName);\n        \n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n        \n        sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n      \n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n      \n      // Replica creation for the new Slices\n      \n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n      \n      int repFactor = parentSlice.getReplicas().size();\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      // TODO: Have maxShardsPerNode param for this operation?\n      \n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<Position, String> nodeMap = identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n        for (Map.Entry<Position, String> entry : nodeMap.entrySet()) {\n          String sliceName = entry.getKey().shard;\n          String subShardNodeName = entry.getValue();\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n              + collectionName + \" on \" + subShardNodeName);\n\n          ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n              ZkStateReader.COLLECTION_PROP, collectionName,\n              ZkStateReader.SHARD_ID_PROP, sliceName,\n              ZkStateReader.CORE_NAME_PROP, shardName,\n              ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n              ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n              ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n          Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n          HashMap<String,Object> propMap = new HashMap<>();\n          propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n          propMap.put(COLLECTION_PROP, collectionName);\n          propMap.put(SHARD_ID_PROP, sliceName);\n          propMap.put(\"node\", subShardNodeName);\n          propMap.put(CoreAdminParams.NAME, shardName);\n          // copy over property params:\n          for (String key : message.keySet()) {\n            if (key.startsWith(COLL_PROP_PREFIX)) {\n              propMap.put(key, message.getStr(key));\n            }\n          }\n          // add async param\n          if (asyncId != null) {\n            propMap.put(ASYNC, asyncId);\n          }\n          // special flag param to instruct addReplica not to create the replica in cluster state again\n          propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n          replicas.add(propMap);\n        }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String,Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        addReplica(clusterState, new ZkNodeProps(replica), results);\n      }\n\n      processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n      \n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n      \n      commit(results, slice, parentShardLeader);\n      \n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1d231959c9d0545adc421b7a2fefa7db47300d8","date":1472269133,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (SolrException e) {\n              throwIfNotNonExistentCoreException(subSlice, e);\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        zkStateReader.forceUpdateCollection(collectionName);\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c","date":1472580862,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (SolrException e) {\n              throwIfNotNonExistentCoreException(subSlice, e);\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        zkStateReader.forceUpdateCollection(collectionName);\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":["d1d231959c9d0545adc421b7a2fefa7db47300d8"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bc8f206328a706450934717bec7ccc22ad166fc0","date":1473142172,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (SolrException e) {\n              throwIfNotNonExistentCoreException(subSlice, e);\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        zkStateReader.forceUpdateCollection(collectionName);\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b","date":1473679846,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub,\n                    e);\n              }\n            }\n          }\n        }\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"/dev/null","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61c45e99cf6676da48f19d7511c73712ad39402b","date":1495508331,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["685af99397b6da31116a2cac747ed255d217d080"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"25e4a4cddd699db6cce60282e747c7705897e821","date":1496721158,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74aea047dff7f7c38a2d766827bd20d356f98c6a","date":1496721416,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"969718c368b28ed1b2335ea2deb275c696cddb4f","date":1498803580,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d92226151c91fb4bebcca6d18782d1c84aee2cd","date":1498804792,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      Map<ReplicaAssigner.Position, String> nodeMap = ocmh.identifyNodes(clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (Map.Entry<ReplicaAssigner.Position, String> entry : nodeMap.entrySet()) {\n        String sliceName = entry.getKey().shard;\n        String subShardNodeName = entry.getValue();\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (entry.getKey().index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"651c3ddf5bc1266d9de0a972ec05e59d77099a4c","date":1500969855,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"936cdd5882761db3b844afd6f84ab81cbb011a75","date":1500973524,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a52341299179de5479672f7cf518bf4b173f34b3","date":1501079746,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(collectionName, subSlice, Replica.Type.NRT, 1);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bccf7971a36bd151490117582a0a1a695081ead3","date":1502778995,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c304e97e7c1d472bc70e801b35ee78583916c6cd","date":1507105431,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"560c18d71dad43d675158783c3840f8c80d6d39c","date":1507105532,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(() -> ocmh.overseer.getZkController().getCoreContainer(),\n          ocmh.zkStateReader, clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85212dad4ed576c7f7e6c165ee19e597b7b4efc8","date":1507997740,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.zkStateReader.getZkClient(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2bcfee499548996a6e5448bbf93b8f276d010270","date":1508336936,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName);\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"427295870ac138112ed6ab0973a2dbe42e0a1a2d","date":1510742913,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(PolicyHelper.getPolicySessionRef(ocmh.overseer.getSolrCloudManager()));\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(ocmh.policySessionRef);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d907c28c7fe6305eaec1756d51365f5149e1e41d","date":1512533044,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      PolicyHelper.clearFlagAndDecref(PolicyHelper.getPolicySessionRef(ocmh.overseer.getSolrCloudManager()));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","date":1513252583,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String collectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n\n    try {\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n      String nodeName = parentShardLeader.getNodeName();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh.cloudManager,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice.get(), parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n\n    String splitKey = message.getStr(\"split.key\");\n    ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n\n    Slice parentSlice;\n\n    if (slice == null) {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \"\n                + router.getClass().getName());\n      }\n    } else {\n      parentSlice = collection.getSlice(slice);\n    }\n\n    if (parentSlice == null) {\n      // no chance of the collection being null because ClusterState#getCollection(String) would have thrown\n      // an exception already\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n    }\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice, 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null) {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else {\n        subRanges = new ArrayList<>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max))) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specified hash ranges: \" + rangesStr\n                + \" either overlap with each other or \" + \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null) {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey\n              + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1) rangesStr += ',';\n        }\n      }\n    } else {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<>(subRanges.size());\n      List<String> subShardNames = new ArrayList<>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = Assign.buildCoreName(ocmh.overseer.getSolrCloudManager().getDistribStateManager(), collection, subSlice, Replica.Type.NRT);\n        subShardNames.add(subShardName);\n      }\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + shardName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, shardName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, shardName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice, Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice, parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","bugFix":null,"bugIntro":["8a8f0700353755635a948651e84c152bff39d8dc","8a8f0700353755635a948651e84c152bff39d8dc","bb222a3f9d9421d5c95afce73013fbd8de07ea1f","685af99397b6da31116a2cac747ed255d217d080"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6146c07c0dee1ae1e42926167acd127fed5ef59d","date":1516129420,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String collectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n\n    try {\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n      String nodeName = parentShardLeader.getNodeName();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh.cloudManager,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(OverseerCollectionMessageHandler.COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(OverseerCollectionMessageHandler.SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice.get(), parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String collectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n\n    try {\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n      String nodeName = parentShardLeader.getNodeName();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh.cloudManager,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice.get(), parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/SplitShardCmd#split(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":null,"sourceOld":"  public boolean split(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception {\n    boolean waitForFinalState = message.getBool(CommonAdminParams.WAIT_FOR_FINAL_STATE, false);\n    String collectionName = message.getStr(CoreAdminParams.COLLECTION);\n\n    log.info(\"Split shard invoked\");\n    ZkStateReader zkStateReader = ocmh.zkStateReader;\n    zkStateReader.forceUpdateCollection(collectionName);\n    AtomicReference<String> slice = new AtomicReference<>();\n    slice.set(message.getStr(ZkStateReader.SHARD_ID_PROP));\n\n    String splitKey = message.getStr(\"split.key\");\n    DocCollection collection = clusterState.getCollection(collectionName);\n\n    PolicyHelper.SessionWrapper sessionWrapper = null;\n\n    Slice parentSlice = getParentSlice(clusterState, collectionName, slice, splitKey);\n\n    // find the leader for the shard\n    Replica parentShardLeader = null;\n    try {\n      parentShardLeader = zkStateReader.getLeaderRetry(collectionName, slice.get(), 10000);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    // let's record the ephemeralOwner of the parent leader node\n    Stat leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n    if (leaderZnodeStat == null)  {\n      // we just got to know the leader but its live node is gone already!\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n    }\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    String rangesStr = fillRanges(ocmh.cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n\n    try {\n\n      boolean oldShardsDeleted = false;\n      for (String subSlice : subSlices) {\n        Slice oSlice = collection.getSlice(subSlice);\n        if (oSlice != null) {\n          final Slice.State state = oSlice.getState();\n          if (state == Slice.State.ACTIVE) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (state == Slice.State.CONSTRUCTION || state == Slice.State.RECOVERY) {\n            // delete the shards\n            log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", subSlice);\n            Map<String, Object> propMap = new HashMap<>();\n            propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n            propMap.put(COLLECTION_PROP, collectionName);\n            propMap.put(SHARD_ID_PROP, subSlice);\n            ZkNodeProps m = new ZkNodeProps(propMap);\n            try {\n              ocmh.commandMap.get(DELETESHARD).call(clusterState, m, new NamedList());\n            } catch (Exception e) {\n              throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + subSlice,\n                  e);\n            }\n\n            oldShardsDeleted = true;\n          }\n        }\n      }\n\n      if (oldShardsDeleted) {\n        // refresh the locally cached cluster state\n        // we know we have the latest because otherwise deleteshard would have failed\n        clusterState = zkStateReader.getClusterState();\n        collection = clusterState.getCollection(collectionName);\n      }\n\n      final String asyncId = message.getStr(ASYNC);\n      Map<String, String> requestMap = new HashMap<>();\n      String nodeName = parentShardLeader.getNodeName();\n\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating slice \" + subSlice + \" of collection \" + collectionName + \" on \" + nodeName);\n\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, CREATESHARD.toLower());\n        propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        propMap.put(ZkStateReader.SHARD_RANGE_PROP, subRange.toString());\n        propMap.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.CONSTRUCTION.toString());\n        propMap.put(ZkStateReader.SHARD_PARENT_PROP, parentSlice.getName());\n        propMap.put(\"shard_parent_node\", parentShardLeader.getNodeName());\n        propMap.put(\"shard_parent_zk_session\", leaderZnodeStat.getEphemeralOwner());\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        inQueue.offer(Utils.toJSON(new ZkNodeProps(propMap)));\n\n        // wait until we are able to see the new shard in cluster state\n        ocmh.waitForNewShard(collectionName, subSlice);\n\n        // refresh cluster state\n        clusterState = zkStateReader.getClusterState();\n\n        log.info(\"Adding replica \" + subShardName + \" as part of slice \" + subSlice + \" of collection \" + collectionName\n            + \" on \" + nodeName);\n        propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, subSlice);\n        propMap.put(\"node\", nodeName);\n        propMap.put(CoreAdminParams.NAME, subShardName);\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        ocmh.addReplica(clusterState, new ZkNodeProps(propMap), results, null);\n      }\n\n      ShardHandler shardHandler = ocmh.shardHandlerFactory.getShardHandler();\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard leaders\", asyncId, requestMap);\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = ocmh.waitForCoreNodeName(collectionName, nodeName, subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(Replica.State.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n\n        ModifiableSolrParams p = new ModifiableSolrParams(cmd.getParams());\n        ocmh.sendShardRequest(nodeName, p, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD timed out waiting for subshard leaders to come up\",\n          asyncId, requestMap);\n\n      log.info(\"Successfully created all sub-shards for collection \" + collectionName + \" parent shard: \" + slice\n          + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \" + slice + \" of collection \"\n          + collectionName + \" on \" + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      ocmh.sendShardRequest(parentShardLeader.getNodeName(), params, shardHandler, asyncId, requestMap);\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to invoke SPLIT core admin command\", asyncId,\n          requestMap);\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        ocmh.sendShardRequest(nodeName, params, shardHandler, asyncId, requestMap);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed while asking sub shard leaders\" +\n          \" to apply buffered updates\", asyncId, requestMap);\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = parentSlice.getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<>(nodes.size());\n      nodeList.addAll(nodes);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n\n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      List<ReplicaPosition> replicaPositions = Assign.identifyNodes(ocmh.cloudManager,\n          clusterState,\n          new ArrayList<>(clusterState.getLiveNodes()),\n          collectionName,\n          new ZkNodeProps(collection.getProperties()),\n          subSlices, repFactor - 1, 0, 0);\n      sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n\n      List<Map<String, Object>> replicas = new ArrayList<>((repFactor - 1) * 2);\n\n      for (ReplicaPosition replicaPosition : replicaPositions) {\n        String sliceName = replicaPosition.shard;\n        String subShardNodeName = replicaPosition.node;\n        String solrCoreName = collectionName + \"_\" + sliceName + \"_replica\" + (replicaPosition.index);\n\n        log.info(\"Creating replica shard \" + solrCoreName + \" as part of slice \" + sliceName + \" of collection \"\n            + collectionName + \" on \" + subShardNodeName);\n\n        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower(),\n            ZkStateReader.COLLECTION_PROP, collectionName,\n            ZkStateReader.SHARD_ID_PROP, sliceName,\n            ZkStateReader.CORE_NAME_PROP, solrCoreName,\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, zkStateReader.getBaseUrlForNodeName(subShardNodeName),\n            ZkStateReader.NODE_NAME_PROP, subShardNodeName,\n            CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n        Overseer.getStateUpdateQueue(zkStateReader.getZkClient()).offer(Utils.toJSON(props));\n\n        HashMap<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, ADDREPLICA.toLower());\n        propMap.put(COLLECTION_PROP, collectionName);\n        propMap.put(SHARD_ID_PROP, sliceName);\n        propMap.put(\"node\", subShardNodeName);\n        propMap.put(CoreAdminParams.NAME, solrCoreName);\n        // copy over property params:\n        for (String key : message.keySet()) {\n          if (key.startsWith(COLL_PROP_PREFIX)) {\n            propMap.put(key, message.getStr(key));\n          }\n        }\n        // add async param\n        if (asyncId != null) {\n          propMap.put(ASYNC, asyncId);\n        }\n        // special flag param to instruct addReplica not to create the replica in cluster state again\n        propMap.put(SKIP_CREATE_REPLICA_IN_CLUSTER_STATE, \"true\");\n\n        propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, Boolean.toString(waitForFinalState));\n\n        replicas.add(propMap);\n      }\n\n      assert TestInjection.injectSplitFailureBeforeReplicaCreation();\n\n      long ephemeralOwner = leaderZnodeStat.getEphemeralOwner();\n      // compare against the ephemeralOwner of the parent leader node\n      leaderZnodeStat = zkStateReader.getZkClient().exists(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + parentShardLeader.getNodeName(), null, true);\n      if (leaderZnodeStat == null || ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n        // put sub-shards in recovery_failed state\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY_FAILED.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n\n        if (leaderZnodeStat == null)  {\n          // the leader is not live anymore, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The shard leader node: \" + parentShardLeader.getNodeName() + \" is not live anymore!\");\n        } else if (ephemeralOwner != leaderZnodeStat.getEphemeralOwner()) {\n          // there's a new leader, fail the split!\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"The zk session id for the shard leader node: \" + parentShardLeader.getNodeName() + \" has changed from \"\n                  + ephemeralOwner + \" to \" + leaderZnodeStat.getEphemeralOwner() + \". This can cause data loss so we must abort the split\");\n        }\n      }\n\n      // we must set the slice state into recovery before actually creating the replica cores\n      // this ensures that the logic inside Overseer to update sub-shard state to 'active'\n      // always gets a chance to execute. See SOLR-7673\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        propMap.put(slice.get(), Slice.State.INACTIVE.toString());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.ACTIVE.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      } else {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getStateUpdateQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<>();\n        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.State.RECOVERY.toString());\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(Utils.toJSON(m));\n      }\n\n      // now actually create replica cores on sub shard nodes\n      for (Map<String, Object> replica : replicas) {\n        ocmh.addReplica(clusterState, new ZkNodeProps(replica), results, null);\n      }\n\n      ocmh.processResponses(results, shardHandler, true, \"SPLITSHARD failed to create subshard replicas\", asyncId, requestMap);\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      ocmh.commit(results, slice.get(), parentShardLeader);\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n    } finally {\n      if (sessionWrapper != null) sessionWrapper.release();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"2bcfee499548996a6e5448bbf93b8f276d010270":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"25e4a4cddd699db6cce60282e747c7705897e821":["61c45e99cf6676da48f19d7511c73712ad39402b"],"c304e97e7c1d472bc70e801b35ee78583916c6cd":["a52341299179de5479672f7cf518bf4b173f34b3","bccf7971a36bd151490117582a0a1a695081ead3"],"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b":["bc8f206328a706450934717bec7ccc22ad166fc0"],"bc8f206328a706450934717bec7ccc22ad166fc0":["d1d231959c9d0545adc421b7a2fefa7db47300d8","092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c"],"89424def13674ea17829b41c5883c54ecc31a132":["092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c","6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","74aea047dff7f7c38a2d766827bd20d356f98c6a"],"74aea047dff7f7c38a2d766827bd20d356f98c6a":["61c45e99cf6676da48f19d7511c73712ad39402b","25e4a4cddd699db6cce60282e747c7705897e821"],"969718c368b28ed1b2335ea2deb275c696cddb4f":["28288370235ed02234a64753cdbf0c6ec096304a"],"0d92226151c91fb4bebcca6d18782d1c84aee2cd":["e9017cf144952056066919f1ebc7897ff9bd71b1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["403d05f7f8d69b65659157eff1bc1d2717f04c66","89424def13674ea17829b41c5883c54ecc31a132"],"66e0b82bd39567aa2bf534e5282d05fb4a4a2c76":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["d907c28c7fe6305eaec1756d51365f5149e1e41d"],"560c18d71dad43d675158783c3840f8c80d6d39c":["a52341299179de5479672f7cf518bf4b173f34b3","c304e97e7c1d472bc70e801b35ee78583916c6cd"],"d1d231959c9d0545adc421b7a2fefa7db47300d8":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"651c3ddf5bc1266d9de0a972ec05e59d77099a4c":["969718c368b28ed1b2335ea2deb275c696cddb4f"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","66e0b82bd39567aa2bf534e5282d05fb4a4a2c76"],"61c45e99cf6676da48f19d7511c73712ad39402b":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"bccf7971a36bd151490117582a0a1a695081ead3":["936cdd5882761db3b844afd6f84ab81cbb011a75"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["560c18d71dad43d675158783c3840f8c80d6d39c"],"936cdd5882761db3b844afd6f84ab81cbb011a75":["0d92226151c91fb4bebcca6d18782d1c84aee2cd","651c3ddf5bc1266d9de0a972ec05e59d77099a4c"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"28288370235ed02234a64753cdbf0c6ec096304a":["61c45e99cf6676da48f19d7511c73712ad39402b","74aea047dff7f7c38a2d766827bd20d356f98c6a"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c":["d1d231959c9d0545adc421b7a2fefa7db47300d8"],"a52341299179de5479672f7cf518bf4b173f34b3":["969718c368b28ed1b2335ea2deb275c696cddb4f","651c3ddf5bc1266d9de0a972ec05e59d77099a4c"],"427295870ac138112ed6ab0973a2dbe42e0a1a2d":["2bcfee499548996a6e5448bbf93b8f276d010270"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b94236357aaa22b76c10629851fe4e376e0cea82"],"d907c28c7fe6305eaec1756d51365f5149e1e41d":["427295870ac138112ed6ab0973a2dbe42e0a1a2d"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2bcfee499548996a6e5448bbf93b8f276d010270":["427295870ac138112ed6ab0973a2dbe42e0a1a2d"],"25e4a4cddd699db6cce60282e747c7705897e821":["74aea047dff7f7c38a2d766827bd20d356f98c6a"],"c304e97e7c1d472bc70e801b35ee78583916c6cd":["560c18d71dad43d675158783c3840f8c80d6d39c"],"6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b":["89424def13674ea17829b41c5883c54ecc31a132"],"bc8f206328a706450934717bec7ccc22ad166fc0":["6a23ab64d81a448ad6ec571cbfc9599cc09b4e4b"],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["0d92226151c91fb4bebcca6d18782d1c84aee2cd"],"74aea047dff7f7c38a2d766827bd20d356f98c6a":["e9017cf144952056066919f1ebc7897ff9bd71b1","28288370235ed02234a64753cdbf0c6ec096304a"],"969718c368b28ed1b2335ea2deb275c696cddb4f":["651c3ddf5bc1266d9de0a972ec05e59d77099a4c","a52341299179de5479672f7cf518bf4b173f34b3"],"0d92226151c91fb4bebcca6d18782d1c84aee2cd":["936cdd5882761db3b844afd6f84ab81cbb011a75"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e9017cf144952056066919f1ebc7897ff9bd71b1","61c45e99cf6676da48f19d7511c73712ad39402b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["66e0b82bd39567aa2bf534e5282d05fb4a4a2c76","403d05f7f8d69b65659157eff1bc1d2717f04c66","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"66e0b82bd39567aa2bf534e5282d05fb4a4a2c76":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["b94236357aaa22b76c10629851fe4e376e0cea82","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"560c18d71dad43d675158783c3840f8c80d6d39c":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"d1d231959c9d0545adc421b7a2fefa7db47300d8":["bc8f206328a706450934717bec7ccc22ad166fc0","092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c"],"651c3ddf5bc1266d9de0a972ec05e59d77099a4c":["936cdd5882761db3b844afd6f84ab81cbb011a75","a52341299179de5479672f7cf518bf4b173f34b3"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","d1d231959c9d0545adc421b7a2fefa7db47300d8"],"61c45e99cf6676da48f19d7511c73712ad39402b":["25e4a4cddd699db6cce60282e747c7705897e821","74aea047dff7f7c38a2d766827bd20d356f98c6a","28288370235ed02234a64753cdbf0c6ec096304a"],"bccf7971a36bd151490117582a0a1a695081ead3":["c304e97e7c1d472bc70e801b35ee78583916c6cd"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["2bcfee499548996a6e5448bbf93b8f276d010270"],"936cdd5882761db3b844afd6f84ab81cbb011a75":["bccf7971a36bd151490117582a0a1a695081ead3"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["b94236357aaa22b76c10629851fe4e376e0cea82"],"28288370235ed02234a64753cdbf0c6ec096304a":["969718c368b28ed1b2335ea2deb275c696cddb4f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"092c3ae5fefa024f6d0c427be5f23dd3bfbdd20c":["bc8f206328a706450934717bec7ccc22ad166fc0","89424def13674ea17829b41c5883c54ecc31a132"],"a52341299179de5479672f7cf518bf4b173f34b3":["c304e97e7c1d472bc70e801b35ee78583916c6cd","560c18d71dad43d675158783c3840f8c80d6d39c"],"427295870ac138112ed6ab0973a2dbe42e0a1a2d":["d907c28c7fe6305eaec1756d51365f5149e1e41d"],"d907c28c7fe6305eaec1756d51365f5149e1e41d":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}