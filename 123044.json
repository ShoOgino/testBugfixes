{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","commits":[{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"/dev/null","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac981db60ef979233b3438ec49ddae82e8cc4697","date":1503407558,"type":3,"author":"Toke Eskildsen","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f15af35d55d70c34451f9df5edeaeff6b31f8cbe","date":1519625627,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c705a0d590cf911e7c942df49563ca2ea176e22","date":1526916174,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex,\n                                                    slotNum -> { return new SlotContext(tt.termQuery); });\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56a9893014b284af4d1af451e6c02e7ffdf5b6e","date":1590065972,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final SlotAcc.CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex,\n                                                    slotNum -> { return new SlotContext(tt.termQuery); });\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex,\n                                                    slotNum -> { return new SlotContext(tt.termQuery); });\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f9e4bd10604489b5817ee29e35ac96a3148cbec","date":1594345357,"type":3,"author":"Michael Gibney","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n    final SweepCountAccStruct baseCountAccStruct = SweepingCountSlotAcc.baseStructOf(processor);\n    final List<SweepCountAccStruct> others = SweepingCountSlotAcc.otherStructsOf(processor);\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet termSet = searcher.getDocSet(tt.termQuery);\n        DocSet intersection = termSet.intersection(docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex,\n                                                    slotNum -> { return new SlotContext(tt.termQuery); });\n        final int termOrd = tt.termNum - startTermIndex;\n        countAcc.incrementCount(termOrd, collected);\n        for (SweepCountAccStruct entry : others) {\n          entry.countAcc.incrementCount(termOrd, termSet.intersectionSize(entry.docSet));\n        }\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      SweepIteratorAndCounts sweepIterAndCounts = SweepDocIterator.newInstance(baseCountAccStruct, others);\n      final SweepDocIterator iter = sweepIterAndCounts.iter;\n      final CountSlotAcc[] countAccs = sweepIterAndCounts.countAccs;\n      final SegCountGlobal counts = new SegCountGlobal(countAccs);\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int maxIdx = iter.registerCounts(counts);\n        boolean collectBase = iter.collectBase();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            counts.incrementCount(arrIdx, 1, maxIdx);\n            if (collectBase) {\n              processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                counts.incrementCount(arrIdx, 1, maxIdx);\n                if (collectBase) {\n                  processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n                }\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final SlotAcc.CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        DocSet intersection = searcher.getDocSet(tt.termQuery, docs);\n        int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex,\n                                                    slotNum -> { return new SlotContext(tt.termQuery); });\n        countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n        if (collected > 0) {\n          uniqueTerms++;\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx, processor.slotContext);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2c705a0d590cf911e7c942df49563ca2ea176e22":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"2f9e4bd10604489b5817ee29e35ac96a3148cbec":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["2c705a0d590cf911e7c942df49563ca2ea176e22"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["403d05f7f8d69b65659157eff1bc1d2717f04c66","ac981db60ef979233b3438ec49ddae82e8cc4697"],"ac981db60ef979233b3438ec49ddae82e8cc4697":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","79759974460bc59933cd169acc94f5c6b16368d5"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["ac981db60ef979233b3438ec49ddae82e8cc4697"],"79759974460bc59933cd169acc94f5c6b16368d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2f9e4bd10604489b5817ee29e35ac96a3148cbec"]},"commit2Childs":{"2c705a0d590cf911e7c942df49563ca2ea176e22":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"],"2f9e4bd10604489b5817ee29e35ac96a3148cbec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["2f9e4bd10604489b5817ee29e35ac96a3148cbec"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","ac981db60ef979233b3438ec49ddae82e8cc4697","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"ac981db60ef979233b3438ec49ddae82e8cc4697":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["2c705a0d590cf911e7c942df49563ca2ea176e22"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}