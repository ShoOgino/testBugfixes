{"path":"contrib/wordnetlc/src/java/org/apache/lucene/wordnet/SynLookup#expand(String,Searcher,Analyzer,String,float).mjava","commits":[{"id":"b4d1f544536b6c6ad3f1d34803c651c019795f51","date":1115339471,"type":1,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/wordnetlc/src/java/org/apache/lucene/wordnet/SynLookup#expand(String,Searcher,Analyzer,String,float).mjava","pathOld":"contrib/WordNet/src/java/org/apache/lucene/wordnet/SynLookup#expand(String,Searcher,Analyzer,String,float).mjava","sourceNew":"\t/**\n\t * Perform synonym expansion on a query.\n\t *\n\t * @param query\n\t * @param syns\n\t * @param a\n\t * @param field\n\t * @param boost\n\t */ \n\tpublic static Query expand( String query,\n\t\t\t\t\t\t\t\tSearcher syns,\n\t\t\t\t\t\t\t\tAnalyzer a,\n\t\t\t\t\t\t\t\tString field,\n\t\t\t\t\t\t\t\tfloat boost)\n\t\tthrows IOException\n\t{\n\t\tSet already = new HashSet(); // avoid dups\t\t\n\t\tList top = new LinkedList(); // needs to be separately listed..\n\n\t\t// [1] Parse query into separate words so that when we expand we can avoid dups\n\t\tTokenStream ts = a.tokenStream( field, new StringReader( query));\n\t\torg.apache.lucene.analysis.Token t;\n\t\twhile ( (t = ts.next()) != null)\n\t\t{\n\t\t\tString word = t.termText();\n\t\t\tif ( already.add( word))\n\t\t\t\ttop.add( word);\n\t\t}\n\t\tBooleanQuery tmp = new BooleanQuery();\n\t\t\n\t\t// [2] form query\n\t\tIterator it = top.iterator();\n\t\twhile ( it.hasNext())\n\t\t{\n\t\t\t// [2a] add to level words in\n\t\t\tString word = (String) it.next();\n\t\t\tTermQuery tq = new TermQuery( new Term( field, word));\n\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD);\n\n\t\t\t// [2b] add in unique synonums\n\t\t\tHits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));\n\t\t\tfor (int i = 0; i < hits.length(); i++)\n\t\t\t{\n\t\t\t\tDocument doc = hits.doc(i);\n\t\t\t\tString[] values = doc.getValues( Syns2Index.F_SYN);\n\t\t\t\tfor ( int j = 0; j < values.length; j++)\n\t\t\t\t{\n\t\t\t\t\tString syn = values[ j];\n\t\t\t\t\tif ( already.add( syn))\n\t\t\t\t\t{\n\t\t\t\t\t\ttq = new TermQuery( new Term( field, syn));\n\t\t\t\t\t\tif ( boost > 0) // else keep normal 1.0\n\t\t\t\t\t\t\ttq.setBoost( boost);\n\t\t\t\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD); \n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\treturn tmp;\n\t}\n\n","sourceOld":"\t/**\n\t * Perform synonym expansion on a query.\n\t *\n\t * @param query\n\t * @param syns\n\t * @param a\n\t * @param field\n\t * @param boost\n\t */ \n\tpublic static Query expand( String query,\n\t\t\t\t\t\t\t\tSearcher syns,\n\t\t\t\t\t\t\t\tAnalyzer a,\n\t\t\t\t\t\t\t\tString field,\n\t\t\t\t\t\t\t\tfloat boost)\n\t\tthrows IOException\n\t{\n\t\tSet already = new HashSet(); // avoid dups\t\t\n\t\tList top = new LinkedList(); // needs to be separately listed..\n\n\t\t// [1] Parse query into separate words so that when we expand we can avoid dups\n\t\tTokenStream ts = a.tokenStream( field, new StringReader( query));\n\t\torg.apache.lucene.analysis.Token t;\n\t\twhile ( (t = ts.next()) != null)\n\t\t{\n\t\t\tString word = t.termText();\n\t\t\tif ( already.add( word))\n\t\t\t\ttop.add( word);\n\t\t}\n\t\tBooleanQuery tmp = new BooleanQuery();\n\t\t\n\t\t// [2] form query\n\t\tIterator it = top.iterator();\n\t\twhile ( it.hasNext())\n\t\t{\n\t\t\t// [2a] add to level words in\n\t\t\tString word = (String) it.next();\n\t\t\tTermQuery tq = new TermQuery( new Term( field, word));\n\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD);\n\n\t\t\t// [2b] add in unique synonums\n\t\t\tHits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));\n\t\t\tfor (int i = 0; i < hits.length(); i++)\n\t\t\t{\n\t\t\t\tDocument doc = hits.doc(i);\n\t\t\t\tString[] values = doc.getValues( Syns2Index.F_SYN);\n\t\t\t\tfor ( int j = 0; j < values.length; j++)\n\t\t\t\t{\n\t\t\t\t\tString syn = values[ j];\n\t\t\t\t\tif ( already.add( syn))\n\t\t\t\t\t{\n\t\t\t\t\t\ttq = new TermQuery( new Term( field, syn));\n\t\t\t\t\t\tif ( boost > 0) // else keep normal 1.0\n\t\t\t\t\t\t\ttq.setBoost( boost);\n\t\t\t\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD); \n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\treturn tmp;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02a71af6b0525092e8cdc9e3649fa77150cc7814","date":1115339520,"type":5,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup#expand(String,Searcher,Analyzer,String,float).mjava","pathOld":"contrib/wordnetlc/src/java/org/apache/lucene/wordnet/SynLookup#expand(String,Searcher,Analyzer,String,float).mjava","sourceNew":"\t/**\n\t * Perform synonym expansion on a query.\n\t *\n\t * @param query\n\t * @param syns\n\t * @param a\n\t * @param field\n\t * @param boost\n\t */ \n\tpublic static Query expand( String query,\n\t\t\t\t\t\t\t\tSearcher syns,\n\t\t\t\t\t\t\t\tAnalyzer a,\n\t\t\t\t\t\t\t\tString field,\n\t\t\t\t\t\t\t\tfloat boost)\n\t\tthrows IOException\n\t{\n\t\tSet already = new HashSet(); // avoid dups\t\t\n\t\tList top = new LinkedList(); // needs to be separately listed..\n\n\t\t// [1] Parse query into separate words so that when we expand we can avoid dups\n\t\tTokenStream ts = a.tokenStream( field, new StringReader( query));\n\t\torg.apache.lucene.analysis.Token t;\n\t\twhile ( (t = ts.next()) != null)\n\t\t{\n\t\t\tString word = t.termText();\n\t\t\tif ( already.add( word))\n\t\t\t\ttop.add( word);\n\t\t}\n\t\tBooleanQuery tmp = new BooleanQuery();\n\t\t\n\t\t// [2] form query\n\t\tIterator it = top.iterator();\n\t\twhile ( it.hasNext())\n\t\t{\n\t\t\t// [2a] add to level words in\n\t\t\tString word = (String) it.next();\n\t\t\tTermQuery tq = new TermQuery( new Term( field, word));\n\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD);\n\n\t\t\t// [2b] add in unique synonums\n\t\t\tHits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));\n\t\t\tfor (int i = 0; i < hits.length(); i++)\n\t\t\t{\n\t\t\t\tDocument doc = hits.doc(i);\n\t\t\t\tString[] values = doc.getValues( Syns2Index.F_SYN);\n\t\t\t\tfor ( int j = 0; j < values.length; j++)\n\t\t\t\t{\n\t\t\t\t\tString syn = values[ j];\n\t\t\t\t\tif ( already.add( syn))\n\t\t\t\t\t{\n\t\t\t\t\t\ttq = new TermQuery( new Term( field, syn));\n\t\t\t\t\t\tif ( boost > 0) // else keep normal 1.0\n\t\t\t\t\t\t\ttq.setBoost( boost);\n\t\t\t\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD); \n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\treturn tmp;\n\t}\n\n","sourceOld":"\t/**\n\t * Perform synonym expansion on a query.\n\t *\n\t * @param query\n\t * @param syns\n\t * @param a\n\t * @param field\n\t * @param boost\n\t */ \n\tpublic static Query expand( String query,\n\t\t\t\t\t\t\t\tSearcher syns,\n\t\t\t\t\t\t\t\tAnalyzer a,\n\t\t\t\t\t\t\t\tString field,\n\t\t\t\t\t\t\t\tfloat boost)\n\t\tthrows IOException\n\t{\n\t\tSet already = new HashSet(); // avoid dups\t\t\n\t\tList top = new LinkedList(); // needs to be separately listed..\n\n\t\t// [1] Parse query into separate words so that when we expand we can avoid dups\n\t\tTokenStream ts = a.tokenStream( field, new StringReader( query));\n\t\torg.apache.lucene.analysis.Token t;\n\t\twhile ( (t = ts.next()) != null)\n\t\t{\n\t\t\tString word = t.termText();\n\t\t\tif ( already.add( word))\n\t\t\t\ttop.add( word);\n\t\t}\n\t\tBooleanQuery tmp = new BooleanQuery();\n\t\t\n\t\t// [2] form query\n\t\tIterator it = top.iterator();\n\t\twhile ( it.hasNext())\n\t\t{\n\t\t\t// [2a] add to level words in\n\t\t\tString word = (String) it.next();\n\t\t\tTermQuery tq = new TermQuery( new Term( field, word));\n\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD);\n\n\t\t\t// [2b] add in unique synonums\n\t\t\tHits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));\n\t\t\tfor (int i = 0; i < hits.length(); i++)\n\t\t\t{\n\t\t\t\tDocument doc = hits.doc(i);\n\t\t\t\tString[] values = doc.getValues( Syns2Index.F_SYN);\n\t\t\t\tfor ( int j = 0; j < values.length; j++)\n\t\t\t\t{\n\t\t\t\t\tString syn = values[ j];\n\t\t\t\t\tif ( already.add( syn))\n\t\t\t\t\t{\n\t\t\t\t\t\ttq = new TermQuery( new Term( field, syn));\n\t\t\t\t\t\tif ( boost > 0) // else keep normal 1.0\n\t\t\t\t\t\t\ttq.setBoost( boost);\n\t\t\t\t\t\ttmp.add( tq, BooleanClause.Occur.SHOULD); \n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\n\t\treturn tmp;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02a71af6b0525092e8cdc9e3649fa77150cc7814":["b4d1f544536b6c6ad3f1d34803c651c019795f51"],"b4d1f544536b6c6ad3f1d34803c651c019795f51":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["02a71af6b0525092e8cdc9e3649fa77150cc7814"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b4d1f544536b6c6ad3f1d34803c651c019795f51"],"02a71af6b0525092e8cdc9e3649fa77150cc7814":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b4d1f544536b6c6ad3f1d34803c651c019795f51":["02a71af6b0525092e8cdc9e3649fa77150cc7814"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}