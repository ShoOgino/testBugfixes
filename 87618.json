{"path":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","commits":[{"id":"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a","date":1240390408,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    Token reusableToken = new Token();\n    Token token = null;\n\n    try {\n      while ((token = tokenStream.next(reusableToken)) != null) {\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef28ac95f5f85bbf872801277448c0924b0a6827","date":1268600312,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    Token reusableToken = new Token();\n    Token token = null;\n\n    try {\n      while ((token = tokenStream.next(reusableToken)) != null) {\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ef28ac95f5f85bbf872801277448c0924b0a6827":["68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a"],"ad94625fb8d088209f46650c8097196fec67f00c":["ef28ac95f5f85bbf872801277448c0924b0a6827"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a":["ef28ac95f5f85bbf872801277448c0924b0a6827"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["68df8db3f6c0c0ebbd1e40ba638115a748fb6a2a"],"ef28ac95f5f85bbf872801277448c0924b0a6827":["ad94625fb8d088209f46650c8097196fec67f00c"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}