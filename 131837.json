{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","commits":[{"id":"95323da8eca89d45766013f5b300a865a5ac7dfb","date":1348933777,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afe38d4771eae27550f75f664e7094cfb7d1e2f2","date":1348936511,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29baaefef1b62d76a3370ff72a0fe5f9bd84e365","date":1348949582,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      final int numTerms = atLeast(10);\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d53704af5dd90675c56e347e9637e8734e73229d","date":1348954019,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      final int numTerms = atLeast(10);\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      final int numTerms = atLeast(10);\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bdb48a021b1ad342569f087c1457740563897dc5","date":1348955678,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      final int numTerms = atLeast(10);\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"03d03ad772742aa485289047359fcfa328660c1f","date":1348956670,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8e0f7eb31bf8a32470d42414f4b1718d4ee3ce61","date":1348957022,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          // TODO: put multplier back, if we can fix OOMEs:\n          //numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          // TODO: put multplier back, if we can fix OOMEs:\n          //numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numDocs = _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39","date":1348960504,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 5) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          // TODO: put multplier back, if we can fix OOMEs:\n          //numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          // TODO: put multplier back, if we can fix OOMEs:\n          //numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numDocs = _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36","date":1349174553,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    maxDocID = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n    fieldsLive = new TreeMap<String,Map<BytesRef,List<Posting>>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (numBigTerms == 0 || (random().nextInt(10) == 3 && numBigTerms < 2)) {\n          // Make at least 1 big term, then maybe (~10%\n          // chance) make another:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (numMediumTerms == 0 || (random().nextInt(10) == 3 && numMediumTerms < 5)) {\n          // Make at least 1 medium term, then maybe (~10%\n          // chance) make up to 4 more:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        int payloadSize;\n        if (random().nextInt(10) == 7) {\n          // 10% of the time create big payloads:\n          payloadSize = 1 + random().nextInt(3);\n        } else {\n          payloadSize = 1 + random().nextInt(1);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 5) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1744e62c7bb40f7a950b360af6371e5d89a1c112","date":1375122209,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = _TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = _TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO:\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,SortedMap<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = _TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,Map<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = _TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,SortedMap<BytesRef,Long>>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,SortedMap<BytesRef,Long>>();\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = _TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<String,SortedMap<BytesRef,Long>>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71","date":1400675008,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":["0837ab0472feecb3a54260729d845f839e1cbd72"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0cdf9cc6702d60334a616bd7db3ae91501d1dce7","date":1405858112,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,Long> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), termSeed);\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05da2d758a6089e737cdfc230e57a51b472b94b6","date":1413392310,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","date":1413458798,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValuesType.NUMERIC, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NO, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS_ONLY, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f95ce1375367b92d411a06175eab3915fe93c6bc","date":1414788502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NO, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        DocsEnum docsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = docsEnum.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, null);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e8715d826e588419327562287d5d6a8040d63d6","date":1427987148,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    postingsTester = new RandomPostingsTester(random());\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2638f781be724518ff6c2263d14a48cf6e68017","date":1427989059,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    postingsTester = new RandomPostingsTester(random());\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    fields = new TreeMap<>();\n\n    final int numFields = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random().nextInt(10) == 7) {\n        numTerms = atLeast(50);\n      } else {\n        numTerms = TestUtil.nextInt(random(), 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random().nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1744e62c7bb40f7a950b360af6371e5d89a1c112":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["afe38d4771eae27550f75f664e7094cfb7d1e2f2"],"afe38d4771eae27550f75f664e7094cfb7d1e2f2":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bdb48a021b1ad342569f087c1457740563897dc5":["d53704af5dd90675c56e347e9637e8734e73229d"],"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39":["8e0f7eb31bf8a32470d42414f4b1718d4ee3ce61"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"b7605579001505896d48b07160075a5c8b8e128e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["51f5280f31484820499077f41fcdfe92d527d9dc","79700663e164dece87bed4adfd3e28bab6cb1385"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d2638f781be724518ff6c2263d14a48cf6e68017":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3e8715d826e588419327562287d5d6a8040d63d6"],"79700663e164dece87bed4adfd3e28bab6cb1385":["51f5280f31484820499077f41fcdfe92d527d9dc"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"0837ab0472feecb3a54260729d845f839e1cbd72":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36"],"d53704af5dd90675c56e347e9637e8734e73229d":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"],"299a2348fa24151d150182211b6208a38e5e3450":["51f5280f31484820499077f41fcdfe92d527d9dc","79700663e164dece87bed4adfd3e28bab6cb1385"],"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36":["c4108b7b6cc881d8b7c6ba2df50010b7bc581a39"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["0cdf9cc6702d60334a616bd7db3ae91501d1dce7"],"0cdf9cc6702d60334a616bd7db3ae91501d1dce7":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["0cdf9cc6702d60334a616bd7db3ae91501d1dce7","05da2d758a6089e737cdfc230e57a51b472b94b6"],"6613659748fe4411a7dcf85266e55db1f95f7315":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"03d03ad772742aa485289047359fcfa328660c1f":["bdb48a021b1ad342569f087c1457740563897dc5"],"51f5280f31484820499077f41fcdfe92d527d9dc":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3184874f7f3aca850248483485b4995343066875":["05da2d758a6089e737cdfc230e57a51b472b94b6"],"8e0f7eb31bf8a32470d42414f4b1718d4ee3ce61":["03d03ad772742aa485289047359fcfa328660c1f"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36","0837ab0472feecb3a54260729d845f839e1cbd72"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["1744e62c7bb40f7a950b360af6371e5d89a1c112"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"3e8715d826e588419327562287d5d6a8040d63d6":["79700663e164dece87bed4adfd3e28bab6cb1385"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e8715d826e588419327562287d5d6a8040d63d6"]},"commit2Childs":{"1744e62c7bb40f7a950b360af6371e5d89a1c112":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["d53704af5dd90675c56e347e9637e8734e73229d"],"afe38d4771eae27550f75f664e7094cfb7d1e2f2":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["afe38d4771eae27550f75f664e7094cfb7d1e2f2"],"bdb48a021b1ad342569f087c1457740563897dc5":["03d03ad772742aa485289047359fcfa328660c1f"],"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"b7605579001505896d48b07160075a5c8b8e128e":[],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["d2638f781be724518ff6c2263d14a48cf6e68017"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"d2638f781be724518ff6c2263d14a48cf6e68017":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","299a2348fa24151d150182211b6208a38e5e3450","3e8715d826e588419327562287d5d6a8040d63d6"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"0837ab0472feecb3a54260729d845f839e1cbd72":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"d53704af5dd90675c56e347e9637e8734e73229d":["bdb48a021b1ad342569f087c1457740563897dc5"],"299a2348fa24151d150182211b6208a38e5e3450":[],"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36":["0837ab0472feecb3a54260729d845f839e1cbd72","d4d69c535930b5cce125cff868d40f6373dc27d4"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"0cdf9cc6702d60334a616bd7db3ae91501d1dce7":["05da2d758a6089e737cdfc230e57a51b472b94b6","c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"03d03ad772742aa485289047359fcfa328660c1f":["8e0f7eb31bf8a32470d42414f4b1718d4ee3ce61"],"51f5280f31484820499077f41fcdfe92d527d9dc":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["51f5280f31484820499077f41fcdfe92d527d9dc"],"8e0f7eb31bf8a32470d42414f4b1718d4ee3ce61":["c4108b7b6cc881d8b7c6ba2df50010b7bc581a39"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"d4d69c535930b5cce125cff868d40f6373dc27d4":["1744e62c7bb40f7a950b360af6371e5d89a1c112","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"3e8715d826e588419327562287d5d6a8040d63d6":["d2638f781be724518ff6c2263d14a48cf6e68017","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["b7605579001505896d48b07160075a5c8b8e128e","0cdf9cc6702d60334a616bd7db3ae91501d1dce7","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","d2638f781be724518ff6c2263d14a48cf6e68017","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","299a2348fa24151d150182211b6208a38e5e3450","0a22eafe3f72a4c2945eaad9547e6c78816978f4","a656b32c3aa151037a8c52e9b134acc3cbf482bc","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}