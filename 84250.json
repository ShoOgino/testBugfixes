{"path":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","commits":[{"id":"e091f281a6e026f8bb17aaf194efd0bbd3a7f549","date":1528221895,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms == null)\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                  \"field \" + indexedField + \" has no indexed data\");\n        Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n                skipAltTokens, ignoreStopWords) {\n          @SuppressWarnings(\"unchecked\")\n          @Override\n          protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n            if (tags.size() >= tagsLimit)\n              return;\n            if (offsetCorrector != null) {\n              int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n              if (offsetPair == null) {\n                log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                        startOffset, endOffset);\n                return;\n              }\n              startOffset = offsetPair[0];\n              endOffset = offsetPair[1];\n            }\n\n            NamedList tag = new NamedList();\n            tag.add(\"startOffset\", startOffset);\n            tag.add(\"endOffset\", endOffset);\n            if (addMatchText)\n              tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n            //below caches, and also flags matchDocIdsBS\n            tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n            tags.add(tag);\n          }\n\n          Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n          ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                  idSchemaField.getType().getValueSource(idSchemaField, null));\n\n          @SuppressWarnings(\"unchecked\")\n          private List lookupSchemaDocIds(Object docIdsKey) {\n            List schemaDocIds = docIdsListCache.get(docIdsKey);\n            if (schemaDocIds != null)\n              return schemaDocIds;\n            IntsRef docIds = lookupDocIds(docIdsKey);\n            //translate lucene docIds to schema ids\n            schemaDocIds = new ArrayList(docIds.length);\n            for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n              int docId = docIds.ints[i];\n              assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n              matchDocIdsBS.set(docId);//also, flip docid in bitset\n              try {\n                schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n              } catch (IOException e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n              }\n            }\n            assert !schemaDocIds.isEmpty();\n\n            docIdsListCache.put(docIds, schemaDocIds);\n            return schemaDocIds;\n          }\n\n        };\n        tagger.enableDocIdsCache(2000);//TODO configurable\n        tagger.process();\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms == null)\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                  \"field \" + indexedField + \" has no indexed data\");\n        Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n                skipAltTokens, ignoreStopWords) {\n          @SuppressWarnings(\"unchecked\")\n          @Override\n          protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n            if (tags.size() >= tagsLimit)\n              return;\n            if (offsetCorrector != null) {\n              int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n              if (offsetPair == null) {\n                log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                        startOffset, endOffset);\n                return;\n              }\n              startOffset = offsetPair[0];\n              endOffset = offsetPair[1];\n            }\n\n            NamedList tag = new NamedList();\n            tag.add(\"startOffset\", startOffset);\n            tag.add(\"endOffset\", endOffset);\n            if (addMatchText)\n              tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n            //below caches, and also flags matchDocIdsBS\n            tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n            tags.add(tag);\n          }\n\n          Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n          ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                  idSchemaField.getType().getValueSource(idSchemaField, null));\n\n          @SuppressWarnings(\"unchecked\")\n          private List lookupSchemaDocIds(Object docIdsKey) {\n            List schemaDocIds = docIdsListCache.get(docIdsKey);\n            if (schemaDocIds != null)\n              return schemaDocIds;\n            IntsRef docIds = lookupDocIds(docIdsKey);\n            //translate lucene docIds to schema ids\n            schemaDocIds = new ArrayList(docIds.length);\n            for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n              int docId = docIds.ints[i];\n              assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n              matchDocIdsBS.set(docId);//also, flip docid in bitset\n              try {\n                schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n              } catch (IOException e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n              }\n            }\n            assert !schemaDocIds.isEmpty();\n\n            docIdsListCache.put(docIds, schemaDocIds);\n            return schemaDocIds;\n          }\n\n        };\n        tagger.enableDocIdsCache(2000);//TODO configurable\n        tagger.process();\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms == null)\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                  \"field \" + indexedField + \" has no indexed data\");\n        Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n                skipAltTokens, ignoreStopWords) {\n          @SuppressWarnings(\"unchecked\")\n          @Override\n          protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n            if (tags.size() >= tagsLimit)\n              return;\n            if (offsetCorrector != null) {\n              int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n              if (offsetPair == null) {\n                log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                        startOffset, endOffset);\n                return;\n              }\n              startOffset = offsetPair[0];\n              endOffset = offsetPair[1];\n            }\n\n            NamedList tag = new NamedList();\n            tag.add(\"startOffset\", startOffset);\n            tag.add(\"endOffset\", endOffset);\n            if (addMatchText)\n              tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n            //below caches, and also flags matchDocIdsBS\n            tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n            tags.add(tag);\n          }\n\n          Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n          ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                  idSchemaField.getType().getValueSource(idSchemaField, null));\n\n          @SuppressWarnings(\"unchecked\")\n          private List lookupSchemaDocIds(Object docIdsKey) {\n            List schemaDocIds = docIdsListCache.get(docIdsKey);\n            if (schemaDocIds != null)\n              return schemaDocIds;\n            IntsRef docIds = lookupDocIds(docIdsKey);\n            //translate lucene docIds to schema ids\n            schemaDocIds = new ArrayList(docIds.length);\n            for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n              int docId = docIds.ints[i];\n              assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n              matchDocIdsBS.set(docId);//also, flip docid in bitset\n              try {\n                schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n              } catch (IOException e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n              }\n            }\n            assert !schemaDocIds.isEmpty();\n\n            docIdsListCache.put(docIds, schemaDocIds);\n            return schemaDocIds;\n          }\n\n        };\n        tagger.enableDocIdsCache(2000);//TODO configurable\n        tagger.process();\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ffa05c362899acc2aea1987e0845c94c36630a93","date":1586742867,"type":3,"author":"Trey Grainger","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","sourceNew":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms != null) {\n          Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n              skipAltTokens, ignoreStopWords) {\n            @SuppressWarnings(\"unchecked\")\n            @Override\n            protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n              if (tags.size() >= tagsLimit)\n                return;\n              if (offsetCorrector != null) {\n                int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n                if (offsetPair == null) {\n                  log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                      startOffset, endOffset);\n                  return;\n                }\n                startOffset = offsetPair[0];\n                endOffset = offsetPair[1];\n              }\n\n              NamedList tag = new NamedList();\n              tag.add(\"startOffset\", startOffset);\n              tag.add(\"endOffset\", endOffset);\n              if (addMatchText)\n                tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n              //below caches, and also flags matchDocIdsBS\n              tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n              tags.add(tag);\n            }\n\n            Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n            ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                idSchemaField.getType().getValueSource(idSchemaField, null));\n\n            @SuppressWarnings(\"unchecked\")\n            private List lookupSchemaDocIds(Object docIdsKey) {\n              List schemaDocIds = docIdsListCache.get(docIdsKey);\n              if (schemaDocIds != null)\n                return schemaDocIds;\n              IntsRef docIds = lookupDocIds(docIdsKey);\n              //translate lucene docIds to schema ids\n              schemaDocIds = new ArrayList(docIds.length);\n              for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n                int docId = docIds.ints[i];\n                assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n                matchDocIdsBS.set(docId);//also, flip docid in bitset\n                try {\n                  schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n                } catch (IOException e) {\n                  throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n                }\n              }\n              assert !schemaDocIds.isEmpty();\n\n              docIdsListCache.put(docIds, schemaDocIds);\n              return schemaDocIds;\n            }\n\n          };\n          tagger.enableDocIdsCache(2000);//TODO configurable\n          tagger.process();\n        }\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","sourceOld":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms == null)\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                  \"field \" + indexedField + \" has no indexed data\");\n        Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n                skipAltTokens, ignoreStopWords) {\n          @SuppressWarnings(\"unchecked\")\n          @Override\n          protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n            if (tags.size() >= tagsLimit)\n              return;\n            if (offsetCorrector != null) {\n              int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n              if (offsetPair == null) {\n                log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                        startOffset, endOffset);\n                return;\n              }\n              startOffset = offsetPair[0];\n              endOffset = offsetPair[1];\n            }\n\n            NamedList tag = new NamedList();\n            tag.add(\"startOffset\", startOffset);\n            tag.add(\"endOffset\", endOffset);\n            if (addMatchText)\n              tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n            //below caches, and also flags matchDocIdsBS\n            tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n            tags.add(tag);\n          }\n\n          Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n          ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                  idSchemaField.getType().getValueSource(idSchemaField, null));\n\n          @SuppressWarnings(\"unchecked\")\n          private List lookupSchemaDocIds(Object docIdsKey) {\n            List schemaDocIds = docIdsListCache.get(docIdsKey);\n            if (schemaDocIds != null)\n              return schemaDocIds;\n            IntsRef docIds = lookupDocIds(docIdsKey);\n            //translate lucene docIds to schema ids\n            schemaDocIds = new ArrayList(docIds.length);\n            for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n              int docId = docIds.ints[i];\n              assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n              matchDocIdsBS.set(docId);//also, flip docid in bitset\n              try {\n                schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n              } catch (IOException e) {\n                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n              }\n            }\n            assert !schemaDocIds.isEmpty();\n\n            docIdsListCache.put(docIds, schemaDocIds);\n            return schemaDocIds;\n          }\n\n        };\n        tagger.enableDocIdsCache(2000);//TODO configurable\n        tagger.process();\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2caf6d6e842e1a4e4ae68ec6dfa5139c31a84ec5","date":1591384964,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/tagger/TaggerRequestHandler#handleRequestBody(SolrQueryRequest,SolrQueryResponse).mjava","sourceNew":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    @SuppressWarnings({\"rawtypes\"})\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms != null) {\n          Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n              skipAltTokens, ignoreStopWords) {\n            @SuppressWarnings(\"unchecked\")\n            @Override\n            protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n              if (tags.size() >= tagsLimit)\n                return;\n              if (offsetCorrector != null) {\n                int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n                if (offsetPair == null) {\n                  log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                      startOffset, endOffset);\n                  return;\n                }\n                startOffset = offsetPair[0];\n                endOffset = offsetPair[1];\n              }\n\n              @SuppressWarnings({\"rawtypes\"})\n              NamedList tag = new NamedList();\n              tag.add(\"startOffset\", startOffset);\n              tag.add(\"endOffset\", endOffset);\n              if (addMatchText)\n                tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n              //below caches, and also flags matchDocIdsBS\n              tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n              tags.add(tag);\n            }\n\n            @SuppressWarnings({\"rawtypes\"})\n            Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n            ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                idSchemaField.getType().getValueSource(idSchemaField, null));\n\n            @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n            private List lookupSchemaDocIds(Object docIdsKey) {\n              List schemaDocIds = docIdsListCache.get(docIdsKey);\n              if (schemaDocIds != null)\n                return schemaDocIds;\n              IntsRef docIds = lookupDocIds(docIdsKey);\n              //translate lucene docIds to schema ids\n              schemaDocIds = new ArrayList<>(docIds.length);\n              for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n                int docId = docIds.ints[i];\n                assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n                matchDocIdsBS.set(docId);//also, flip docid in bitset\n                try {\n                  schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n                } catch (IOException e) {\n                  throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n                }\n              }\n              assert !schemaDocIds.isEmpty();\n\n              docIdsListCache.put(docIds, schemaDocIds);\n              return schemaDocIds;\n            }\n\n          };\n          tagger.enableDocIdsCache(2000);//TODO configurable\n          tagger.process();\n        }\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","sourceOld":"  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n\n    //--Read params\n    final String indexedField = req.getParams().get(\"field\");\n    if (indexedField == null)\n      throw new RuntimeException(\"required param 'field'\");\n\n    final TagClusterReducer tagClusterReducer =\n            chooseTagClusterReducer(req.getParams().get(OVERLAPS));\n    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);\n    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);\n    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);\n    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();\n    if (idSchemaField == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"The tagger requires a\" +\n              \"uniqueKey in the schema.\");//TODO this could be relaxed\n    }\n    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);\n    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,\n            fieldHasIndexedStopFilter(indexedField, req));\n\n    //--Get posted data\n    Reader inputReader = null;\n    Iterable<ContentStream> streams = req.getContentStreams();\n    if (streams != null) {\n      Iterator<ContentStream> iter = streams.iterator();\n      if (iter.hasNext()) {\n        inputReader = iter.next().getReader();\n      }\n      if (iter.hasNext()) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            getClass().getSimpleName()+\" does not support multiple ContentStreams\"); //TODO support bulk tagging?\n      }\n    }\n    if (inputReader == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          getClass().getSimpleName()+\" requires text to be POSTed to it\");\n    }\n\n    // We may or may not need to read the input into a string\n    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);\n\n    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);\n\n    final String inputString;//only populated if needed\n    if (addMatchText || inputStringFuture.inputString != null) {\n      //Read the input fully into a String buffer that we'll need later,\n      // then replace the input with a reader wrapping the buffer.\n      inputString = inputStringFuture.call();\n      inputReader.close();\n      inputReader = new StringReader(inputString);\n    } else {\n      inputString = null;//not used\n    }\n\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());\n    final List tags = new ArrayList(2000);\n\n    try {\n      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();\n      try (TokenStream tokenStream = analyzer.tokenStream(\"\", inputReader)) {\n        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);\n        if (terms != null) {\n          Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,\n              skipAltTokens, ignoreStopWords) {\n            @SuppressWarnings(\"unchecked\")\n            @Override\n            protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {\n              if (tags.size() >= tagsLimit)\n                return;\n              if (offsetCorrector != null) {\n                int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);\n                if (offsetPair == null) {\n                  log.debug(\"Discarded offsets [{}, {}] because couldn't balance XML.\",\n                      startOffset, endOffset);\n                  return;\n                }\n                startOffset = offsetPair[0];\n                endOffset = offsetPair[1];\n              }\n\n              NamedList tag = new NamedList();\n              tag.add(\"startOffset\", startOffset);\n              tag.add(\"endOffset\", endOffset);\n              if (addMatchText)\n                tag.add(\"matchText\", inputString.substring(startOffset, endOffset));\n              //below caches, and also flags matchDocIdsBS\n              tag.add(\"ids\", lookupSchemaDocIds(docIdsKey));\n              tags.add(tag);\n            }\n\n            Map<Object, List> docIdsListCache = new HashMap<>(2000);\n\n            ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,\n                idSchemaField.getType().getValueSource(idSchemaField, null));\n\n            @SuppressWarnings(\"unchecked\")\n            private List lookupSchemaDocIds(Object docIdsKey) {\n              List schemaDocIds = docIdsListCache.get(docIdsKey);\n              if (schemaDocIds != null)\n                return schemaDocIds;\n              IntsRef docIds = lookupDocIds(docIdsKey);\n              //translate lucene docIds to schema ids\n              schemaDocIds = new ArrayList(docIds.length);\n              for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {\n                int docId = docIds.ints[i];\n                assert i == docIds.offset || docIds.ints[i - 1] < docId : \"not sorted?\";\n                matchDocIdsBS.set(docId);//also, flip docid in bitset\n                try {\n                  schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here\n                } catch (IOException e) {\n                  throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n                }\n              }\n              assert !schemaDocIds.isEmpty();\n\n              docIdsListCache.put(docIds, schemaDocIds);\n              return schemaDocIds;\n            }\n\n          };\n          tagger.enableDocIdsCache(2000);//TODO configurable\n          tagger.process();\n        }\n      }\n    } finally {\n      inputReader.close();\n    }\n    rsp.add(\"tagsCount\",tags.size());\n    rsp.add(\"tags\", tags);\n\n    rsp.setReturnFields(new SolrReturnFields( req ));\n\n    //Solr's standard name for matching docs in response\n    rsp.add(\"response\", getDocList(rows, matchDocIdsBS));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ffa05c362899acc2aea1987e0845c94c36630a93":["e091f281a6e026f8bb17aaf194efd0bbd3a7f549"],"2caf6d6e842e1a4e4ae68ec6dfa5139c31a84ec5":["ffa05c362899acc2aea1987e0845c94c36630a93"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e091f281a6e026f8bb17aaf194efd0bbd3a7f549"],"f592209545c71895260367152601e9200399776d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e091f281a6e026f8bb17aaf194efd0bbd3a7f549"],"e091f281a6e026f8bb17aaf194efd0bbd3a7f549":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2caf6d6e842e1a4e4ae68ec6dfa5139c31a84ec5"]},"commit2Childs":{"ffa05c362899acc2aea1987e0845c94c36630a93":["2caf6d6e842e1a4e4ae68ec6dfa5139c31a84ec5"],"2caf6d6e842e1a4e4ae68ec6dfa5139c31a84ec5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","e091f281a6e026f8bb17aaf194efd0bbd3a7f549"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"f592209545c71895260367152601e9200399776d":[],"e091f281a6e026f8bb17aaf194efd0bbd3a7f549":["ffa05c362899acc2aea1987e0845c94c36630a93","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}