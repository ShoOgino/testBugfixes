{"path":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b241ea5e635d896cc0af83cd96ffd0322e0aba7","date":1294226200,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":null,"sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":null,"sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":null,"sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["9454a6510e2db155fb01faa5c049b06ece95fab9","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"868da859b43505d9d2a023bfeae6dd0c795f5295":["9454a6510e2db155fb01faa5c049b06ece95fab9","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":[],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["70ad682703b8585f5d0a637efec044d57ec05efb","8b241ea5e635d896cc0af83cd96ffd0322e0aba7","868da859b43505d9d2a023bfeae6dd0c795f5295"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}