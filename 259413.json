{"path":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","commits":[{"id":"a44b232879361a7ace3520b5b313094a9a35e044","date":1327356188,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"/dev/null","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"/dev/null","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"/dev/null","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a44b232879361a7ace3520b5b313094a9a35e044"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a44b232879361a7ace3520b5b313094a9a35e044"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a44b232879361a7ace3520b5b313094a9a35e044"],"a44b232879361a7ace3520b5b313094a9a35e044":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","a44b232879361a7ace3520b5b313094a9a35e044"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"a44b232879361a7ace3520b5b313094a9a35e044":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","3a119bbc8703c10faa329ec201c654b3a35a1e3e","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}