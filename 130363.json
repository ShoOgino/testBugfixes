{"path":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","commits":[{"id":"c6744d321d7fd4ced5c99aeff28c036ac61e2314","date":1272929763,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( \"h i j k ll cccc bbb aa\" ) ) );\n    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22}\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( \"h i j k ll cccc bbb aa\" ) ) );\n    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22}\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( \"h i j k ll cccc bbb aa\" ) ) );\n    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22}\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( \"h i j k ll cccc bbb aa\" ) ) );\n    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22}\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["c6744d321d7fd4ced5c99aeff28c036ac61e2314"],"c6744d321d7fd4ced5c99aeff28c036ac61e2314":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f080986da691a3bba7b757f43ab72cdc82b57ce"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c6744d321d7fd4ced5c99aeff28c036ac61e2314"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c6744d321d7fd4ced5c99aeff28c036ac61e2314":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}