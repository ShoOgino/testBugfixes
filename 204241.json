{"path":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","commits":[{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f7178a82d1134111f4511f28bb9ad57573a57d93","date":1354112608,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd, maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2","date":1354573582,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd, maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd, maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1ed9002c5afac843c7f2d04d88e74b40d627e1af","date":1357602069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd, maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8750a5466b5d1b5eb7ae596d683f85b2f8d6cc70","date":1358730615,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit we need thread DV test that would\n        // uncover this bug!!\n        // nocommit we should not cache in this case?\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6312aec6ba581f919d406ceff362bef430382c31","date":1358775555,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit we need thread DV test that would\n        // uncover this bug!!\n        // nocommit we should not cache in this case?\n        return valuesIn;\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // TODO: use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // TODO: use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":["b0b3768e97375c7a745c68f0b54710e8bedccc11","b0b3768e97375c7a745c68f0b54710e8bedccc11","b0b3768e97375c7a745c68f0b54710e8bedccc11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7","date":1370473544,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"59a0020b413d44dd79d85d7a66ed5004265fb453","date":1371758877,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","date":1373959221,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startTermsBPV;\n\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      // TODO: use Uninvert?\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n        } else {\n          startTermsBPV = 1;\n        }\n      } else {\n        startTermsBPV = 1;\n      }\n\n      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      int termOrd = 0;\n\n      // TODO: use Uninvert?\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            // Store 1+ ord into packed bits\n            docToTermOrd.set(docID, 1+termOrd);\n          }\n          termOrd++;\n        }\n      }\n      termOrdToBytesOffset.freeze();\n\n      // maybe an int-only impl?\n      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa":["59a0020b413d44dd79d85d7a66ed5004265fb453"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["1ed9002c5afac843c7f2d04d88e74b40d627e1af","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7","3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa"],"f7178a82d1134111f4511f28bb9ad57573a57d93":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa"],"c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"56572ec06f1407c066d6b7399413178b33176cd8":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","93dd449115a9247533e44bab47e8429e5dccbc6d"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6312aec6ba581f919d406ceff362bef430382c31"],"8750a5466b5d1b5eb7ae596d683f85b2f8d6cc70":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"59a0020b413d44dd79d85d7a66ed5004265fb453":["c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7"],"6312aec6ba581f919d406ceff362bef430382c31":["8750a5466b5d1b5eb7ae596d683f85b2f8d6cc70"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2":["f7178a82d1134111f4511f28bb9ad57573a57d93"],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["4bc9c79bf95b1262e0a6908ffbd895de19e33dc2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["93dd449115a9247533e44bab47e8429e5dccbc6d"]},"commit2Childs":{"3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa":["37a0f60745e53927c4c876cfe5b5a58170f0646c","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["8750a5466b5d1b5eb7ae596d683f85b2f8d6cc70"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"f7178a82d1134111f4511f28bb9ad57573a57d93":["4bc9c79bf95b1262e0a6908ffbd895de19e33dc2"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7":["37a0f60745e53927c4c876cfe5b5a58170f0646c","59a0020b413d44dd79d85d7a66ed5004265fb453"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"d4d69c535930b5cce125cff868d40f6373dc27d4":["c78b4d7d117bf3813b5bcf91a74a9df5d2531aa7"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8750a5466b5d1b5eb7ae596d683f85b2f8d6cc70":["6312aec6ba581f919d406ceff362bef430382c31"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","d4d69c535930b5cce125cff868d40f6373dc27d4","9d7e5f3aa5935964617824d1f9b2599ddb334464"],"59a0020b413d44dd79d85d7a66ed5004265fb453":["3618e9b99a76b26b0dbb5e7ea75cbb6065433eaa"],"6312aec6ba581f919d406ceff362bef430382c31":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["f7178a82d1134111f4511f28bb9ad57573a57d93"],"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2":["1ed9002c5afac843c7f2d04d88e74b40d627e1af"],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}