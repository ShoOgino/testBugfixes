{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","commits":[{"id":"da0d58b6bf72ebfd4d6722289ea725809c20c987","date":1531207054,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an append-only use-case, ie. there are no deletes.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateAppendOnly(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      numDocs += flushDocCount;\n      double flushSizeMB = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSizeMB * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSizeMB, IndexWriter.SOURCE_FLUSH));\n\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for append-only: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an append-only use-case, ie. there are no deletes.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateAppendOnly(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      numDocs += flushDocCount;\n      double flushSizeMB = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSizeMB * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSizeMB, IndexWriter.SOURCE_FLUSH));\n\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for append-only: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a90cc8c90aa53ddf51fbd15019989ac269514a3","date":1531845066,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","sourceNew":"  /**\n   * Simulate an append-only use-case, ie. there are no deletes.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateAppendOnly(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      numDocs += flushDocCount;\n      double flushSizeMB = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSizeMB * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSizeMB, IndexWriter.SOURCE_FLUSH));\n\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertTrue(merges.merges.size() > 0);\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for append-only: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n    }\n  }\n\n","sourceOld":"  /**\n   * Simulate an append-only use-case, ie. there are no deletes.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateAppendOnly(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      numDocs += flushDocCount;\n      double flushSizeMB = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSizeMB * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSizeMB, IndexWriter.SOURCE_FLUSH));\n\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for append-only: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateAppendOnly(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an append-only use-case, ie. there are no deletes.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateAppendOnly(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      numDocs += flushDocCount;\n      double flushSizeMB = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSizeMB * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSizeMB, IndexWriter.SOURCE_FLUSH));\n\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertTrue(merges.merges.size() > 0);\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for append-only: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"da0d58b6bf72ebfd4d6722289ea725809c20c987":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["da0d58b6bf72ebfd4d6722289ea725809c20c987"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","da0d58b6bf72ebfd4d6722289ea725809c20c987"]},"commit2Childs":{"da0d58b6bf72ebfd4d6722289ea725809c20c987":["4a90cc8c90aa53ddf51fbd15019989ac269514a3","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["da0d58b6bf72ebfd4d6722289ea725809c20c987","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}