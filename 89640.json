{"path":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","commits":[{"id":"b373db031e25f03ad6783efcfb77809dcd963565","date":1565686445,"type":0,"author":"noble","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"/dev/null","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    MiniSolrCloudCluster cluster = configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c44cc06c26e456fe9c215072b79fce30babe3975","date":1570365040,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    MiniSolrCloudCluster cluster = configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":3,"author":"jimczi","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    MiniSolrCloudCluster cluster = configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"161e330a2c1ea9c6baa3615ab380472a4ae80749","date":1582252910,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).info.docsWritten.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).docs.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3fba3dcc5d2c31b4652c425cc1cbada9dff4a51","date":1582784911,"type":3,"author":"noble","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).info.docsWritten.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount,null);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).info.docsWritten.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/util/TestExportTool#testVeryLargeCluster().mjava","sourceNew":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).info.docsWritten.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount,null);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","sourceOld":"  @Nightly\n  public void testVeryLargeCluster() throws Exception {\n    String COLLECTION_NAME = \"veryLargeColl\";\n    configureCluster(4)\n        .addConfig(\"conf\", configset(\"cloud-minimal\"))\n        .configure();\n\n    try {\n      CollectionAdminRequest\n          .createCollection(COLLECTION_NAME, \"conf\", 8, 1)\n          .setMaxShardsPerNode(10)\n          .process(cluster.getSolrClient());\n      cluster.waitForActiveCollection(COLLECTION_NAME, 8, 8);\n\n      String tmpFileLoc = new File(cluster.getBaseDir().toFile().getAbsolutePath() +\n          File.separator).getPath();\n      String url = cluster.getRandomJetty(random()).getBaseUrl() + \"/\" + COLLECTION_NAME;\n\n\n      int docCount = 0;\n\n      for (int j = 0; j < 4; j++) {\n        int bsz = 10000;\n        UpdateRequest ur = new UpdateRequest();\n        ur.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n        for (int i = 0; i < bsz; i++) {\n          ur.add(\"id\", String.valueOf((j * bsz) + i), \"desc_s\", TestUtil.randomSimpleString(random(), 10, 50));\n        }\n        cluster.getSolrClient().request(ur, COLLECTION_NAME);\n        docCount += bsz;\n      }\n\n      QueryResponse qr = cluster.getSolrClient().query(COLLECTION_NAME, new SolrQuery(\"*:*\").setRows(0));\n      assertEquals(docCount, qr.getResults().getNumFound());\n\n      DocCollection coll = cluster.getSolrClient().getClusterStateProvider().getCollection(COLLECTION_NAME);\n      HashMap<String, Long> docCounts = new HashMap<>();\n      long totalDocsFromCores = 0;\n      for (Slice slice : coll.getSlices()) {\n        Replica replica = slice.getLeader();\n        try (HttpSolrClient client = new HttpSolrClient.Builder(replica.getBaseUrl()).build()) {\n          long count = ExportTool.getDocCount(replica.getCoreName(), client);\n          docCounts.put(replica.getCoreName(), count);\n          totalDocsFromCores += count;\n        }\n      }\n      assertEquals(docCount, totalDocsFromCores);\n\n      ExportTool.MultiThreadedRunner info = null;\n      String absolutePath = null;\n\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".javabin\";\n      info.setOutFormat(absolutePath, \"javabin\");\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      assertJavabinDocsCount(info, docCount);\n      for (Map.Entry<String, Long> e : docCounts.entrySet()) {\n        assertEquals(e.getValue().longValue(), info.corehandlers.get(e.getKey()).receivedDocs.get());\n      }\n      info = new ExportTool.MultiThreadedRunner(url);\n      info.output = System.out;\n      absolutePath = tmpFileLoc + COLLECTION_NAME + random().nextInt(100000) + \".json\";\n      info.setOutFormat(absolutePath, \"jsonl\");\n      info.fields = \"id,desc_s\";\n      info.setLimit(\"-1\");\n      info.exportDocs();\n      long actual = ((ExportTool.JsonSink) info.sink).info.docsWritten.get();\n      assertTrue(\"docs written :\" + actual + \"docs produced : \" + info.docsWritten.get(), actual >= docCount);\n      assertJsonDocsCount(info, docCount,null);\n    } finally {\n      cluster.shutdown();\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b373db031e25f03ad6783efcfb77809dcd963565":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b3fba3dcc5d2c31b4652c425cc1cbada9dff4a51":["161e330a2c1ea9c6baa3615ab380472a4ae80749"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c44cc06c26e456fe9c215072b79fce30babe3975":["b373db031e25f03ad6783efcfb77809dcd963565"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["b3fba3dcc5d2c31b4652c425cc1cbada9dff4a51"],"161e330a2c1ea9c6baa3615ab380472a4ae80749":["c44cc06c26e456fe9c215072b79fce30babe3975"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"b0b597c65628ca9e73913a07e81691f8229bae35":["b373db031e25f03ad6783efcfb77809dcd963565","c44cc06c26e456fe9c215072b79fce30babe3975"]},"commit2Childs":{"b373db031e25f03ad6783efcfb77809dcd963565":["c44cc06c26e456fe9c215072b79fce30babe3975","b0b597c65628ca9e73913a07e81691f8229bae35"],"b3fba3dcc5d2c31b4652c425cc1cbada9dff4a51":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b373db031e25f03ad6783efcfb77809dcd963565"],"c44cc06c26e456fe9c215072b79fce30babe3975":["161e330a2c1ea9c6baa3615ab380472a4ae80749","b0b597c65628ca9e73913a07e81691f8229bae35"],"161e330a2c1ea9c6baa3615ab380472a4ae80749":["b3fba3dcc5d2c31b4652c425cc1cbada9dff4a51"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}