{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","commits":[{"id":"6256acedd658c13275a01e4ba106a621956a22f6","date":1400192928,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a2a0b58a171748f1022e63a0483908e6f50b0abf","date":1400686165,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e","date":1400786907,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e":["a2a0b58a171748f1022e63a0483908e6f50b0abf"],"6256acedd658c13275a01e4ba106a621956a22f6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a2a0b58a171748f1022e63a0483908e6f50b0abf":["6256acedd658c13275a01e4ba106a621956a22f6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e":[],"6256acedd658c13275a01e4ba106a621956a22f6":["a2a0b58a171748f1022e63a0483908e6f50b0abf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6256acedd658c13275a01e4ba106a621956a22f6","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a2a0b58a171748f1022e63a0483908e6f50b0abf":["3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}