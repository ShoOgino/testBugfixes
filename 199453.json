{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","commits":[{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene45Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene45Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fea9ea7d920261c39df5e00a1cab3749da3e04c","date":1383965836,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    // must make copy because this very code sometimes makes puts while iterating?!\n                    Set<String> copy = new HashSet<String>(termFreqs.keySet());\n                    for(String term : copy) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55f05605e5088b4c77d91d851ecb883f776266d0","date":1383996316,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    // must make copy because this very code sometimes makes puts while iterating?!\n                    Set<String> copy = new HashSet<String>(termFreqs.keySet());\n                    for(String term : copy) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    final Map<String,TermFreqs> termFreqs = new HashMap<String,TermFreqs>();\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false && addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    // must make copy because this very code sometimes makes puts while iterating?!\n                    Set<String> copy = new HashSet<String>(termFreqs.keySet());\n                    for(String term : copy) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(term, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"59835e98cc66398669702821a396204c7ae935ce","date":1383996402,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    // must make copy because this very code sometimes makes puts while iterating?!\n                    Set<String> copy = new HashSet<String>(termFreqs.keySet());\n                    for(String term : copy) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = _TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e1151ecb4798f5c31137aec032c241638018ed20","date":1394284367,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","date":1394564625,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","date":1398957288,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8106bc60c7452250f84c65cdb43ab6b1d8eb1534","date":1401906364,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene46Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["8435160e9702b19398118ddf76b61c846612b6a4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e1eb6b3ce884c0b9e064e112da158013ec33cd91","date":1402692077,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageEstimator.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe","date":1405588742,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0cdf9cc6702d60334a616bd7db3ae91501d1dce7","date":1405858112,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      termCount++;\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.shutdown();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793","date":1408030244,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene410Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene49Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e49088db00ea6cb232fbde9c8c646c721d4d049f","date":1411433559,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new Lucene410Codec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.FLAG_POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.FLAG_POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    DocsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n\n                      if (random().nextBoolean()) {\n                        docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                      } else if (docs instanceof DocsAndPositionsEnum) {\n                        docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                      } else {\n                        docs = termsEnum.docsAndPositions(null, null, 0);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        if (docs instanceof DocsAndPositionsEnum) {\n                          DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          for(int i=0;i<limit;i++) {\n                            posEnum.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        if (random().nextBoolean()) {\n                          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);\n                        } else if (docs instanceof DocsAndPositionsEnum) {\n                          docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);\n                        } else {\n                          docs = termsEnum.docsAndPositions(null, null, 0);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          if (docs instanceof DocsAndPositionsEnum) {\n                            DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;\n                            int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                            for(int i=0;i<limit;i++) {\n                              posEnum.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.FLAG_POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.FLAG_POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb5311f0bff57ce15a23909f4cfb953773630534","date":1424827033,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      // nocommit: can we remove the noPositions and always ask for positions here?\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5c65b6cf6c1636b32efc0890a3eafed66cb91e83","date":1427825679,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      // nocommit: can we remove the noPositions and always ask for positions here?\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b4e3cd382d0d075a0f1725649c084bb6510c483","date":1428096423,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator();\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator(null);\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator(null);\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator();\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator();\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(null, docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["2af5333eca069fd35d7e0572227a82d0696ce137"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2af5333eca069fd35d7e0572227a82d0696ce137","date":1465574261,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields) throws IOException {\n                  fieldsConsumer.write(fields);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator();\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","59835e98cc66398669702821a396204c7ae935ce","0f4464508ee83288c8c4585b533f9faaa93aa314","55f05605e5088b4c77d91d851ecb883f776266d0","5c65b6cf6c1636b32efc0890a3eafed66cb91e83","e49088db00ea6cb232fbde9c8c646c721d4d049f","51f5280f31484820499077f41fcdfe92d527d9dc","7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","2e0cd078dfbe2df4d73a3db81ed598a118caf5fe","e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields) throws IOException {\n                  fieldsConsumer.write(fields);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new AssertingCodec() {\n        @Override\n        public PostingsFormat getPostingsFormatForField(String field) {\n\n          PostingsFormat p = getCodec().postingsFormat();\n          if (p instanceof PerFieldPostingsFormat) {\n            p = ((PerFieldPostingsFormat) p).getPostingsFormatForField(field);\n          }\n          final PostingsFormat defaultPostingsFormat = p;\n\n          final Thread mainThread = Thread.currentThread();\n\n          if (field.equals(\"body\")) {\n\n            // A PF that counts up some stats and then in\n            // the end we verify the stats match what the\n            // final IndexReader says, just to exercise the\n            // new freedom of iterating the postings more\n            // than once at flush/merge:\n\n            return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n              @Override\n              public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n                final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n                return new FieldsConsumer() {\n                  @Override\n                  public void write(Fields fields) throws IOException {\n                    fieldsConsumer.write(fields);\n\n                    boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                    // We only use one thread for flushing\n                    // in this test:\n                    assert isMerge || Thread.currentThread() == mainThread;\n\n                    // We iterate the provided TermsEnum\n                    // twice, so we excercise this new freedom\n                    // with the inverted API; if\n                    // addOnSecondPass is true, we add up\n                    // term stats on the 2nd iteration:\n                    boolean addOnSecondPass = random().nextBoolean();\n\n                    //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                    // Gather our own stats:\n                    Terms terms = fields.terms(\"body\");\n                    assert terms != null;\n\n                    TermsEnum termsEnum = terms.iterator();\n                    PostingsEnum docs = null;\n                    while(termsEnum.next() != null) {\n                      BytesRef term = termsEnum.term();\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      String termString = term.utf8ToString();\n\n                      // During merge we should only see terms\n                      // we had already seen during a\n                      // previous flush:\n                      assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                      if (isMerge == false) {\n                        if (addOnSecondPass == false) {\n                          TermFreqs tf = termFreqs.get(termString);\n                          if (tf == null) {\n                            tf = new TermFreqs();\n                            termFreqs.put(termString, tf);\n                          }\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        } else if (termFreqs.containsKey(termString) == false) {\n                          // Add placeholder (2nd pass will\n                          // set its counts):\n                          termFreqs.put(termString, new TermFreqs());\n                        }\n                      }\n                    }\n\n                    // Also test seeking the TermsEnum:\n                    for(String term : termFreqs.keySet()) {\n                      if (termsEnum.seekExact(new BytesRef(term))) {\n                        // TODO: also sometimes ask for payloads/offsets?\n                        boolean noPositions = random().nextBoolean();\n                        if (noPositions) {\n                          docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                        } else {\n                          docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                        }\n\n                        int docFreq = 0;\n                        long totalTermFreq = 0;\n                        while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                          docFreq++;\n                          totalTermFreq += docs.freq();\n                          int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                          if (!noPositions) {\n                            for (int i = 0; i < limit; i++) {\n                              docs.nextPosition();\n                            }\n                          }\n                        }\n\n                        if (isMerge == false && addOnSecondPass) {\n                          TermFreqs tf = termFreqs.get(term);\n                          assert tf != null;\n                          tf.docFreq += docFreq;\n                          tf.totalTermFreq += totalTermFreq;\n                          sumDocFreq.addAndGet(docFreq);\n                          sumTotalTermFreq.addAndGet(totalTermFreq);\n                        }\n\n                        //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                        assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                        assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                      }\n                    }\n\n                    // Also test seekCeil\n                    for(int iter=0;iter<10;iter++) {\n                      BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                      SeekStatus status = termsEnum.seekCeil(term);\n                      if (status == SeekStatus.NOT_FOUND) {\n                        assertTrue(term.compareTo(termsEnum.term()) < 0);\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void close() throws IOException {\n                    fieldsConsumer.close();\n                  }\n                };\n              }\n\n              @Override\n              public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n                return defaultPostingsFormat.fieldsProducer(state);\n              }\n            };\n          } else {\n            return defaultPostingsFormat;\n          }\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      w.addDocument(doc);\n      bytesIndexed += RamUsageTester.sizeOf(doc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"622a708571e534680618b3c5e0c28ac539a47776","date":1517406892,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields, NormsProducer norms) throws IOException {\n                  fieldsConsumer.write(fields, norms);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields) throws IOException {\n                  fieldsConsumer.write(fields);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#testInvertedWrite().mjava","sourceNew":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields, NormsProducer norms) throws IOException {\n                  fieldsConsumer.write(fields, norms);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiTerms.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5123: make sure we can visit postings twice\n  // during flush/merge\n  public void testInvertedWrite() throws Exception {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);\n\n    // Must be concurrent because thread(s) can be merging\n    // while up to one thread flushes, and each of those\n    // threads iterates over the map while the flushing\n    // thread might be adding to it:\n    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();\n\n    final AtomicLong sumDocFreq = new AtomicLong();\n    final AtomicLong sumTotalTermFreq = new AtomicLong();\n\n    // TODO: would be better to use / delegate to the current\n    // Codec returned by getCodec()\n\n    iwc.setCodec(new FilterCodec(getCodec().getName(), getCodec()) {\n        @Override\n        public PostingsFormat postingsFormat() {\n\n          final PostingsFormat defaultPostingsFormat = delegate.postingsFormat();\n\n          final Thread mainThread = Thread.currentThread();\n\n          // A PF that counts up some stats and then in\n          // the end we verify the stats match what the\n          // final IndexReader says, just to exercise the\n          // new freedom of iterating the postings more\n          // than once at flush/merge:\n\n          return new PostingsFormat(defaultPostingsFormat.getName()) {\n\n            @Override\n            public FieldsConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {\n\n              final FieldsConsumer fieldsConsumer = defaultPostingsFormat.fieldsConsumer(state);\n\n              return new FieldsConsumer() {\n                @Override\n                public void write(Fields fields, NormsProducer norms) throws IOException {\n                  fieldsConsumer.write(fields, norms);\n\n                  boolean isMerge = state.context.context == IOContext.Context.MERGE;\n\n                  // We only use one thread for flushing\n                  // in this test:\n                  assert isMerge || Thread.currentThread() == mainThread;\n\n                  // We iterate the provided TermsEnum\n                  // twice, so we excercise this new freedom\n                  // with the inverted API; if\n                  // addOnSecondPass is true, we add up\n                  // term stats on the 2nd iteration:\n                  boolean addOnSecondPass = random().nextBoolean();\n\n                  //System.out.println(\"write isMerge=\" + isMerge + \" 2ndPass=\" + addOnSecondPass);\n\n                  // Gather our own stats:\n                  Terms terms = fields.terms(\"body\");\n                  assert terms != null;\n\n                  TermsEnum termsEnum = terms.iterator();\n                  PostingsEnum docs = null;\n                  while(termsEnum.next() != null) {\n                    BytesRef term = termsEnum.term();\n                    // TODO: also sometimes ask for payloads/offsets?\n                    boolean noPositions = random().nextBoolean();\n                    if (noPositions) {\n                      docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                    } else {\n                      docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                    }\n                    int docFreq = 0;\n                    long totalTermFreq = 0;\n                    while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                      docFreq++;\n                      totalTermFreq += docs.freq();\n                      int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                      if (!noPositions) {\n                        for (int i = 0; i < limit; i++) {\n                          docs.nextPosition();\n                        }\n                      }\n                    }\n\n                    String termString = term.utf8ToString();\n\n                    // During merge we should only see terms\n                    // we had already seen during a\n                    // previous flush:\n                    assertTrue(isMerge==false || termFreqs.containsKey(termString));\n\n                    if (isMerge == false) {\n                      if (addOnSecondPass == false) {\n                        TermFreqs tf = termFreqs.get(termString);\n                        if (tf == null) {\n                          tf = new TermFreqs();\n                          termFreqs.put(termString, tf);\n                        }\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      } else if (termFreqs.containsKey(termString) == false) {\n                        // Add placeholder (2nd pass will\n                        // set its counts):\n                        termFreqs.put(termString, new TermFreqs());\n                      }\n                    }\n                  }\n\n                  // Also test seeking the TermsEnum:\n                  for(String term : termFreqs.keySet()) {\n                    if (termsEnum.seekExact(new BytesRef(term))) {\n                      // TODO: also sometimes ask for payloads/offsets?\n                      boolean noPositions = random().nextBoolean();\n                      if (noPositions) {\n                        docs = termsEnum.postings(docs, PostingsEnum.FREQS);\n                      } else {\n                        docs = termsEnum.postings(null, PostingsEnum.POSITIONS);\n                      }\n\n                      int docFreq = 0;\n                      long totalTermFreq = 0;\n                      while (docs.nextDoc() != PostingsEnum.NO_MORE_DOCS) {\n                        docFreq++;\n                        totalTermFreq += docs.freq();\n                        int limit = TestUtil.nextInt(random(), 1, docs.freq());\n                        if (!noPositions) {\n                          for (int i = 0; i < limit; i++) {\n                            docs.nextPosition();\n                          }\n                        }\n                      }\n\n                      if (isMerge == false && addOnSecondPass) {\n                        TermFreqs tf = termFreqs.get(term);\n                        assert tf != null;\n                        tf.docFreq += docFreq;\n                        tf.totalTermFreq += totalTermFreq;\n                        sumDocFreq.addAndGet(docFreq);\n                        sumTotalTermFreq.addAndGet(totalTermFreq);\n                      }\n\n                      //System.out.println(\"  term=\" + term + \" docFreq=\" + docFreq + \" ttDF=\" + termToDocFreq.get(term));\n                      assertTrue(docFreq <= termFreqs.get(term).docFreq);\n                      assertTrue(totalTermFreq <= termFreqs.get(term).totalTermFreq);\n                    }\n                  }\n\n                  // Also test seekCeil\n                  for(int iter=0;iter<10;iter++) {\n                    BytesRef term = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));\n                    SeekStatus status = termsEnum.seekCeil(term);\n                    if (status == SeekStatus.NOT_FOUND) {\n                      assertTrue(term.compareTo(termsEnum.term()) < 0);\n                    }\n                  }\n                }\n\n                @Override\n                public void close() throws IOException {\n                  fieldsConsumer.close();\n                }\n              };\n            }\n\n            @Override\n            public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n              return defaultPostingsFormat.fieldsProducer(state);\n            }\n          };\n        }\n      });\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    LineFileDocs docs = new LineFileDocs(random());\n    int bytesToIndex = atLeast(100) * 1024;\n    int bytesIndexed = 0;\n    while (bytesIndexed < bytesToIndex) {\n      Document doc = docs.nextDoc();\n      Document justBodyDoc = new Document();\n      justBodyDoc.add(doc.getField(\"body\"));\n      w.addDocument(justBodyDoc);\n      bytesIndexed += RamUsageTester.sizeOf(justBodyDoc);\n    }\n\n    IndexReader r = w.getReader();\n    w.close();\n\n    Terms terms = MultiFields.getTerms(r, \"body\");\n    assertEquals(sumDocFreq.get(), terms.getSumDocFreq());\n    assertEquals(sumTotalTermFreq.get(), terms.getSumTotalTermFreq());\n\n    TermsEnum termsEnum = terms.iterator();\n    long termCount = 0;\n    boolean supportsOrds = true;\n    while(termsEnum.next() != null) {\n      BytesRef term = termsEnum.term();\n      assertEquals(termFreqs.get(term.utf8ToString()).docFreq, termsEnum.docFreq());\n      assertEquals(termFreqs.get(term.utf8ToString()).totalTermFreq, termsEnum.totalTermFreq());\n      if (supportsOrds) {\n        long ord;\n        try {\n          ord = termsEnum.ord();\n        } catch (UnsupportedOperationException uoe) {\n          supportsOrds = false;\n          ord = -1;\n        }\n        if (ord != -1) {\n          assertEquals(termCount, ord);\n        }\n      }\n      termCount++;\n    }\n    assertEquals(termFreqs.size(), termCount);\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"e49088db00ea6cb232fbde9c8c646c721d4d049f":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":["6613659748fe4411a7dcf85266e55db1f95f7315","e1151ecb4798f5c31137aec032c241638018ed20"],"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe":["e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"59835e98cc66398669702821a396204c7ae935ce":["55f05605e5088b4c77d91d851ecb883f776266d0"],"2af5333eca069fd35d7e0572227a82d0696ce137":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"e1151ecb4798f5c31137aec032c241638018ed20":["6613659748fe4411a7dcf85266e55db1f95f7315"],"cb5311f0bff57ce15a23909f4cfb953773630534":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4","5c65b6cf6c1636b32efc0890a3eafed66cb91e83"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"622a708571e534680618b3c5e0c28ac539a47776":["2af5333eca069fd35d7e0572227a82d0696ce137"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"55f05605e5088b4c77d91d851ecb883f776266d0":["8fea9ea7d920261c39df5e00a1cab3749da3e04c"],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"5c65b6cf6c1636b32efc0890a3eafed66cb91e83":["cb5311f0bff57ce15a23909f4cfb953773630534"],"0cdf9cc6702d60334a616bd7db3ae91501d1dce7":["2e0cd078dfbe2df4d73a3db81ed598a118caf5fe"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e1151ecb4798f5c31137aec032c241638018ed20"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["0cdf9cc6702d60334a616bd7db3ae91501d1dce7"],"6613659748fe4411a7dcf85266e55db1f95f7315":["59835e98cc66398669702821a396204c7ae935ce"],"8fea9ea7d920261c39df5e00a1cab3749da3e04c":["8435160e9702b19398118ddf76b61c846612b6a4"],"8435160e9702b19398118ddf76b61c846612b6a4":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534","e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e49088db00ea6cb232fbde9c8c646c721d4d049f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0f4464508ee83288c8c4585b533f9faaa93aa314","2af5333eca069fd35d7e0572227a82d0696ce137"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["622a708571e534680618b3c5e0c28ac539a47776"],"e1eb6b3ce884c0b9e064e112da158013ec33cd91":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"e49088db00ea6cb232fbde9c8c646c721d4d049f":["51f5280f31484820499077f41fcdfe92d527d9dc"],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":[],"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe":["0cdf9cc6702d60334a616bd7db3ae91501d1dce7"],"59835e98cc66398669702821a396204c7ae935ce":["6613659748fe4411a7dcf85266e55db1f95f7315"],"e1151ecb4798f5c31137aec032c241638018ed20":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"2af5333eca069fd35d7e0572227a82d0696ce137":["622a708571e534680618b3c5e0c28ac539a47776","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"cb5311f0bff57ce15a23909f4cfb953773630534":["5c65b6cf6c1636b32efc0890a3eafed66cb91e83"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"622a708571e534680618b3c5e0c28ac539a47776":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63"],"55f05605e5088b4c77d91d851ecb883f776266d0":["59835e98cc66398669702821a396204c7ae935ce"],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["e49088db00ea6cb232fbde9c8c646c721d4d049f"],"5c65b6cf6c1636b32efc0890a3eafed66cb91e83":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"0cdf9cc6702d60334a616bd7db3ae91501d1dce7":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","e1151ecb4798f5c31137aec032c241638018ed20"],"8fea9ea7d920261c39df5e00a1cab3749da3e04c":["55f05605e5088b4c77d91d851ecb883f776266d0"],"8435160e9702b19398118ddf76b61c846612b6a4":["8fea9ea7d920261c39df5e00a1cab3749da3e04c"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"0f4464508ee83288c8c4585b533f9faaa93aa314":["2af5333eca069fd35d7e0572227a82d0696ce137","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["cb5311f0bff57ce15a23909f4cfb953773630534","6b4e3cd382d0d075a0f1725649c084bb6510c483"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["8435160e9702b19398118ddf76b61c846612b6a4"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","e1eb6b3ce884c0b9e064e112da158013ec33cd91"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e1eb6b3ce884c0b9e064e112da158013ec33cd91":["2e0cd078dfbe2df4d73a3db81ed598a118caf5fe","c6f080a2ab37c464dd98db173f6cbf10dc74f211"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","c6f080a2ab37c464dd98db173f6cbf10dc74f211","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}