{"path":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","commits":[{"id":"444d4b906d0e3398f87d6a5c4967c508f11a7f0b","date":1466507434,"type":1,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues == null) {\n        throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n      }\n      noOfClasses += classValues.getValueCount();\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eada4de4a8b5a468d01cc6095412f30c353c232","date":1469858305,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        throw new IllegalStateException(\"field \\\"\" + classFieldName + \"\\\" must have sorted (set) doc values\");\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues == null) {\n        throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n      }\n      noOfClasses += classValues.getValueCount();\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        throw new IllegalStateException(\"field \\\"\" + classFieldName + \"\\\" must have sorted (set) doc values\");\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues == null) {\n        throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n      }\n      noOfClasses += classValues.getValueCount();\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        throw new IllegalStateException(\"field \\\"\" + classFieldName + \"\\\" must have sorted (set) doc values\");\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a226be2a131427d694ef2cb7cadc58f2be111f6b","date":1481533221,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        throw new IllegalStateException(\"field \\\"\" + classFieldName + \"\\\" must have sorted (set) doc values\");\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        throw new IllegalStateException(\"field \\\"\" + classFieldName + \"\\\" must have sorted (set) doc values\");\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e71b30ca45cace3116f6ee06a0bbcb578707fa52","date":1495110978,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6","date":1498031702,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        long totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        long totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        long totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        assert group.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        long totalHits = group.totalHits.value;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      long valueCount = 0;\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues != null) {\n        valueCount = classValues.getValueCount();\n      } else {\n        SortedSetDocValues sortedSetDocValues = leave.reader().getSortedSetDocValues(classFieldName);\n        if (sortedSetDocValues != null) {\n          valueCount = sortedSetDocValues.getValueCount();\n        }\n      }\n      if (classValues == null) {\n        // approximate with no. of terms\n        noOfClasses += leave.reader().terms(classFieldName).size();\n      }\n      noOfClasses += valueCount;\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs<Object> group : topGroups.groups) {\n        long totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e71b30ca45cace3116f6ee06a0bbcb578707fa52":["a226be2a131427d694ef2cb7cadc58f2be111f6b"],"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7eada4de4a8b5a468d01cc6095412f30c353c232":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6":["e71b30ca45cace3116f6ee06a0bbcb578707fa52"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["e9017cf144952056066919f1ebc7897ff9bd71b1","f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6"],"28288370235ed02234a64753cdbf0c6ec096304a":["e71b30ca45cace3116f6ee06a0bbcb578707fa52","f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7eada4de4a8b5a468d01cc6095412f30c353c232"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["a226be2a131427d694ef2cb7cadc58f2be111f6b","e71b30ca45cace3116f6ee06a0bbcb578707fa52"],"a226be2a131427d694ef2cb7cadc58f2be111f6b":["7eada4de4a8b5a468d01cc6095412f30c353c232"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a226be2a131427d694ef2cb7cadc58f2be111f6b"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["28288370235ed02234a64753cdbf0c6ec096304a"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","7eada4de4a8b5a468d01cc6095412f30c353c232"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"]},"commit2Childs":{"e71b30ca45cace3116f6ee06a0bbcb578707fa52":["f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6","28288370235ed02234a64753cdbf0c6ec096304a","e9017cf144952056066919f1ebc7897ff9bd71b1"],"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["7eada4de4a8b5a468d01cc6095412f30c353c232","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"7eada4de4a8b5a468d01cc6095412f30c353c232":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a226be2a131427d694ef2cb7cadc58f2be111f6b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1"],"a226be2a131427d694ef2cb7cadc58f2be111f6b":["e71b30ca45cace3116f6ee06a0bbcb578707fa52","e9017cf144952056066919f1ebc7897ff9bd71b1","9856095f7afb5a607bf5e65077615ed91273508c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","9856095f7afb5a607bf5e65077615ed91273508c","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}