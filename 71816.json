{"path":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","commits":[{"id":"d60c1bb96a28a26d197c36299f7b6c9c5da617a1","date":1522484702,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        pendingDeletes.onNewReader(reader, info);\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa5e39259dfd4a68287c824d3b7e1bc9097dc895","date":1522505041,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        pendingDeletes.onNewReader(reader, info);\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ee0394b8176abd7c90a4be8c05465be1879db79","date":1522842314,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        pendingDeletes.onNewReader(newReader, info);\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        pendingDeletes.onNewReader(reader, info);\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72","date":1523453225,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        pendingDeletes.onNewReader(newReader, info);\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43345f1452f9510f8aaadae6156fe0c834e7d957","date":1523483670,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, pendingDeletes.getLiveDocs(), info.info.maxDoc() - info.getDelCount() - pendingDeletes.numPendingDeletes());\n      boolean success2 = false;\n      try {\n        pendingDeletes.onNewReader(newReader, info);\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"49575fe0d33c4904ac42b0526411d1dee7549e9b","date":1523529874,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d36ba65c7e095c7938bfc2343a9a6cf689bfb43","date":1523531370,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4c088100f7646827db17ea080925f7a0a916b1cd","date":1524662254,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE,\n            // we don't need terms - lets leave them off-heap if possible\n            Collections.singletonMap(BlockTreeTermsReader.FST_MODE_KEY, BlockTreeTermsReader.FSTLoadMode.OFF_HEAP.name()));\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE,\n            // we don't need terms - lets leave them off-heap if possible\n            Collections.singletonMap(BlockTreeTermsReader.FST_MODE_KEY, BlockTreeTermsReader.FSTLoadMode.OFF_HEAP.name()));\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, true, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11b5a40d323ce34cc4159e1b8a44aeea352e0222","date":1585915085,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,FieldInfos.FieldNumbers,long,InfoStream).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file.\n        int maxFieldNumber = -1;\n        Map<String, FieldInfo> byName = new HashMap<>();\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          // cannot use builder.add(fi) because it does not preserve\n          // the local field number. Field numbers can be different from\n          // the global ones if the segment was created externally (and added to\n          // this index with IndexWriter#addIndexes(Directory)).\n          byName.put(fi.name, cloneFieldInfo(fi, fi.number));\n          maxFieldNumber = Math.max(fi.number, maxFieldNumber);\n        }\n\n        // create new fields with the right DV type\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n\n          if (byName.containsKey(update.field)) {\n            // the field already exists in this segment\n            FieldInfo fi = byName.get(update.field);\n            fi.setDocValuesType(update.type);\n          } else {\n            // the field is not present in this segment so we clone the global field\n            // (which is guaranteed to exist) and remaps its field number locally.\n            assert fieldNumbers.contains(update.field, update.type);\n            FieldInfo fi = cloneFieldInfo(builder.getOrAdd(update.field), ++maxFieldNumber);\n            fi.setDocValuesType(update.type);\n            byName.put(fi.name, fi);\n          }\n        }\n        fieldInfos = new FieldInfos(byName.values().toArray(new FieldInfo[0]));\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","sourceOld":"  public synchronized boolean writeFieldUpdates(Directory dir, FieldInfos.FieldNumbers fieldNumbers, long maxDelGen, InfoStream infoStream) throws IOException {\n    long startTimeNS = System.nanoTime();\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean any = false;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, Comparator.comparingLong(a -> a.delGen));\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, indexCreatedVersionMajor, IOContext.READONCE);\n        pendingDeletes.onNewReader(reader, info);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      swapNewReaderWithLatestLiveDocs();\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n    return true;\n  }\n\n","bugFix":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["4c088100f7646827db17ea080925f7a0a916b1cd"],"11b5a40d323ce34cc4159e1b8a44aeea352e0222":["bec68e7c41fed133827595747d853cad504e481e"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"],"5d36ba65c7e095c7938bfc2343a9a6cf689bfb43":["43345f1452f9510f8aaadae6156fe0c834e7d957","49575fe0d33c4904ac42b0526411d1dee7549e9b"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["5ee0394b8176abd7c90a4be8c05465be1879db79","9ae87c7be37e537f40fa3bb2c35fa4a368d12a72"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"4c088100f7646827db17ea080925f7a0a916b1cd":["5d36ba65c7e095c7938bfc2343a9a6cf689bfb43"],"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"49575fe0d33c4904ac42b0526411d1dee7549e9b":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["11b5a40d323ce34cc4159e1b8a44aeea352e0222"]},"commit2Childs":{"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"11b5a40d323ce34cc4159e1b8a44aeea352e0222":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["43345f1452f9510f8aaadae6156fe0c834e7d957","9ae87c7be37e537f40fa3bb2c35fa4a368d12a72"],"5d36ba65c7e095c7938bfc2343a9a6cf689bfb43":["4c088100f7646827db17ea080925f7a0a916b1cd"],"bec68e7c41fed133827595747d853cad504e481e":["11b5a40d323ce34cc4159e1b8a44aeea352e0222"],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["5d36ba65c7e095c7938bfc2343a9a6cf689bfb43","49575fe0d33c4904ac42b0526411d1dee7549e9b"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"4c088100f7646827db17ea080925f7a0a916b1cd":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"49575fe0d33c4904ac42b0526411d1dee7549e9b":["5d36ba65c7e095c7938bfc2343a9a6cf689bfb43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}