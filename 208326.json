{"path":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"/dev/null","sourceNew":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  * docs added since last flush. */\n  void abort() throws IOException {\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    try {\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fde68de507dbf344495d7b5e8052866fe5f254ab","date":1189434831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  * docs added since last flush. */\n  void abort() throws IOException {\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    try {\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"741a5cca05cabe1e7482410a29e563a08379251a","date":1196676550,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n      docStoreSegment = null;\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a251aa47d1808cbae42c0e172d698c377430e60","date":1199375390,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    bufferedDeleteDocIDs.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n      docStoreSegment = null;\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n      docStoreSegment = null;\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":null,"sourceOld":"  /** Called if we hit an exception when adding docs,\n   *  flushing, etc.  This resets our state, discarding any\n   *  docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    if (infoStream != null)\n      infoStream.println(\"docWriter: now abort\");\n\n    // Forcefully remove waiting ThreadStates from line\n    for(int i=0;i<numWaiting;i++)\n      waitingThreadStates[i].isIdle = true;\n    numWaiting = 0;\n\n    pauseAllThreads();\n\n    bufferedDeleteTerms.clear();\n    bufferedDeleteDocIDs.clear();\n    numBufferedDeleteTerms = 0;\n\n    try {\n\n      abortedFiles = files();\n\n      // Discard pending norms:\n      final int numField = fieldInfos.size();\n      for (int i=0;i<numField;i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          BufferedNorms n = norms[i];\n          if (n != null) {\n            n.out.reset();\n            n.reset();\n          }\n        }\n      }\n\n      // Reset vectors writer\n      if (tvx != null) {\n        tvx.close();\n        tvf.close();\n        tvd.close();\n        tvx = null;\n      }\n\n      // Reset fields writer\n      if (fieldsWriter != null) {\n        fieldsWriter.close();\n        fieldsWriter = null;\n      }\n\n      // Reset all postings data\n      resetPostingsData();\n\n      // Clear vectors & fields from ThreadStates\n      for(int i=0;i<threadStates.length;i++) {\n        ThreadState state = threadStates[i];\n        if (state.localFieldsWriter != null) {\n          state.localFieldsWriter.close();\n          state.localFieldsWriter = null;\n        }\n        state.tvfLocal.reset();\n        state.fdtLocal.reset();\n      }\n      docStoreSegment = null;\n      files = null;\n\n    } finally {\n      resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"/dev/null","sourceNew":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      message(\"docWriter: now abort\");\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["5ef87af8c7bd0f8429622b83aa74202383f2e757"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2993c85d947e3191bba14229ea72fd5675d048e2","date":1228593940,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      if (infoStream != null)\n        message(\"docWriter: now abort\");\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      message(\"docWriter: now abort\");\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87c966e9308847938a7c905c2e46a56d8df788b8","date":1255035452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      if (infoStream != null) {\n        message(\"docWriter: now abort\");\n      }\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n      if (infoStream != null) {\n        message(\"docWriter: done abort\");\n      }\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      if (infoStream != null)\n        message(\"docWriter: now abort\");\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#abort().mjava","sourceNew":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      if (infoStream != null) {\n        message(\"docWriter: now abort\");\n      }\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n      if (infoStream != null) {\n        message(\"docWriter: done abort\");\n      }\n    }\n  }\n\n","sourceOld":"  /** Called if we hit an exception at a bad time (when\n   *  updating the index files) and must discard all\n   *  currently buffered docs.  This resets our state,\n   *  discarding any docs added since last flush. */\n  synchronized void abort() throws IOException {\n\n    try {\n      if (infoStream != null) {\n        message(\"docWriter: now abort\");\n      }\n\n      // Forcefully remove waiting ThreadStates from line\n      waitQueue.abort();\n\n      // Wait for all other threads to finish with\n      // DocumentsWriter:\n      pauseAllThreads();\n\n      try {\n\n        assert 0 == waitQueue.numWaiting;\n\n        waitQueue.waitingBytes = 0;\n\n        try {\n          abortedFiles = openFiles();\n        } catch (Throwable t) {\n          abortedFiles = null;\n        }\n\n        deletesInRAM.clear();\n\n        openFiles.clear();\n\n        for(int i=0;i<threadStates.length;i++)\n          try {\n            threadStates[i].consumer.abort();\n          } catch (Throwable t) {\n          }\n\n        try {\n          consumer.abort();\n        } catch (Throwable t) {\n        }\n\n        docStoreSegment = null;\n        numDocsInStore = 0;\n        docStoreOffset = 0;\n\n        // Reset all postings data\n        doAfterFlush();\n\n      } finally {\n        resumeAllThreads();\n      }\n    } finally {\n      aborting = false;\n      notifyAll();\n      if (infoStream != null) {\n        message(\"docWriter: done abort\");\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a251aa47d1808cbae42c0e172d698c377430e60"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a251aa47d1808cbae42c0e172d698c377430e60":["741a5cca05cabe1e7482410a29e563a08379251a"],"87c966e9308847938a7c905c2e46a56d8df788b8":["2993c85d947e3191bba14229ea72fd5675d048e2"],"2993c85d947e3191bba14229ea72fd5675d048e2":["5350389bf83287111f7760b9e3db3af8e3648474"],"5350389bf83287111f7760b9e3db3af8e3648474":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"741a5cca05cabe1e7482410a29e563a08379251a":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["87c966e9308847938a7c905c2e46a56d8df788b8"]},"commit2Childs":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["5350389bf83287111f7760b9e3db3af8e3648474"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["741a5cca05cabe1e7482410a29e563a08379251a"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5a251aa47d1808cbae42c0e172d698c377430e60":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"87c966e9308847938a7c905c2e46a56d8df788b8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"741a5cca05cabe1e7482410a29e563a08379251a":["5a251aa47d1808cbae42c0e172d698c377430e60"],"2993c85d947e3191bba14229ea72fd5675d048e2":["87c966e9308847938a7c905c2e46a56d8df788b8"],"5350389bf83287111f7760b9e3db3af8e3648474":["2993c85d947e3191bba14229ea72fd5675d048e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}